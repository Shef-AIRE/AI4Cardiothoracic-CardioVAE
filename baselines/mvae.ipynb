{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "MVAE model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "559982695fd5a732"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "\n",
    "# Image Modality Encoder\n",
    "class ImageVAEEncoder(nn.Module):\n",
    "    def __init__(self, input_channels=1, latent_dim=256):\n",
    "        super(ImageVAEEncoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc_mu = nn.Linear(in_features=64 * 28 * 28, out_features=latent_dim)\n",
    "        self.fc_logvar = nn.Linear(in_features=64 * 28 * 28, out_features=latent_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.flatten(x)\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "    \n",
    "    \n",
    "\n",
    "# Image Modality Decoder\n",
    "class ImageVAEDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim=256, output_channels=1):\n",
    "        super(ImageVAEDecoder, self).__init__()\n",
    "        self.fc = nn.Linear(in_features=latent_dim, out_features=64 * 28 * 28)\n",
    "        self.convtrans1 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.convtrans2 = nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.convtrans3 = nn.ConvTranspose2d(16, output_channels, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output_activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = self.fc(z)\n",
    "        z = z.view(-1, 64, 28, 28)\n",
    "        z = self.relu(self.convtrans1(z))\n",
    "        z = self.relu(self.convtrans2(z))\n",
    "        z = self.output_activation(self.convtrans3(z))\n",
    "        return z\n",
    "\n",
    "# ECG Modality Encoder\n",
    "class ECGVAEEncoder(nn.Module):\n",
    "    def __init__(self, input_dim=60000, latent_dim=256):\n",
    "        super(ECGVAEEncoder, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv1d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc_mu = nn.Linear(in_features=64 * (input_dim // 8), out_features=latent_dim)  # Adjusted for stride=2, 3 layers\n",
    "        self.fc_logvar = nn.Linear(in_features=64 * (input_dim // 8), out_features=latent_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.flatten(x)\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "# ECG Modality Decoder\n",
    "class ECGVAEDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim=256, output_dim=60000):\n",
    "        super(ECGVAEDecoder, self).__init__()\n",
    "        self.fc = nn.Linear(in_features=latent_dim, out_features=64 * (output_dim // 8))\n",
    "        self.convtrans1 = nn.ConvTranspose1d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.convtrans2 = nn.ConvTranspose1d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.convtrans3 = nn.ConvTranspose1d(16, 1, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output_activation = nn.Identity()  # Suitable for standardized data\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = self.fc(z)\n",
    "        z = z.view(-1, 64, z.size(1) // 64)  # Adjust the reshape for proper dimensions\n",
    "        z = self.relu(self.convtrans1(z))\n",
    "        z = self.relu(self.convtrans2(z))\n",
    "        z = self.output_activation(self.convtrans3(z))\n",
    "        return z\n",
    "\n",
    "class MultimodalVAE(nn.Module):\n",
    "\n",
    "    def __init__(self, image_input_channels=1, ecg_input_dim=60000, latent_dim=256):\n",
    "        super(MultimodalVAE, self).__init__()\n",
    "        self.image_encoder = ImageVAEEncoder(image_input_channels, latent_dim)\n",
    "        self.ecg_encoder = ECGVAEEncoder(ecg_input_dim, latent_dim)\n",
    "        self.image_decoder = ImageVAEDecoder(latent_dim, image_input_channels)\n",
    "        self.ecg_decoder = ECGVAEDecoder(latent_dim, ecg_input_dim)\n",
    "\n",
    "        self.experts       = ProductOfExperts()\n",
    "        self.n_latents     = latent_dim\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return mu + eps * std\n",
    "        else:\n",
    "          return mu\n",
    "\n",
    "    def forward(self, image=None, ecg=None):\n",
    "        mu, logvar = self.infer(image, ecg)\n",
    "        # reparametrization trick to sample\n",
    "        z          = self.reparametrize(mu, logvar)\n",
    "        # reconstruct inputs based on that gaussian\n",
    "        img_recon  = self.image_decoder(z)\n",
    "        ecg_recon  = self.ecg_decoder(z)\n",
    "        return img_recon, ecg_recon, mu, logvar\n",
    "\n",
    "    def infer(self, image=None, ecg=None): \n",
    "        batch_size = image.size(0) if image is not None else ecg.size(0)\n",
    "        use_cuda   = next(self.parameters()).is_cuda  # check if CUDA\n",
    "        # initialize the universal prior expert\n",
    "        mu, logvar = prior_expert((1, batch_size, self.n_latents), \n",
    "                                  use_cuda=use_cuda)\n",
    "        if image is not None:\n",
    "            img_mu, img_logvar = self.image_encoder(image)\n",
    "            mu     = torch.cat((mu, img_mu.unsqueeze(0)), dim=0)\n",
    "            logvar = torch.cat((logvar, img_logvar.unsqueeze(0)), dim=0)\n",
    "\n",
    "        if ecg is not None:\n",
    "            ecg_mu, ecg_logvar = self.ecg_encoder(ecg)\n",
    "            mu     = torch.cat((mu, ecg_mu.unsqueeze(0)), dim=0)\n",
    "            logvar = torch.cat((logvar, ecg_logvar.unsqueeze(0)), dim=0)\n",
    "\n",
    "        # product of experts to combine gaussians\n",
    "        mu, logvar = self.experts(mu, logvar)\n",
    "        return mu, logvar\n",
    "\n",
    "\n",
    "class ProductOfExperts(nn.Module):\n",
    "    \"\"\"Return parameters for product of independent experts.\n",
    "    See https://arxiv.org/pdf/1410.7827.pdf for equations.\n",
    "\n",
    "    @param mu: M x D for M experts\n",
    "    @param logvar: M x D for M experts\n",
    "    \"\"\"\n",
    "    def forward(self, mu, logvar, eps=1e-8):\n",
    "        var       = torch.exp(logvar) + eps\n",
    "        # precision of i-th Gaussian expert at point x\n",
    "        T         = 1. / (var + eps)\n",
    "        pd_mu     = torch.sum(mu * T, dim=0) / torch.sum(T, dim=0)\n",
    "        pd_var    = 1. / torch.sum(T, dim=0)\n",
    "        pd_logvar = torch.log(pd_var + eps)\n",
    "        return pd_mu, pd_logvar\n",
    "\n",
    "\n",
    "\n",
    "def prior_expert(size, use_cuda=False):\n",
    "    \"\"\"Universal prior expert. Here we use a spherical\n",
    "    Gaussian: N(0, 1).\n",
    "\n",
    "    @param size: integer\n",
    "                 dimensionality of Gaussian\n",
    "    @param use_cuda: boolean [default: False]\n",
    "                     cast CUDA on variables\n",
    "    \"\"\"\n",
    "    mu     = Variable(torch.zeros(size))\n",
    "    logvar = Variable(torch.zeros(size))\n",
    "    if use_cuda:\n",
    "        mu, logvar = mu.cuda(), logvar.cuda()\n",
    "    return mu, logvar"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-23T01:00:43.768670100Z",
     "start_time": "2024-02-23T01:00:41.512622300Z"
    }
   },
   "id": "initial_id",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "MIMIC Dataloader"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a5feb03266a7f7b7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as nnf\n",
    "\n",
    "X_ecg_tensor = torch.load('data_feature/ecg_features_tensor.pt')\n",
    "X_image_tensor = torch.load('data_feature/encoder_image_tensor.pt')\n",
    "\n",
    "\n",
    "class ECGImageDataset(Dataset):\n",
    "    def __init__(self, ecg_features, image_features):\n",
    "        self.ecg_features = ecg_features\n",
    "        self.image_features = image_features\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ecg_features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.ecg_features[idx], self.image_features[idx]\n",
    "    \n",
    "dataset = ECGImageDataset(X_ecg_tensor, X_image_tensor)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T18:10:52.126545500Z",
     "start_time": "2024-02-18T18:02:27.313192100Z"
    }
   },
   "id": "b6407e893a05f595",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pre-Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77aa6dd23470bded"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Train Epoch: 1 [0/50981 (0%)]\tLoss: 81838808.000000\n",
      "Train Epoch: 1 [1280/50981 (3%)]\tLoss: 121808086.545455\n",
      "Train Epoch: 1 [2560/50981 (5%)]\tLoss: 101290457.142857\n",
      "Train Epoch: 1 [3840/50981 (8%)]\tLoss: 93793371.354839\n",
      "Train Epoch: 1 [5120/50981 (10%)]\tLoss: 89804247.024390\n",
      "Train Epoch: 1 [6400/50981 (13%)]\tLoss: 87307186.509804\n",
      "Train Epoch: 1 [7680/50981 (15%)]\tLoss: 85633081.180328\n",
      "Train Epoch: 1 [8960/50981 (18%)]\tLoss: 84412325.408451\n",
      "Train Epoch: 1 [10240/50981 (20%)]\tLoss: 83491665.185185\n",
      "Train Epoch: 1 [11520/50981 (23%)]\tLoss: 82765491.252747\n",
      "Train Epoch: 1 [12800/50981 (25%)]\tLoss: 82174091.722772\n",
      "Train Epoch: 1 [14080/50981 (28%)]\tLoss: 81680581.045045\n",
      "Train Epoch: 1 [15360/50981 (30%)]\tLoss: 81262929.190083\n",
      "Train Epoch: 1 [16640/50981 (33%)]\tLoss: 80895726.900763\n",
      "Train Epoch: 1 [17920/50981 (35%)]\tLoss: 80564842.212766\n",
      "Train Epoch: 1 [19200/50981 (38%)]\tLoss: 80265654.569536\n",
      "Train Epoch: 1 [20480/50981 (40%)]\tLoss: 79977689.590062\n",
      "Train Epoch: 1 [21760/50981 (43%)]\tLoss: 79700197.567251\n",
      "Train Epoch: 1 [23040/50981 (45%)]\tLoss: 79418514.519337\n",
      "Train Epoch: 1 [24320/50981 (48%)]\tLoss: 79141374.282723\n",
      "Train Epoch: 1 [25600/50981 (50%)]\tLoss: 78859307.184080\n",
      "Train Epoch: 1 [26880/50981 (53%)]\tLoss: 78565343.203791\n",
      "Train Epoch: 1 [28160/50981 (55%)]\tLoss: 78249811.438914\n",
      "Train Epoch: 1 [29440/50981 (58%)]\tLoss: 77938915.290043\n",
      "Train Epoch: 1 [30720/50981 (60%)]\tLoss: 77624687.170124\n",
      "Train Epoch: 1 [32000/50981 (63%)]\tLoss: 77296777.721116\n",
      "Train Epoch: 1 [33280/50981 (65%)]\tLoss: 76944958.406130\n",
      "Train Epoch: 1 [34560/50981 (68%)]\tLoss: 76583593.033210\n",
      "Train Epoch: 1 [35840/50981 (70%)]\tLoss: 76215976.156584\n",
      "Train Epoch: 1 [37120/50981 (73%)]\tLoss: 75831584.137457\n",
      "Train Epoch: 1 [38400/50981 (75%)]\tLoss: 75438623.162791\n",
      "Train Epoch: 1 [39680/50981 (78%)]\tLoss: 75003037.594855\n",
      "Train Epoch: 1 [40960/50981 (80%)]\tLoss: 74557194.641745\n",
      "Train Epoch: 1 [42240/50981 (83%)]\tLoss: 74098499.722054\n",
      "Train Epoch: 1 [43520/50981 (85%)]\tLoss: 73623036.832845\n",
      "Train Epoch: 1 [44800/50981 (88%)]\tLoss: 73132045.732194\n",
      "Train Epoch: 1 [46080/50981 (90%)]\tLoss: 72619480.387812\n",
      "Train Epoch: 1 [47360/50981 (93%)]\tLoss: 72090342.350404\n",
      "Train Epoch: 1 [48640/50981 (95%)]\tLoss: 71550135.160105\n",
      "Train Epoch: 1 [49920/50981 (98%)]\tLoss: 71025744.736573\n",
      "Train Epoch: 2 [0/50981 (0%)]\tLoss: 49091528.000000\n",
      "Train Epoch: 2 [1280/50981 (3%)]\tLoss: 48517915.272727\n",
      "Train Epoch: 2 [2560/50981 (5%)]\tLoss: 48039989.904762\n",
      "Train Epoch: 2 [3840/50981 (8%)]\tLoss: 47583264.903226\n",
      "Train Epoch: 2 [5120/50981 (10%)]\tLoss: 47197482.048780\n",
      "Train Epoch: 2 [6400/50981 (13%)]\tLoss: 46790852.627451\n",
      "Train Epoch: 2 [7680/50981 (15%)]\tLoss: 46424675.868852\n",
      "Train Epoch: 2 [8960/50981 (18%)]\tLoss: 46137046.704225\n",
      "Train Epoch: 2 [10240/50981 (20%)]\tLoss: 45827762.617284\n",
      "Train Epoch: 2 [11520/50981 (23%)]\tLoss: 45619044.175824\n",
      "Train Epoch: 2 [12800/50981 (25%)]\tLoss: 45323185.980198\n",
      "Train Epoch: 2 [14080/50981 (28%)]\tLoss: 45051600.756757\n",
      "Train Epoch: 2 [15360/50981 (30%)]\tLoss: 44812398.314050\n",
      "Train Epoch: 2 [16640/50981 (33%)]\tLoss: 44602422.381679\n",
      "Train Epoch: 2 [17920/50981 (35%)]\tLoss: 44358354.269504\n",
      "Train Epoch: 2 [19200/50981 (38%)]\tLoss: 44127974.172185\n",
      "Train Epoch: 2 [20480/50981 (40%)]\tLoss: 43889982.285714\n",
      "Train Epoch: 2 [21760/50981 (43%)]\tLoss: 43658268.046784\n",
      "Train Epoch: 2 [23040/50981 (45%)]\tLoss: 43479701.193370\n",
      "Train Epoch: 2 [24320/50981 (48%)]\tLoss: 43307486.848168\n",
      "Train Epoch: 2 [25600/50981 (50%)]\tLoss: 43126440.835821\n",
      "Train Epoch: 2 [26880/50981 (53%)]\tLoss: 42929544.928910\n",
      "Train Epoch: 2 [28160/50981 (55%)]\tLoss: 42747439.511312\n",
      "Train Epoch: 2 [29440/50981 (58%)]\tLoss: 42587104.225108\n",
      "Train Epoch: 2 [30720/50981 (60%)]\tLoss: 42405733.311203\n",
      "Train Epoch: 2 [32000/50981 (63%)]\tLoss: 42247174.964143\n",
      "Train Epoch: 2 [33280/50981 (65%)]\tLoss: 42086433.164751\n",
      "Train Epoch: 2 [34560/50981 (68%)]\tLoss: 41929948.619926\n",
      "Train Epoch: 2 [35840/50981 (70%)]\tLoss: 41774614.078292\n",
      "Train Epoch: 2 [37120/50981 (73%)]\tLoss: 41616961.608247\n",
      "Train Epoch: 2 [38400/50981 (75%)]\tLoss: 41467057.621262\n",
      "Train Epoch: 2 [39680/50981 (78%)]\tLoss: 41300006.160772\n",
      "Train Epoch: 2 [40960/50981 (80%)]\tLoss: 41151218.305296\n",
      "Train Epoch: 2 [42240/50981 (83%)]\tLoss: 40980185.824773\n",
      "Train Epoch: 2 [43520/50981 (85%)]\tLoss: 40804617.266862\n",
      "Train Epoch: 2 [44800/50981 (88%)]\tLoss: 40650969.652422\n",
      "Train Epoch: 2 [46080/50981 (90%)]\tLoss: 40501850.382271\n",
      "Train Epoch: 2 [47360/50981 (93%)]\tLoss: 40346027.967655\n",
      "Train Epoch: 2 [48640/50981 (95%)]\tLoss: 40187755.905512\n",
      "Train Epoch: 2 [49920/50981 (98%)]\tLoss: 40037560.163683\n",
      "Train Epoch: 3 [0/50981 (0%)]\tLoss: 34559908.000000\n",
      "Train Epoch: 3 [1280/50981 (3%)]\tLoss: 32938293.454545\n",
      "Train Epoch: 3 [2560/50981 (5%)]\tLoss: 32638496.571429\n",
      "Train Epoch: 3 [3840/50981 (8%)]\tLoss: 32597842.387097\n",
      "Train Epoch: 3 [5120/50981 (10%)]\tLoss: 32429386.195122\n",
      "Train Epoch: 3 [6400/50981 (13%)]\tLoss: 32269189.372549\n",
      "Train Epoch: 3 [7680/50981 (15%)]\tLoss: 32239987.409836\n",
      "Train Epoch: 3 [8960/50981 (18%)]\tLoss: 32128653.521127\n",
      "Train Epoch: 3 [10240/50981 (20%)]\tLoss: 32002156.000000\n",
      "Train Epoch: 3 [11520/50981 (23%)]\tLoss: 31935197.098901\n",
      "Train Epoch: 3 [12800/50981 (25%)]\tLoss: 31875616.000000\n",
      "Train Epoch: 3 [14080/50981 (28%)]\tLoss: 31787918.180180\n",
      "Train Epoch: 3 [15360/50981 (30%)]\tLoss: 31669393.719008\n",
      "Train Epoch: 3 [16640/50981 (33%)]\tLoss: 31579043.496183\n",
      "Train Epoch: 3 [17920/50981 (35%)]\tLoss: 31520329.290780\n",
      "Train Epoch: 3 [19200/50981 (38%)]\tLoss: 31449132.092715\n",
      "Train Epoch: 3 [20480/50981 (40%)]\tLoss: 31340701.453416\n",
      "Train Epoch: 3 [21760/50981 (43%)]\tLoss: 31257889.684211\n",
      "Train Epoch: 3 [23040/50981 (45%)]\tLoss: 31187186.850829\n",
      "Train Epoch: 3 [24320/50981 (48%)]\tLoss: 31142749.308901\n",
      "Train Epoch: 3 [25600/50981 (50%)]\tLoss: 31063805.781095\n",
      "Train Epoch: 3 [26880/50981 (53%)]\tLoss: 31012049.014218\n",
      "Train Epoch: 3 [28160/50981 (55%)]\tLoss: 30944012.117647\n",
      "Train Epoch: 3 [29440/50981 (58%)]\tLoss: 30895502.441558\n",
      "Train Epoch: 3 [30720/50981 (60%)]\tLoss: 30850911.145228\n",
      "Train Epoch: 3 [32000/50981 (63%)]\tLoss: 30782951.458167\n",
      "Train Epoch: 3 [33280/50981 (65%)]\tLoss: 30711172.398467\n",
      "Train Epoch: 3 [34560/50981 (68%)]\tLoss: 30651493.712177\n",
      "Train Epoch: 3 [35840/50981 (70%)]\tLoss: 30591004.519573\n",
      "Train Epoch: 3 [37120/50981 (73%)]\tLoss: 30528259.553265\n",
      "Train Epoch: 3 [38400/50981 (75%)]\tLoss: 30472236.890365\n",
      "Train Epoch: 3 [39680/50981 (78%)]\tLoss: 30406791.241158\n",
      "Train Epoch: 3 [40960/50981 (80%)]\tLoss: 30339912.984424\n",
      "Train Epoch: 3 [42240/50981 (83%)]\tLoss: 30276997.861027\n",
      "Train Epoch: 3 [43520/50981 (85%)]\tLoss: 30214973.460411\n",
      "Train Epoch: 3 [44800/50981 (88%)]\tLoss: 30160779.156695\n",
      "Train Epoch: 3 [46080/50981 (90%)]\tLoss: 30110862.559557\n",
      "Train Epoch: 3 [47360/50981 (93%)]\tLoss: 30059029.778976\n",
      "Train Epoch: 3 [48640/50981 (95%)]\tLoss: 29999596.519685\n",
      "Train Epoch: 3 [49920/50981 (98%)]\tLoss: 29948776.936061\n",
      "Train Epoch: 4 [0/50981 (0%)]\tLoss: 24739250.000000\n",
      "Train Epoch: 4 [1280/50981 (3%)]\tLoss: 26291657.636364\n",
      "Train Epoch: 4 [2560/50981 (5%)]\tLoss: 26374567.523810\n",
      "Train Epoch: 4 [3840/50981 (8%)]\tLoss: 26305564.709677\n",
      "Train Epoch: 4 [5120/50981 (10%)]\tLoss: 26272301.951220\n",
      "Train Epoch: 4 [6400/50981 (13%)]\tLoss: 26133045.411765\n",
      "Train Epoch: 4 [7680/50981 (15%)]\tLoss: 26210251.704918\n",
      "Train Epoch: 4 [8960/50981 (18%)]\tLoss: 26201040.816901\n",
      "Train Epoch: 4 [10240/50981 (20%)]\tLoss: 26215442.543210\n",
      "Train Epoch: 4 [11520/50981 (23%)]\tLoss: 26277853.670330\n",
      "Train Epoch: 4 [12800/50981 (25%)]\tLoss: 26310075.564356\n",
      "Train Epoch: 4 [14080/50981 (28%)]\tLoss: 26293584.630631\n",
      "Train Epoch: 4 [15360/50981 (30%)]\tLoss: 26250483.256198\n",
      "Train Epoch: 4 [16640/50981 (33%)]\tLoss: 26240662.717557\n",
      "Train Epoch: 4 [17920/50981 (35%)]\tLoss: 26237988.808511\n",
      "Train Epoch: 4 [19200/50981 (38%)]\tLoss: 26238146.119205\n",
      "Train Epoch: 4 [20480/50981 (40%)]\tLoss: 26214694.236025\n",
      "Train Epoch: 4 [21760/50981 (43%)]\tLoss: 26209853.228070\n",
      "Train Epoch: 4 [23040/50981 (45%)]\tLoss: 26190398.375691\n",
      "Train Epoch: 4 [24320/50981 (48%)]\tLoss: 26160375.298429\n",
      "Train Epoch: 4 [25600/50981 (50%)]\tLoss: 26130193.860697\n",
      "Train Epoch: 4 [26880/50981 (53%)]\tLoss: 26099694.331754\n",
      "Train Epoch: 4 [28160/50981 (55%)]\tLoss: 26073333.176471\n",
      "Train Epoch: 4 [29440/50981 (58%)]\tLoss: 26061879.619048\n",
      "Train Epoch: 4 [30720/50981 (60%)]\tLoss: 26056433.278008\n",
      "Train Epoch: 4 [32000/50981 (63%)]\tLoss: 26051333.019920\n",
      "Train Epoch: 4 [33280/50981 (65%)]\tLoss: 26035664.812261\n",
      "Train Epoch: 4 [34560/50981 (68%)]\tLoss: 26030487.121771\n",
      "Train Epoch: 4 [35840/50981 (70%)]\tLoss: 26025496.476868\n",
      "Train Epoch: 4 [37120/50981 (73%)]\tLoss: 26021272.563574\n",
      "Train Epoch: 4 [38400/50981 (75%)]\tLoss: 26006859.574751\n",
      "Train Epoch: 4 [39680/50981 (78%)]\tLoss: 25988667.536977\n",
      "Train Epoch: 4 [40960/50981 (80%)]\tLoss: 25980880.847352\n",
      "Train Epoch: 4 [42240/50981 (83%)]\tLoss: 25968760.604230\n",
      "Train Epoch: 4 [43520/50981 (85%)]\tLoss: 25957462.457478\n",
      "Train Epoch: 4 [44800/50981 (88%)]\tLoss: 25944664.176638\n",
      "Train Epoch: 4 [46080/50981 (90%)]\tLoss: 25930989.573407\n",
      "Train Epoch: 4 [47360/50981 (93%)]\tLoss: 25924364.345013\n",
      "Train Epoch: 4 [48640/50981 (95%)]\tLoss: 25908111.181102\n",
      "Train Epoch: 4 [49920/50981 (98%)]\tLoss: 25893720.547315\n",
      "Train Epoch: 5 [0/50981 (0%)]\tLoss: 24139700.000000\n",
      "Train Epoch: 5 [1280/50981 (3%)]\tLoss: 23513796.545455\n",
      "Train Epoch: 5 [2560/50981 (5%)]\tLoss: 23834163.904762\n",
      "Train Epoch: 5 [3840/50981 (8%)]\tLoss: 23785199.161290\n",
      "Train Epoch: 5 [5120/50981 (10%)]\tLoss: 23707708.000000\n",
      "Train Epoch: 5 [6400/50981 (13%)]\tLoss: 23756903.764706\n",
      "Train Epoch: 5 [7680/50981 (15%)]\tLoss: 23787399.967213\n",
      "Train Epoch: 5 [8960/50981 (18%)]\tLoss: 23793713.718310\n",
      "Train Epoch: 5 [10240/50981 (20%)]\tLoss: 23783461.827160\n",
      "Train Epoch: 5 [11520/50981 (23%)]\tLoss: 23757727.758242\n",
      "Train Epoch: 5 [12800/50981 (25%)]\tLoss: 23744564.257426\n",
      "Train Epoch: 5 [14080/50981 (28%)]\tLoss: 23796734.180180\n",
      "Train Epoch: 5 [15360/50981 (30%)]\tLoss: 23808916.776860\n",
      "Train Epoch: 5 [16640/50981 (33%)]\tLoss: 23791369.160305\n",
      "Train Epoch: 5 [17920/50981 (35%)]\tLoss: 23797640.085106\n",
      "Train Epoch: 5 [19200/50981 (38%)]\tLoss: 23798554.052980\n",
      "Train Epoch: 5 [20480/50981 (40%)]\tLoss: 23793190.956522\n",
      "Train Epoch: 5 [21760/50981 (43%)]\tLoss: 23786920.456140\n",
      "Train Epoch: 5 [23040/50981 (45%)]\tLoss: 23820150.618785\n",
      "Train Epoch: 5 [24320/50981 (48%)]\tLoss: 23803916.827225\n",
      "Train Epoch: 5 [25600/50981 (50%)]\tLoss: 23788234.825871\n",
      "Train Epoch: 5 [26880/50981 (53%)]\tLoss: 23793842.454976\n",
      "Train Epoch: 5 [28160/50981 (55%)]\tLoss: 23792759.420814\n",
      "Train Epoch: 5 [29440/50981 (58%)]\tLoss: 23792792.077922\n",
      "Train Epoch: 5 [30720/50981 (60%)]\tLoss: 23808437.302905\n",
      "Train Epoch: 5 [32000/50981 (63%)]\tLoss: 23802877.306773\n",
      "Train Epoch: 5 [33280/50981 (65%)]\tLoss: 23787044.659004\n",
      "Train Epoch: 5 [34560/50981 (68%)]\tLoss: 23779225.719557\n",
      "Train Epoch: 5 [35840/50981 (70%)]\tLoss: 23769061.871886\n",
      "Train Epoch: 5 [37120/50981 (73%)]\tLoss: 23766617.972509\n",
      "Train Epoch: 5 [38400/50981 (75%)]\tLoss: 23768645.906977\n",
      "Train Epoch: 5 [39680/50981 (78%)]\tLoss: 23748788.591640\n",
      "Train Epoch: 5 [40960/50981 (80%)]\tLoss: 23735838.797508\n",
      "Train Epoch: 5 [42240/50981 (83%)]\tLoss: 23744103.764350\n",
      "Train Epoch: 5 [43520/50981 (85%)]\tLoss: 23723858.205279\n",
      "Train Epoch: 5 [44800/50981 (88%)]\tLoss: 23719308.142450\n",
      "Train Epoch: 5 [46080/50981 (90%)]\tLoss: 23713280.642659\n",
      "Train Epoch: 5 [47360/50981 (93%)]\tLoss: 23703261.078167\n",
      "Train Epoch: 5 [48640/50981 (95%)]\tLoss: 23702327.989501\n",
      "Train Epoch: 5 [49920/50981 (98%)]\tLoss: 23692479.319693\n",
      "Train Epoch: 6 [0/50981 (0%)]\tLoss: 22206022.000000\n",
      "Train Epoch: 6 [1280/50981 (3%)]\tLoss: 21887351.636364\n",
      "Train Epoch: 6 [2560/50981 (5%)]\tLoss: 21936464.380952\n",
      "Train Epoch: 6 [3840/50981 (8%)]\tLoss: 21820259.806452\n",
      "Train Epoch: 6 [5120/50981 (10%)]\tLoss: 21888882.878049\n",
      "Train Epoch: 6 [6400/50981 (13%)]\tLoss: 22031122.901961\n",
      "Train Epoch: 6 [7680/50981 (15%)]\tLoss: 21991365.704918\n",
      "Train Epoch: 6 [8960/50981 (18%)]\tLoss: 21931373.746479\n",
      "Train Epoch: 6 [10240/50981 (20%)]\tLoss: 21937721.012346\n",
      "Train Epoch: 6 [11520/50981 (23%)]\tLoss: 21931147.736264\n",
      "Train Epoch: 6 [12800/50981 (25%)]\tLoss: 21975901.247525\n",
      "Train Epoch: 6 [14080/50981 (28%)]\tLoss: 21995484.216216\n",
      "Train Epoch: 6 [15360/50981 (30%)]\tLoss: 21996423.107438\n",
      "Train Epoch: 6 [16640/50981 (33%)]\tLoss: 22017820.396947\n",
      "Train Epoch: 6 [17920/50981 (35%)]\tLoss: 22018874.524823\n",
      "Train Epoch: 6 [19200/50981 (38%)]\tLoss: 22019690.000000\n",
      "Train Epoch: 6 [20480/50981 (40%)]\tLoss: 22019504.645963\n",
      "Train Epoch: 6 [21760/50981 (43%)]\tLoss: 22045997.555556\n",
      "Train Epoch: 6 [23040/50981 (45%)]\tLoss: 22068722.000000\n",
      "Train Epoch: 6 [24320/50981 (48%)]\tLoss: 22065617.424084\n",
      "Train Epoch: 6 [25600/50981 (50%)]\tLoss: 22052211.582090\n",
      "Train Epoch: 6 [26880/50981 (53%)]\tLoss: 22047450.445498\n",
      "Train Epoch: 6 [28160/50981 (55%)]\tLoss: 22033712.099548\n",
      "Train Epoch: 6 [29440/50981 (58%)]\tLoss: 22044500.415584\n",
      "Train Epoch: 6 [30720/50981 (60%)]\tLoss: 22034769.775934\n",
      "Train Epoch: 6 [32000/50981 (63%)]\tLoss: 22013718.645418\n",
      "Train Epoch: 6 [33280/50981 (65%)]\tLoss: 22011796.429119\n",
      "Train Epoch: 6 [34560/50981 (68%)]\tLoss: 22002652.516605\n",
      "Train Epoch: 6 [35840/50981 (70%)]\tLoss: 22006325.829181\n",
      "Train Epoch: 6 [37120/50981 (73%)]\tLoss: 22000864.955326\n",
      "Train Epoch: 6 [38400/50981 (75%)]\tLoss: 21989406.511628\n",
      "Train Epoch: 6 [39680/50981 (78%)]\tLoss: 21983942.289389\n",
      "Train Epoch: 6 [40960/50981 (80%)]\tLoss: 21983723.919003\n",
      "Train Epoch: 6 [42240/50981 (83%)]\tLoss: 21990634.435045\n",
      "Train Epoch: 6 [43520/50981 (85%)]\tLoss: 21990780.762463\n",
      "Train Epoch: 6 [44800/50981 (88%)]\tLoss: 21983722.997151\n",
      "Train Epoch: 6 [46080/50981 (90%)]\tLoss: 21980303.855956\n",
      "Train Epoch: 6 [47360/50981 (93%)]\tLoss: 21980564.194070\n",
      "Train Epoch: 6 [48640/50981 (95%)]\tLoss: 21976396.073491\n",
      "Train Epoch: 6 [49920/50981 (98%)]\tLoss: 21982006.956522\n",
      "Train Epoch: 7 [0/50981 (0%)]\tLoss: 20845252.000000\n",
      "Train Epoch: 7 [1280/50981 (3%)]\tLoss: 20501860.181818\n",
      "Train Epoch: 7 [2560/50981 (5%)]\tLoss: 20545653.142857\n",
      "Train Epoch: 7 [3840/50981 (8%)]\tLoss: 20546072.838710\n",
      "Train Epoch: 7 [5120/50981 (10%)]\tLoss: 20635559.414634\n",
      "Train Epoch: 7 [6400/50981 (13%)]\tLoss: 20648476.784314\n",
      "Train Epoch: 7 [7680/50981 (15%)]\tLoss: 20671106.918033\n",
      "Train Epoch: 7 [8960/50981 (18%)]\tLoss: 20647102.338028\n",
      "Train Epoch: 7 [10240/50981 (20%)]\tLoss: 20677364.567901\n",
      "Train Epoch: 7 [11520/50981 (23%)]\tLoss: 20671909.296703\n",
      "Train Epoch: 7 [12800/50981 (25%)]\tLoss: 20695071.326733\n",
      "Train Epoch: 7 [14080/50981 (28%)]\tLoss: 20703207.261261\n",
      "Train Epoch: 7 [15360/50981 (30%)]\tLoss: 20721247.008264\n",
      "Train Epoch: 7 [16640/50981 (33%)]\tLoss: 20701589.145038\n",
      "Train Epoch: 7 [17920/50981 (35%)]\tLoss: 20712262.000000\n",
      "Train Epoch: 7 [19200/50981 (38%)]\tLoss: 20720542.556291\n",
      "Train Epoch: 7 [20480/50981 (40%)]\tLoss: 20716395.204969\n",
      "Train Epoch: 7 [21760/50981 (43%)]\tLoss: 20721339.368421\n",
      "Train Epoch: 7 [23040/50981 (45%)]\tLoss: 20726640.861878\n",
      "Train Epoch: 7 [24320/50981 (48%)]\tLoss: 20721865.769634\n",
      "Train Epoch: 7 [25600/50981 (50%)]\tLoss: 20699452.895522\n",
      "Train Epoch: 7 [26880/50981 (53%)]\tLoss: 20681371.649289\n",
      "Train Epoch: 7 [28160/50981 (55%)]\tLoss: 20657536.235294\n",
      "Train Epoch: 7 [29440/50981 (58%)]\tLoss: 20652507.991342\n",
      "Train Epoch: 7 [30720/50981 (60%)]\tLoss: 20653688.224066\n",
      "Train Epoch: 7 [32000/50981 (63%)]\tLoss: 20643360.390438\n",
      "Train Epoch: 7 [33280/50981 (65%)]\tLoss: 20644898.850575\n",
      "Train Epoch: 7 [34560/50981 (68%)]\tLoss: 20638686.332103\n",
      "Train Epoch: 7 [35840/50981 (70%)]\tLoss: 20631581.345196\n",
      "Train Epoch: 7 [37120/50981 (73%)]\tLoss: 20634494.927835\n",
      "Train Epoch: 7 [38400/50981 (75%)]\tLoss: 20643228.119601\n",
      "Train Epoch: 7 [39680/50981 (78%)]\tLoss: 20621516.797428\n",
      "Train Epoch: 7 [40960/50981 (80%)]\tLoss: 20621769.806854\n",
      "Train Epoch: 7 [42240/50981 (83%)]\tLoss: 20625100.996979\n",
      "Train Epoch: 7 [43520/50981 (85%)]\tLoss: 20627621.501466\n",
      "Train Epoch: 7 [44800/50981 (88%)]\tLoss: 20630618.227920\n",
      "Train Epoch: 7 [46080/50981 (90%)]\tLoss: 20621950.958449\n",
      "Train Epoch: 7 [47360/50981 (93%)]\tLoss: 20615434.318059\n",
      "Train Epoch: 7 [48640/50981 (95%)]\tLoss: 20621451.212598\n",
      "Train Epoch: 7 [49920/50981 (98%)]\tLoss: 20627563.524297\n",
      "Train Epoch: 8 [0/50981 (0%)]\tLoss: 19342432.000000\n",
      "Train Epoch: 8 [1280/50981 (3%)]\tLoss: 19370143.454545\n",
      "Train Epoch: 8 [2560/50981 (5%)]\tLoss: 19320092.571429\n",
      "Train Epoch: 8 [3840/50981 (8%)]\tLoss: 19360038.451613\n",
      "Train Epoch: 8 [5120/50981 (10%)]\tLoss: 19453338.487805\n",
      "Train Epoch: 8 [6400/50981 (13%)]\tLoss: 19400372.274510\n",
      "Train Epoch: 8 [7680/50981 (15%)]\tLoss: 19426094.557377\n",
      "Train Epoch: 8 [8960/50981 (18%)]\tLoss: 19441000.197183\n",
      "Train Epoch: 8 [10240/50981 (20%)]\tLoss: 19460865.209877\n",
      "Train Epoch: 8 [11520/50981 (23%)]\tLoss: 19434088.307692\n",
      "Train Epoch: 8 [12800/50981 (25%)]\tLoss: 19460452.217822\n",
      "Train Epoch: 8 [14080/50981 (28%)]\tLoss: 19425085.837838\n",
      "Train Epoch: 8 [15360/50981 (30%)]\tLoss: 19404193.438017\n",
      "Train Epoch: 8 [16640/50981 (33%)]\tLoss: 19379953.251908\n",
      "Train Epoch: 8 [17920/50981 (35%)]\tLoss: 19376424.978723\n",
      "Train Epoch: 8 [19200/50981 (38%)]\tLoss: 19390023.245033\n",
      "Train Epoch: 8 [20480/50981 (40%)]\tLoss: 19383614.472050\n",
      "Train Epoch: 8 [21760/50981 (43%)]\tLoss: 19380775.321637\n",
      "Train Epoch: 8 [23040/50981 (45%)]\tLoss: 19385341.812155\n",
      "Train Epoch: 8 [24320/50981 (48%)]\tLoss: 19383182.544503\n",
      "Train Epoch: 8 [25600/50981 (50%)]\tLoss: 19383750.507463\n",
      "Train Epoch: 8 [26880/50981 (53%)]\tLoss: 19381015.440758\n",
      "Train Epoch: 8 [28160/50981 (55%)]\tLoss: 19358566.660633\n",
      "Train Epoch: 8 [29440/50981 (58%)]\tLoss: 19355544.761905\n",
      "Train Epoch: 8 [30720/50981 (60%)]\tLoss: 19358131.020747\n",
      "Train Epoch: 8 [32000/50981 (63%)]\tLoss: 19355998.645418\n",
      "Train Epoch: 8 [33280/50981 (65%)]\tLoss: 19354109.800766\n",
      "Train Epoch: 8 [34560/50981 (68%)]\tLoss: 19361402.103321\n",
      "Train Epoch: 8 [35840/50981 (70%)]\tLoss: 19354757.494662\n",
      "Train Epoch: 8 [37120/50981 (73%)]\tLoss: 19356619.773196\n",
      "Train Epoch: 8 [38400/50981 (75%)]\tLoss: 19369343.827243\n",
      "Train Epoch: 8 [39680/50981 (78%)]\tLoss: 19382987.241158\n",
      "Train Epoch: 8 [40960/50981 (80%)]\tLoss: 19374669.239875\n",
      "Train Epoch: 8 [42240/50981 (83%)]\tLoss: 19387034.821752\n",
      "Train Epoch: 8 [43520/50981 (85%)]\tLoss: 19393295.988270\n",
      "Train Epoch: 8 [44800/50981 (88%)]\tLoss: 19390511.236467\n",
      "Train Epoch: 8 [46080/50981 (90%)]\tLoss: 19397402.022161\n",
      "Train Epoch: 8 [47360/50981 (93%)]\tLoss: 19390571.762803\n",
      "Train Epoch: 8 [48640/50981 (95%)]\tLoss: 19390812.041995\n",
      "Train Epoch: 8 [49920/50981 (98%)]\tLoss: 19386385.191816\n",
      "Train Epoch: 9 [0/50981 (0%)]\tLoss: 18158210.000000\n",
      "Train Epoch: 9 [1280/50981 (3%)]\tLoss: 18033929.272727\n",
      "Train Epoch: 9 [2560/50981 (5%)]\tLoss: 18373912.952381\n",
      "Train Epoch: 9 [3840/50981 (8%)]\tLoss: 18390263.870968\n",
      "Train Epoch: 9 [5120/50981 (10%)]\tLoss: 18461004.048780\n",
      "Train Epoch: 9 [6400/50981 (13%)]\tLoss: 18523506.196078\n",
      "Train Epoch: 9 [7680/50981 (15%)]\tLoss: 18422417.081967\n",
      "Train Epoch: 9 [8960/50981 (18%)]\tLoss: 18433373.605634\n",
      "Train Epoch: 9 [10240/50981 (20%)]\tLoss: 18395311.160494\n",
      "Train Epoch: 9 [11520/50981 (23%)]\tLoss: 18384699.934066\n",
      "Train Epoch: 9 [12800/50981 (25%)]\tLoss: 18355900.554455\n",
      "Train Epoch: 9 [14080/50981 (28%)]\tLoss: 18357363.351351\n",
      "Train Epoch: 9 [15360/50981 (30%)]\tLoss: 18338638.231405\n",
      "Train Epoch: 9 [16640/50981 (33%)]\tLoss: 18314428.717557\n",
      "Train Epoch: 9 [17920/50981 (35%)]\tLoss: 18333104.765957\n",
      "Train Epoch: 9 [19200/50981 (38%)]\tLoss: 18323084.900662\n",
      "Train Epoch: 9 [20480/50981 (40%)]\tLoss: 18329682.049689\n",
      "Train Epoch: 9 [21760/50981 (43%)]\tLoss: 18345912.549708\n",
      "Train Epoch: 9 [23040/50981 (45%)]\tLoss: 18340553.193370\n",
      "Train Epoch: 9 [24320/50981 (48%)]\tLoss: 18333275.874346\n",
      "Train Epoch: 9 [25600/50981 (50%)]\tLoss: 18354575.781095\n",
      "Train Epoch: 9 [26880/50981 (53%)]\tLoss: 18351625.488152\n",
      "Train Epoch: 9 [28160/50981 (55%)]\tLoss: 18336471.809955\n",
      "Train Epoch: 9 [29440/50981 (58%)]\tLoss: 18335946.320346\n",
      "Train Epoch: 9 [30720/50981 (60%)]\tLoss: 18330179.195021\n",
      "Train Epoch: 9 [32000/50981 (63%)]\tLoss: 18324174.326693\n",
      "Train Epoch: 9 [33280/50981 (65%)]\tLoss: 18331017.065134\n",
      "Train Epoch: 9 [34560/50981 (68%)]\tLoss: 18333109.284133\n",
      "Train Epoch: 9 [35840/50981 (70%)]\tLoss: 18342330.540925\n",
      "Train Epoch: 9 [37120/50981 (73%)]\tLoss: 18340069.099656\n",
      "Train Epoch: 9 [38400/50981 (75%)]\tLoss: 18339353.036545\n",
      "Train Epoch: 9 [39680/50981 (78%)]\tLoss: 18346712.270096\n",
      "Train Epoch: 9 [40960/50981 (80%)]\tLoss: 18361734.299065\n",
      "Train Epoch: 9 [42240/50981 (83%)]\tLoss: 18359533.172205\n",
      "Train Epoch: 9 [43520/50981 (85%)]\tLoss: 18348464.891496\n",
      "Train Epoch: 9 [44800/50981 (88%)]\tLoss: 18344789.908832\n",
      "Train Epoch: 9 [46080/50981 (90%)]\tLoss: 18337243.351801\n",
      "Train Epoch: 9 [47360/50981 (93%)]\tLoss: 18335785.137466\n",
      "Train Epoch: 9 [48640/50981 (95%)]\tLoss: 18338292.624672\n",
      "Train Epoch: 9 [49920/50981 (98%)]\tLoss: 18333136.710997\n",
      "Train Epoch: 10 [0/50981 (0%)]\tLoss: 17599054.000000\n",
      "Train Epoch: 10 [1280/50981 (3%)]\tLoss: 17651459.000000\n",
      "Train Epoch: 10 [2560/50981 (5%)]\tLoss: 17581454.619048\n",
      "Train Epoch: 10 [3840/50981 (8%)]\tLoss: 17475386.516129\n",
      "Train Epoch: 10 [5120/50981 (10%)]\tLoss: 17385106.048780\n",
      "Train Epoch: 10 [6400/50981 (13%)]\tLoss: 17392134.254902\n",
      "Train Epoch: 10 [7680/50981 (15%)]\tLoss: 17409729.360656\n",
      "Train Epoch: 10 [8960/50981 (18%)]\tLoss: 17395586.380282\n",
      "Train Epoch: 10 [10240/50981 (20%)]\tLoss: 17407887.913580\n",
      "Train Epoch: 10 [11520/50981 (23%)]\tLoss: 17372160.956044\n",
      "Train Epoch: 10 [12800/50981 (25%)]\tLoss: 17356078.910891\n",
      "Train Epoch: 10 [14080/50981 (28%)]\tLoss: 17338213.855856\n",
      "Train Epoch: 10 [15360/50981 (30%)]\tLoss: 17325637.727273\n",
      "Train Epoch: 10 [16640/50981 (33%)]\tLoss: 17308774.770992\n",
      "Train Epoch: 10 [17920/50981 (35%)]\tLoss: 17336945.255319\n",
      "Train Epoch: 10 [19200/50981 (38%)]\tLoss: 17352473.225166\n",
      "Train Epoch: 10 [20480/50981 (40%)]\tLoss: 17352094.298137\n",
      "Train Epoch: 10 [21760/50981 (43%)]\tLoss: 17354521.654971\n",
      "Train Epoch: 10 [23040/50981 (45%)]\tLoss: 17360418.171271\n",
      "Train Epoch: 10 [24320/50981 (48%)]\tLoss: 17369189.376963\n",
      "Train Epoch: 10 [25600/50981 (50%)]\tLoss: 17344600.756219\n",
      "Train Epoch: 10 [26880/50981 (53%)]\tLoss: 17354201.834123\n",
      "Train Epoch: 10 [28160/50981 (55%)]\tLoss: 17344068.841629\n",
      "Train Epoch: 10 [29440/50981 (58%)]\tLoss: 17333755.575758\n",
      "Train Epoch: 10 [30720/50981 (60%)]\tLoss: 17341903.136929\n",
      "Train Epoch: 10 [32000/50981 (63%)]\tLoss: 17345699.729084\n",
      "Train Epoch: 10 [33280/50981 (65%)]\tLoss: 17343800.367816\n",
      "Train Epoch: 10 [34560/50981 (68%)]\tLoss: 17360639.900369\n",
      "Train Epoch: 10 [35840/50981 (70%)]\tLoss: 17373242.060498\n",
      "Train Epoch: 10 [37120/50981 (73%)]\tLoss: 17370395.721649\n",
      "Train Epoch: 10 [38400/50981 (75%)]\tLoss: 17376083.757475\n",
      "Train Epoch: 10 [39680/50981 (78%)]\tLoss: 17388274.163987\n",
      "Train Epoch: 10 [40960/50981 (80%)]\tLoss: 17382134.227414\n",
      "Train Epoch: 10 [42240/50981 (83%)]\tLoss: 17391909.725076\n",
      "Train Epoch: 10 [43520/50981 (85%)]\tLoss: 17396574.601173\n",
      "Train Epoch: 10 [44800/50981 (88%)]\tLoss: 17397618.925926\n",
      "Train Epoch: 10 [46080/50981 (90%)]\tLoss: 17406588.146814\n",
      "Train Epoch: 10 [47360/50981 (93%)]\tLoss: 17408138.126685\n",
      "Train Epoch: 10 [48640/50981 (95%)]\tLoss: 17413176.060367\n",
      "Train Epoch: 10 [49920/50981 (98%)]\tLoss: 17424244.570332\n",
      "Train Epoch: 11 [0/50981 (0%)]\tLoss: 18060356.000000\n",
      "Train Epoch: 11 [1280/50981 (3%)]\tLoss: 17172050.181818\n",
      "Train Epoch: 11 [2560/50981 (5%)]\tLoss: 17016800.285714\n",
      "Train Epoch: 11 [3840/50981 (8%)]\tLoss: 16898329.580645\n",
      "Train Epoch: 11 [5120/50981 (10%)]\tLoss: 16830814.073171\n",
      "Train Epoch: 11 [6400/50981 (13%)]\tLoss: 16779466.254902\n",
      "Train Epoch: 11 [7680/50981 (15%)]\tLoss: 16736487.114754\n",
      "Train Epoch: 11 [8960/50981 (18%)]\tLoss: 16684351.619718\n",
      "Train Epoch: 11 [10240/50981 (20%)]\tLoss: 16672422.148148\n",
      "Train Epoch: 11 [11520/50981 (23%)]\tLoss: 16661623.989011\n",
      "Train Epoch: 11 [12800/50981 (25%)]\tLoss: 16633336.435644\n",
      "Train Epoch: 11 [14080/50981 (28%)]\tLoss: 16625392.513514\n",
      "Train Epoch: 11 [15360/50981 (30%)]\tLoss: 16622355.595041\n",
      "Train Epoch: 11 [16640/50981 (33%)]\tLoss: 16621295.366412\n",
      "Train Epoch: 11 [17920/50981 (35%)]\tLoss: 16624503.680851\n",
      "Train Epoch: 11 [19200/50981 (38%)]\tLoss: 16594481.397351\n",
      "Train Epoch: 11 [20480/50981 (40%)]\tLoss: 16586639.708075\n",
      "Train Epoch: 11 [21760/50981 (43%)]\tLoss: 16566888.795322\n",
      "Train Epoch: 11 [23040/50981 (45%)]\tLoss: 16590600.265193\n",
      "Train Epoch: 11 [24320/50981 (48%)]\tLoss: 16588942.481675\n",
      "Train Epoch: 11 [25600/50981 (50%)]\tLoss: 16581063.940299\n",
      "Train Epoch: 11 [26880/50981 (53%)]\tLoss: 16572579.450237\n",
      "Train Epoch: 11 [28160/50981 (55%)]\tLoss: 16579656.113122\n",
      "Train Epoch: 11 [29440/50981 (58%)]\tLoss: 16601695.809524\n",
      "Train Epoch: 11 [30720/50981 (60%)]\tLoss: 16623371.360996\n",
      "Train Epoch: 11 [32000/50981 (63%)]\tLoss: 16624237.478088\n",
      "Train Epoch: 11 [33280/50981 (65%)]\tLoss: 16626621.279693\n",
      "Train Epoch: 11 [34560/50981 (68%)]\tLoss: 16633412.394834\n",
      "Train Epoch: 11 [35840/50981 (70%)]\tLoss: 16638536.807829\n",
      "Train Epoch: 11 [37120/50981 (73%)]\tLoss: 16650082.573883\n",
      "Train Epoch: 11 [38400/50981 (75%)]\tLoss: 16644366.465116\n",
      "Train Epoch: 11 [39680/50981 (78%)]\tLoss: 16636650.964630\n",
      "Train Epoch: 11 [40960/50981 (80%)]\tLoss: 16643767.682243\n",
      "Train Epoch: 11 [42240/50981 (83%)]\tLoss: 16648861.516616\n",
      "Train Epoch: 11 [43520/50981 (85%)]\tLoss: 16661485.847507\n",
      "Train Epoch: 11 [44800/50981 (88%)]\tLoss: 16659223.980057\n",
      "Train Epoch: 11 [46080/50981 (90%)]\tLoss: 16664627.847645\n",
      "Train Epoch: 11 [47360/50981 (93%)]\tLoss: 16660981.285714\n",
      "Train Epoch: 11 [48640/50981 (95%)]\tLoss: 16662028.522310\n",
      "Train Epoch: 11 [49920/50981 (98%)]\tLoss: 16657921.552430\n",
      "Train Epoch: 12 [0/50981 (0%)]\tLoss: 15483513.000000\n",
      "Train Epoch: 12 [1280/50981 (3%)]\tLoss: 15423858.454545\n",
      "Train Epoch: 12 [2560/50981 (5%)]\tLoss: 15730694.761905\n",
      "Train Epoch: 12 [3840/50981 (8%)]\tLoss: 15702426.903226\n",
      "Train Epoch: 12 [5120/50981 (10%)]\tLoss: 15926195.756098\n",
      "Train Epoch: 12 [6400/50981 (13%)]\tLoss: 15993088.000000\n",
      "Train Epoch: 12 [7680/50981 (15%)]\tLoss: 15921646.803279\n",
      "Train Epoch: 12 [8960/50981 (18%)]\tLoss: 15891690.422535\n",
      "Train Epoch: 12 [10240/50981 (20%)]\tLoss: 15888464.765432\n",
      "Train Epoch: 12 [11520/50981 (23%)]\tLoss: 15910339.571429\n",
      "Train Epoch: 12 [12800/50981 (25%)]\tLoss: 15950506.029703\n",
      "Train Epoch: 12 [14080/50981 (28%)]\tLoss: 15950718.405405\n",
      "Train Epoch: 12 [15360/50981 (30%)]\tLoss: 15957315.338843\n",
      "Train Epoch: 12 [16640/50981 (33%)]\tLoss: 15942383.839695\n",
      "Train Epoch: 12 [17920/50981 (35%)]\tLoss: 15963740.588652\n",
      "Train Epoch: 12 [19200/50981 (38%)]\tLoss: 15963896.000000\n",
      "Train Epoch: 12 [20480/50981 (40%)]\tLoss: 15958154.888199\n",
      "Train Epoch: 12 [21760/50981 (43%)]\tLoss: 15968338.304094\n",
      "Train Epoch: 12 [23040/50981 (45%)]\tLoss: 15950814.729282\n",
      "Train Epoch: 12 [24320/50981 (48%)]\tLoss: 15948991.188482\n",
      "Train Epoch: 12 [25600/50981 (50%)]\tLoss: 15939121.736318\n",
      "Train Epoch: 12 [26880/50981 (53%)]\tLoss: 15954782.507109\n",
      "Train Epoch: 12 [28160/50981 (55%)]\tLoss: 15965617.574661\n",
      "Train Epoch: 12 [29440/50981 (58%)]\tLoss: 15964887.272727\n",
      "Train Epoch: 12 [30720/50981 (60%)]\tLoss: 15970342.987552\n",
      "Train Epoch: 12 [32000/50981 (63%)]\tLoss: 15966945.940239\n",
      "Train Epoch: 12 [33280/50981 (65%)]\tLoss: 15971667.363985\n",
      "Train Epoch: 12 [34560/50981 (68%)]\tLoss: 15963192.095941\n",
      "Train Epoch: 12 [35840/50981 (70%)]\tLoss: 15969951.359431\n",
      "Train Epoch: 12 [37120/50981 (73%)]\tLoss: 15968505.402062\n",
      "Train Epoch: 12 [38400/50981 (75%)]\tLoss: 15979273.096346\n",
      "Train Epoch: 12 [39680/50981 (78%)]\tLoss: 15982144.893891\n",
      "Train Epoch: 12 [40960/50981 (80%)]\tLoss: 15988964.109034\n",
      "Train Epoch: 12 [42240/50981 (83%)]\tLoss: 15993194.531722\n",
      "Train Epoch: 12 [43520/50981 (85%)]\tLoss: 16005973.917889\n",
      "Train Epoch: 12 [44800/50981 (88%)]\tLoss: 16007720.316239\n",
      "Train Epoch: 12 [46080/50981 (90%)]\tLoss: 16012131.357341\n",
      "Train Epoch: 12 [47360/50981 (93%)]\tLoss: 16009411.876011\n",
      "Train Epoch: 12 [48640/50981 (95%)]\tLoss: 16004883.144357\n",
      "Train Epoch: 12 [49920/50981 (98%)]\tLoss: 16007293.209719\n",
      "Train Epoch: 13 [0/50981 (0%)]\tLoss: 16170393.000000\n",
      "Train Epoch: 13 [1280/50981 (3%)]\tLoss: 14897446.000000\n",
      "Train Epoch: 13 [2560/50981 (5%)]\tLoss: 15143626.857143\n",
      "Train Epoch: 13 [3840/50981 (8%)]\tLoss: 15206573.064516\n",
      "Train Epoch: 13 [5120/50981 (10%)]\tLoss: 15203642.341463\n",
      "Train Epoch: 13 [6400/50981 (13%)]\tLoss: 15219546.019608\n",
      "Train Epoch: 13 [7680/50981 (15%)]\tLoss: 15190521.655738\n",
      "Train Epoch: 13 [8960/50981 (18%)]\tLoss: 15188378.464789\n",
      "Train Epoch: 13 [10240/50981 (20%)]\tLoss: 15213997.469136\n",
      "Train Epoch: 13 [11520/50981 (23%)]\tLoss: 15202566.648352\n",
      "Train Epoch: 13 [12800/50981 (25%)]\tLoss: 15195993.574257\n",
      "Train Epoch: 13 [14080/50981 (28%)]\tLoss: 15219673.612613\n",
      "Train Epoch: 13 [15360/50981 (30%)]\tLoss: 15229997.305785\n",
      "Train Epoch: 13 [16640/50981 (33%)]\tLoss: 15266436.045802\n",
      "Train Epoch: 13 [17920/50981 (35%)]\tLoss: 15287246.269504\n",
      "Train Epoch: 13 [19200/50981 (38%)]\tLoss: 15325031.649007\n",
      "Train Epoch: 13 [20480/50981 (40%)]\tLoss: 15323925.031056\n",
      "Train Epoch: 13 [21760/50981 (43%)]\tLoss: 15328957.736842\n",
      "Train Epoch: 13 [23040/50981 (45%)]\tLoss: 15354020.696133\n",
      "Train Epoch: 13 [24320/50981 (48%)]\tLoss: 15345468.041885\n",
      "Train Epoch: 13 [25600/50981 (50%)]\tLoss: 15364489.194030\n",
      "Train Epoch: 13 [26880/50981 (53%)]\tLoss: 15353987.260664\n",
      "Train Epoch: 13 [28160/50981 (55%)]\tLoss: 15358654.719457\n",
      "Train Epoch: 13 [29440/50981 (58%)]\tLoss: 15371278.571429\n",
      "Train Epoch: 13 [30720/50981 (60%)]\tLoss: 15378659.273859\n",
      "Train Epoch: 13 [32000/50981 (63%)]\tLoss: 15385104.621514\n",
      "Train Epoch: 13 [33280/50981 (65%)]\tLoss: 15391778.919540\n",
      "Train Epoch: 13 [34560/50981 (68%)]\tLoss: 15405931.236162\n",
      "Train Epoch: 13 [35840/50981 (70%)]\tLoss: 15413957.323843\n",
      "Train Epoch: 13 [37120/50981 (73%)]\tLoss: 15427751.577320\n",
      "Train Epoch: 13 [38400/50981 (75%)]\tLoss: 15422658.850498\n",
      "Train Epoch: 13 [39680/50981 (78%)]\tLoss: 15419850.189711\n",
      "Train Epoch: 13 [40960/50981 (80%)]\tLoss: 15425412.747664\n",
      "Train Epoch: 13 [42240/50981 (83%)]\tLoss: 15425284.371601\n",
      "Train Epoch: 13 [43520/50981 (85%)]\tLoss: 15424807.023460\n",
      "Train Epoch: 13 [44800/50981 (88%)]\tLoss: 15426254.045584\n",
      "Train Epoch: 13 [46080/50981 (90%)]\tLoss: 15431279.642659\n",
      "Train Epoch: 13 [47360/50981 (93%)]\tLoss: 15437643.024259\n",
      "Train Epoch: 13 [48640/50981 (95%)]\tLoss: 15437905.301837\n",
      "Train Epoch: 13 [49920/50981 (98%)]\tLoss: 15441067.979540\n",
      "Train Epoch: 14 [0/50981 (0%)]\tLoss: 14525646.000000\n",
      "Train Epoch: 14 [1280/50981 (3%)]\tLoss: 14604584.727273\n",
      "Train Epoch: 14 [2560/50981 (5%)]\tLoss: 14685288.285714\n",
      "Train Epoch: 14 [3840/50981 (8%)]\tLoss: 14722052.774194\n",
      "Train Epoch: 14 [5120/50981 (10%)]\tLoss: 14680096.121951\n",
      "Train Epoch: 14 [6400/50981 (13%)]\tLoss: 14704876.803922\n",
      "Train Epoch: 14 [7680/50981 (15%)]\tLoss: 14678499.311475\n",
      "Train Epoch: 14 [8960/50981 (18%)]\tLoss: 14710904.845070\n",
      "Train Epoch: 14 [10240/50981 (20%)]\tLoss: 14693765.827160\n",
      "Train Epoch: 14 [11520/50981 (23%)]\tLoss: 14726459.813187\n",
      "Train Epoch: 14 [12800/50981 (25%)]\tLoss: 14761100.316832\n",
      "Train Epoch: 14 [14080/50981 (28%)]\tLoss: 14774686.495495\n",
      "Train Epoch: 14 [15360/50981 (30%)]\tLoss: 14777227.107438\n",
      "Train Epoch: 14 [16640/50981 (33%)]\tLoss: 14810511.633588\n",
      "Train Epoch: 14 [17920/50981 (35%)]\tLoss: 14815231.624113\n",
      "Train Epoch: 14 [19200/50981 (38%)]\tLoss: 14825893.860927\n",
      "Train Epoch: 14 [20480/50981 (40%)]\tLoss: 14840785.434783\n",
      "Train Epoch: 14 [21760/50981 (43%)]\tLoss: 14818621.339181\n",
      "Train Epoch: 14 [23040/50981 (45%)]\tLoss: 14825022.055249\n",
      "Train Epoch: 14 [24320/50981 (48%)]\tLoss: 14841999.047120\n",
      "Train Epoch: 14 [25600/50981 (50%)]\tLoss: 14870618.313433\n",
      "Train Epoch: 14 [26880/50981 (53%)]\tLoss: 14873408.848341\n",
      "Train Epoch: 14 [28160/50981 (55%)]\tLoss: 14890614.190045\n",
      "Train Epoch: 14 [29440/50981 (58%)]\tLoss: 14893225.831169\n",
      "Train Epoch: 14 [30720/50981 (60%)]\tLoss: 14900763.419087\n",
      "Train Epoch: 14 [32000/50981 (63%)]\tLoss: 14906645.764940\n",
      "Train Epoch: 14 [33280/50981 (65%)]\tLoss: 14911950.674330\n",
      "Train Epoch: 14 [34560/50981 (68%)]\tLoss: 14908881.309963\n",
      "Train Epoch: 14 [35840/50981 (70%)]\tLoss: 14911491.341637\n",
      "Train Epoch: 14 [37120/50981 (73%)]\tLoss: 14911316.137457\n",
      "Train Epoch: 14 [38400/50981 (75%)]\tLoss: 14913923.607973\n",
      "Train Epoch: 14 [39680/50981 (78%)]\tLoss: 14917613.234727\n",
      "Train Epoch: 14 [40960/50981 (80%)]\tLoss: 14930137.744548\n",
      "Train Epoch: 14 [42240/50981 (83%)]\tLoss: 14934953.785498\n",
      "Train Epoch: 14 [43520/50981 (85%)]\tLoss: 14938822.478006\n",
      "Train Epoch: 14 [44800/50981 (88%)]\tLoss: 14945887.384615\n",
      "Train Epoch: 14 [46080/50981 (90%)]\tLoss: 14960233.385042\n",
      "Train Epoch: 14 [47360/50981 (93%)]\tLoss: 14966738.293801\n",
      "Train Epoch: 14 [48640/50981 (95%)]\tLoss: 14973541.905512\n",
      "Train Epoch: 14 [49920/50981 (98%)]\tLoss: 14970319.352941\n",
      "Train Epoch: 15 [0/50981 (0%)]\tLoss: 13958475.000000\n",
      "Train Epoch: 15 [1280/50981 (3%)]\tLoss: 14333222.272727\n",
      "Train Epoch: 15 [2560/50981 (5%)]\tLoss: 14243010.428571\n",
      "Train Epoch: 15 [3840/50981 (8%)]\tLoss: 14308660.032258\n",
      "Train Epoch: 15 [5120/50981 (10%)]\tLoss: 14461442.121951\n",
      "Train Epoch: 15 [6400/50981 (13%)]\tLoss: 14437615.549020\n",
      "Train Epoch: 15 [7680/50981 (15%)]\tLoss: 14443440.147541\n",
      "Train Epoch: 15 [8960/50981 (18%)]\tLoss: 14430878.169014\n",
      "Train Epoch: 15 [10240/50981 (20%)]\tLoss: 14465210.074074\n",
      "Train Epoch: 15 [11520/50981 (23%)]\tLoss: 14449587.230769\n",
      "Train Epoch: 15 [12800/50981 (25%)]\tLoss: 14466882.990099\n",
      "Train Epoch: 15 [14080/50981 (28%)]\tLoss: 14482905.252252\n",
      "Train Epoch: 15 [15360/50981 (30%)]\tLoss: 14488325.355372\n",
      "Train Epoch: 15 [16640/50981 (33%)]\tLoss: 14497264.946565\n",
      "Train Epoch: 15 [17920/50981 (35%)]\tLoss: 14489626.382979\n",
      "Train Epoch: 15 [19200/50981 (38%)]\tLoss: 14484841.821192\n",
      "Train Epoch: 15 [20480/50981 (40%)]\tLoss: 14489372.819876\n",
      "Train Epoch: 15 [21760/50981 (43%)]\tLoss: 14526751.900585\n",
      "Train Epoch: 15 [23040/50981 (45%)]\tLoss: 14521213.983425\n",
      "Train Epoch: 15 [24320/50981 (48%)]\tLoss: 14504325.450262\n",
      "Train Epoch: 15 [25600/50981 (50%)]\tLoss: 14501008.646766\n",
      "Train Epoch: 15 [26880/50981 (53%)]\tLoss: 14506743.037915\n",
      "Train Epoch: 15 [28160/50981 (55%)]\tLoss: 14535441.248869\n",
      "Train Epoch: 15 [29440/50981 (58%)]\tLoss: 14540611.380952\n",
      "Train Epoch: 15 [30720/50981 (60%)]\tLoss: 14559842.390041\n",
      "Train Epoch: 15 [32000/50981 (63%)]\tLoss: 14553266.637450\n",
      "Train Epoch: 15 [33280/50981 (65%)]\tLoss: 14560025.448276\n",
      "Train Epoch: 15 [34560/50981 (68%)]\tLoss: 14566033.324723\n",
      "Train Epoch: 15 [35840/50981 (70%)]\tLoss: 14569278.676157\n",
      "Train Epoch: 15 [37120/50981 (73%)]\tLoss: 14584215.061856\n",
      "Train Epoch: 15 [38400/50981 (75%)]\tLoss: 14589524.182724\n",
      "Train Epoch: 15 [39680/50981 (78%)]\tLoss: 14591835.643087\n",
      "Train Epoch: 15 [40960/50981 (80%)]\tLoss: 14592016.959502\n",
      "Train Epoch: 15 [42240/50981 (83%)]\tLoss: 14593296.625378\n",
      "Train Epoch: 15 [43520/50981 (85%)]\tLoss: 14601288.560117\n",
      "Train Epoch: 15 [44800/50981 (88%)]\tLoss: 14604750.962963\n",
      "Train Epoch: 15 [46080/50981 (90%)]\tLoss: 14607971.831025\n",
      "Train Epoch: 15 [47360/50981 (93%)]\tLoss: 14609012.822102\n",
      "Train Epoch: 15 [48640/50981 (95%)]\tLoss: 14605741.081365\n",
      "Train Epoch: 15 [49920/50981 (98%)]\tLoss: 14599661.416880\n",
      "Train Epoch: 16 [0/50981 (0%)]\tLoss: 14220042.000000\n",
      "Train Epoch: 16 [1280/50981 (3%)]\tLoss: 13856674.000000\n",
      "Train Epoch: 16 [2560/50981 (5%)]\tLoss: 13913376.238095\n",
      "Train Epoch: 16 [3840/50981 (8%)]\tLoss: 13933969.903226\n",
      "Train Epoch: 16 [5120/50981 (10%)]\tLoss: 13984299.048780\n",
      "Train Epoch: 16 [6400/50981 (13%)]\tLoss: 13918907.686275\n",
      "Train Epoch: 16 [7680/50981 (15%)]\tLoss: 13938002.442623\n",
      "Train Epoch: 16 [8960/50981 (18%)]\tLoss: 13938403.943662\n",
      "Train Epoch: 16 [10240/50981 (20%)]\tLoss: 13944184.790123\n",
      "Train Epoch: 16 [11520/50981 (23%)]\tLoss: 13989943.076923\n",
      "Train Epoch: 16 [12800/50981 (25%)]\tLoss: 14026413.089109\n",
      "Train Epoch: 16 [14080/50981 (28%)]\tLoss: 14046557.756757\n",
      "Train Epoch: 16 [15360/50981 (30%)]\tLoss: 14049340.884298\n",
      "Train Epoch: 16 [16640/50981 (33%)]\tLoss: 14054428.572519\n",
      "Train Epoch: 16 [17920/50981 (35%)]\tLoss: 14080586.283688\n",
      "Train Epoch: 16 [19200/50981 (38%)]\tLoss: 14090145.774834\n",
      "Train Epoch: 16 [20480/50981 (40%)]\tLoss: 14099538.689441\n",
      "Train Epoch: 16 [21760/50981 (43%)]\tLoss: 14103228.403509\n",
      "Train Epoch: 16 [23040/50981 (45%)]\tLoss: 14118169.447514\n",
      "Train Epoch: 16 [24320/50981 (48%)]\tLoss: 14114488.691099\n",
      "Train Epoch: 16 [25600/50981 (50%)]\tLoss: 14155046.955224\n",
      "Train Epoch: 16 [26880/50981 (53%)]\tLoss: 14156903.341232\n",
      "Train Epoch: 16 [28160/50981 (55%)]\tLoss: 14155943.511312\n",
      "Train Epoch: 16 [29440/50981 (58%)]\tLoss: 14167998.004329\n",
      "Train Epoch: 16 [30720/50981 (60%)]\tLoss: 14175294.651452\n",
      "Train Epoch: 16 [32000/50981 (63%)]\tLoss: 14177209.000000\n",
      "Train Epoch: 16 [33280/50981 (65%)]\tLoss: 14187075.934866\n",
      "Train Epoch: 16 [34560/50981 (68%)]\tLoss: 14180967.697417\n",
      "Train Epoch: 16 [35840/50981 (70%)]\tLoss: 14179795.209964\n",
      "Train Epoch: 16 [37120/50981 (73%)]\tLoss: 14184501.560137\n",
      "Train Epoch: 16 [38400/50981 (75%)]\tLoss: 14178119.627907\n",
      "Train Epoch: 16 [39680/50981 (78%)]\tLoss: 14181518.508039\n",
      "Train Epoch: 16 [40960/50981 (80%)]\tLoss: 14181721.757009\n",
      "Train Epoch: 16 [42240/50981 (83%)]\tLoss: 14186840.199396\n",
      "Train Epoch: 16 [43520/50981 (85%)]\tLoss: 14191911.510264\n",
      "Train Epoch: 16 [44800/50981 (88%)]\tLoss: 14190014.663818\n",
      "Train Epoch: 16 [46080/50981 (90%)]\tLoss: 14205062.908587\n",
      "Train Epoch: 16 [47360/50981 (93%)]\tLoss: 14218550.374663\n",
      "Train Epoch: 16 [48640/50981 (95%)]\tLoss: 14227151.351706\n",
      "Train Epoch: 16 [49920/50981 (98%)]\tLoss: 14229828.352941\n",
      "Train Epoch: 17 [0/50981 (0%)]\tLoss: 13188966.000000\n",
      "Train Epoch: 17 [1280/50981 (3%)]\tLoss: 13372854.090909\n",
      "Train Epoch: 17 [2560/50981 (5%)]\tLoss: 13575513.761905\n",
      "Train Epoch: 17 [3840/50981 (8%)]\tLoss: 13650404.741935\n",
      "Train Epoch: 17 [5120/50981 (10%)]\tLoss: 13642093.609756\n",
      "Train Epoch: 17 [6400/50981 (13%)]\tLoss: 13647613.254902\n",
      "Train Epoch: 17 [7680/50981 (15%)]\tLoss: 13606161.737705\n",
      "Train Epoch: 17 [8960/50981 (18%)]\tLoss: 13671586.788732\n",
      "Train Epoch: 17 [10240/50981 (20%)]\tLoss: 13708104.407407\n",
      "Train Epoch: 17 [11520/50981 (23%)]\tLoss: 13678232.065934\n",
      "Train Epoch: 17 [12800/50981 (25%)]\tLoss: 13672906.306931\n",
      "Train Epoch: 17 [14080/50981 (28%)]\tLoss: 13661012.081081\n",
      "Train Epoch: 17 [15360/50981 (30%)]\tLoss: 13674228.644628\n",
      "Train Epoch: 17 [16640/50981 (33%)]\tLoss: 13683417.786260\n",
      "Train Epoch: 17 [17920/50981 (35%)]\tLoss: 13686638.489362\n",
      "Train Epoch: 17 [19200/50981 (38%)]\tLoss: 13673621.046358\n",
      "Train Epoch: 17 [20480/50981 (40%)]\tLoss: 13692593.043478\n",
      "Train Epoch: 17 [21760/50981 (43%)]\tLoss: 13721545.789474\n",
      "Train Epoch: 17 [23040/50981 (45%)]\tLoss: 13747539.513812\n",
      "Train Epoch: 17 [24320/50981 (48%)]\tLoss: 13760050.178010\n",
      "Train Epoch: 17 [25600/50981 (50%)]\tLoss: 13770186.149254\n",
      "Train Epoch: 17 [26880/50981 (53%)]\tLoss: 13781808.853081\n",
      "Train Epoch: 17 [28160/50981 (55%)]\tLoss: 13777393.855204\n",
      "Train Epoch: 17 [29440/50981 (58%)]\tLoss: 13781827.666667\n",
      "Train Epoch: 17 [30720/50981 (60%)]\tLoss: 13780545.522822\n",
      "Train Epoch: 17 [32000/50981 (63%)]\tLoss: 13780421.928287\n",
      "Train Epoch: 17 [33280/50981 (65%)]\tLoss: 13798230.444444\n",
      "Train Epoch: 17 [34560/50981 (68%)]\tLoss: 13808341.767528\n",
      "Train Epoch: 17 [35840/50981 (70%)]\tLoss: 13817745.227758\n",
      "Train Epoch: 17 [37120/50981 (73%)]\tLoss: 13831724.402062\n",
      "Train Epoch: 17 [38400/50981 (75%)]\tLoss: 13841371.455150\n",
      "Train Epoch: 17 [39680/50981 (78%)]\tLoss: 13851725.758842\n",
      "Train Epoch: 17 [40960/50981 (80%)]\tLoss: 13858190.968847\n",
      "Train Epoch: 17 [42240/50981 (83%)]\tLoss: 13863472.619335\n",
      "Train Epoch: 17 [43520/50981 (85%)]\tLoss: 13879350.000000\n",
      "Train Epoch: 17 [44800/50981 (88%)]\tLoss: 13872863.660969\n",
      "Train Epoch: 17 [46080/50981 (90%)]\tLoss: 13881759.700831\n",
      "Train Epoch: 17 [47360/50981 (93%)]\tLoss: 13888165.256065\n",
      "Train Epoch: 17 [48640/50981 (95%)]\tLoss: 13893255.047244\n",
      "Train Epoch: 17 [49920/50981 (98%)]\tLoss: 13892657.342711\n",
      "Train Epoch: 18 [0/50981 (0%)]\tLoss: 13430168.000000\n",
      "Train Epoch: 18 [1280/50981 (3%)]\tLoss: 13404902.363636\n",
      "Train Epoch: 18 [2560/50981 (5%)]\tLoss: 13332039.142857\n",
      "Train Epoch: 18 [3840/50981 (8%)]\tLoss: 13322167.225806\n",
      "Train Epoch: 18 [5120/50981 (10%)]\tLoss: 13296983.585366\n",
      "Train Epoch: 18 [6400/50981 (13%)]\tLoss: 13289948.313725\n",
      "Train Epoch: 18 [7680/50981 (15%)]\tLoss: 13320014.524590\n",
      "Train Epoch: 18 [8960/50981 (18%)]\tLoss: 13326420.661972\n",
      "Train Epoch: 18 [10240/50981 (20%)]\tLoss: 13348436.864198\n",
      "Train Epoch: 18 [11520/50981 (23%)]\tLoss: 13331703.351648\n",
      "Train Epoch: 18 [12800/50981 (25%)]\tLoss: 13358408.069307\n",
      "Train Epoch: 18 [14080/50981 (28%)]\tLoss: 13357980.225225\n",
      "Train Epoch: 18 [15360/50981 (30%)]\tLoss: 13339277.719008\n",
      "Train Epoch: 18 [16640/50981 (33%)]\tLoss: 13371709.694656\n",
      "Train Epoch: 18 [17920/50981 (35%)]\tLoss: 13384837.248227\n",
      "Train Epoch: 18 [19200/50981 (38%)]\tLoss: 13399320.788079\n",
      "Train Epoch: 18 [20480/50981 (40%)]\tLoss: 13398662.416149\n",
      "Train Epoch: 18 [21760/50981 (43%)]\tLoss: 13406177.391813\n",
      "Train Epoch: 18 [23040/50981 (45%)]\tLoss: 13443704.872928\n",
      "Train Epoch: 18 [24320/50981 (48%)]\tLoss: 13457179.486911\n",
      "Train Epoch: 18 [25600/50981 (50%)]\tLoss: 13470881.820896\n",
      "Train Epoch: 18 [26880/50981 (53%)]\tLoss: 13485909.483412\n",
      "Train Epoch: 18 [28160/50981 (55%)]\tLoss: 13494509.561086\n",
      "Train Epoch: 18 [29440/50981 (58%)]\tLoss: 13504717.835498\n",
      "Train Epoch: 18 [30720/50981 (60%)]\tLoss: 13516191.651452\n",
      "Train Epoch: 18 [32000/50981 (63%)]\tLoss: 13527720.386454\n",
      "Train Epoch: 18 [33280/50981 (65%)]\tLoss: 13529115.463602\n",
      "Train Epoch: 18 [34560/50981 (68%)]\tLoss: 13521705.180812\n",
      "Train Epoch: 18 [35840/50981 (70%)]\tLoss: 13528748.928826\n",
      "Train Epoch: 18 [37120/50981 (73%)]\tLoss: 13540639.900344\n",
      "Train Epoch: 18 [38400/50981 (75%)]\tLoss: 13556816.043189\n",
      "Train Epoch: 18 [39680/50981 (78%)]\tLoss: 13559992.234727\n",
      "Train Epoch: 18 [40960/50981 (80%)]\tLoss: 13569254.990654\n",
      "Train Epoch: 18 [42240/50981 (83%)]\tLoss: 13575484.570997\n",
      "Train Epoch: 18 [43520/50981 (85%)]\tLoss: 13571128.041056\n",
      "Train Epoch: 18 [44800/50981 (88%)]\tLoss: 13573559.350427\n",
      "Train Epoch: 18 [46080/50981 (90%)]\tLoss: 13580182.792244\n",
      "Train Epoch: 18 [47360/50981 (93%)]\tLoss: 13581600.239892\n",
      "Train Epoch: 18 [48640/50981 (95%)]\tLoss: 13585454.173228\n",
      "Train Epoch: 18 [49920/50981 (98%)]\tLoss: 13591173.874680\n",
      "Train Epoch: 19 [0/50981 (0%)]\tLoss: 13635328.000000\n",
      "Train Epoch: 19 [1280/50981 (3%)]\tLoss: 13129643.090909\n",
      "Train Epoch: 19 [2560/50981 (5%)]\tLoss: 12990942.523810\n",
      "Train Epoch: 19 [3840/50981 (8%)]\tLoss: 12937818.870968\n",
      "Train Epoch: 19 [5120/50981 (10%)]\tLoss: 13002229.902439\n",
      "Train Epoch: 19 [6400/50981 (13%)]\tLoss: 13069296.745098\n",
      "Train Epoch: 19 [7680/50981 (15%)]\tLoss: 13108702.032787\n",
      "Train Epoch: 19 [8960/50981 (18%)]\tLoss: 13115635.507042\n",
      "Train Epoch: 19 [10240/50981 (20%)]\tLoss: 13099829.148148\n",
      "Train Epoch: 19 [11520/50981 (23%)]\tLoss: 13104220.098901\n",
      "Train Epoch: 19 [12800/50981 (25%)]\tLoss: 13102761.148515\n",
      "Train Epoch: 19 [14080/50981 (28%)]\tLoss: 13119145.495495\n",
      "Train Epoch: 19 [15360/50981 (30%)]\tLoss: 13136365.223140\n",
      "Train Epoch: 19 [16640/50981 (33%)]\tLoss: 13155079.923664\n",
      "Train Epoch: 19 [17920/50981 (35%)]\tLoss: 13181475.992908\n",
      "Train Epoch: 19 [19200/50981 (38%)]\tLoss: 13198405.158940\n",
      "Train Epoch: 19 [20480/50981 (40%)]\tLoss: 13180092.372671\n",
      "Train Epoch: 19 [21760/50981 (43%)]\tLoss: 13178618.865497\n",
      "Train Epoch: 19 [23040/50981 (45%)]\tLoss: 13187705.016575\n",
      "Train Epoch: 19 [24320/50981 (48%)]\tLoss: 13201223.748691\n",
      "Train Epoch: 19 [25600/50981 (50%)]\tLoss: 13207707.318408\n",
      "Train Epoch: 19 [26880/50981 (53%)]\tLoss: 13199900.379147\n",
      "Train Epoch: 19 [28160/50981 (55%)]\tLoss: 13196904.914027\n",
      "Train Epoch: 19 [29440/50981 (58%)]\tLoss: 13210277.441558\n",
      "Train Epoch: 19 [30720/50981 (60%)]\tLoss: 13216833.095436\n",
      "Train Epoch: 19 [32000/50981 (63%)]\tLoss: 13217959.000000\n",
      "Train Epoch: 19 [33280/50981 (65%)]\tLoss: 13229059.379310\n",
      "Train Epoch: 19 [34560/50981 (68%)]\tLoss: 13239105.619926\n",
      "Train Epoch: 19 [35840/50981 (70%)]\tLoss: 13258651.686833\n",
      "Train Epoch: 19 [37120/50981 (73%)]\tLoss: 13257493.522337\n",
      "Train Epoch: 19 [38400/50981 (75%)]\tLoss: 13262136.255814\n",
      "Train Epoch: 19 [39680/50981 (78%)]\tLoss: 13264660.655949\n",
      "Train Epoch: 19 [40960/50981 (80%)]\tLoss: 13268630.785047\n",
      "Train Epoch: 19 [42240/50981 (83%)]\tLoss: 13282217.891239\n",
      "Train Epoch: 19 [43520/50981 (85%)]\tLoss: 13287376.354839\n",
      "Train Epoch: 19 [44800/50981 (88%)]\tLoss: 13293538.692308\n",
      "Train Epoch: 19 [46080/50981 (90%)]\tLoss: 13299076.174515\n",
      "Train Epoch: 19 [47360/50981 (93%)]\tLoss: 13305051.234501\n",
      "Train Epoch: 19 [48640/50981 (95%)]\tLoss: 13304181.761155\n",
      "Train Epoch: 19 [49920/50981 (98%)]\tLoss: 13305442.360614\n",
      "Train Epoch: 20 [0/50981 (0%)]\tLoss: 13901899.000000\n",
      "Train Epoch: 20 [1280/50981 (3%)]\tLoss: 13054531.818182\n",
      "Train Epoch: 20 [2560/50981 (5%)]\tLoss: 13080839.333333\n",
      "Train Epoch: 20 [3840/50981 (8%)]\tLoss: 12956149.322581\n",
      "Train Epoch: 20 [5120/50981 (10%)]\tLoss: 12949232.097561\n",
      "Train Epoch: 20 [6400/50981 (13%)]\tLoss: 12905940.980392\n",
      "Train Epoch: 20 [7680/50981 (15%)]\tLoss: 12934026.868852\n",
      "Train Epoch: 20 [8960/50981 (18%)]\tLoss: 12931533.338028\n",
      "Train Epoch: 20 [10240/50981 (20%)]\tLoss: 12952373.172840\n",
      "Train Epoch: 20 [11520/50981 (23%)]\tLoss: 12940472.846154\n",
      "Train Epoch: 20 [12800/50981 (25%)]\tLoss: 12932036.663366\n",
      "Train Epoch: 20 [14080/50981 (28%)]\tLoss: 12911990.504505\n",
      "Train Epoch: 20 [15360/50981 (30%)]\tLoss: 12914770.330579\n",
      "Train Epoch: 20 [16640/50981 (33%)]\tLoss: 12941124.847328\n",
      "Train Epoch: 20 [17920/50981 (35%)]\tLoss: 12932534.787234\n",
      "Train Epoch: 20 [19200/50981 (38%)]\tLoss: 12937139.821192\n",
      "Train Epoch: 20 [20480/50981 (40%)]\tLoss: 12941989.385093\n",
      "Train Epoch: 20 [21760/50981 (43%)]\tLoss: 12948634.327485\n",
      "Train Epoch: 20 [23040/50981 (45%)]\tLoss: 12953180.480663\n",
      "Train Epoch: 20 [24320/50981 (48%)]\tLoss: 12958659.178010\n",
      "Train Epoch: 20 [25600/50981 (50%)]\tLoss: 12957054.039801\n",
      "Train Epoch: 20 [26880/50981 (53%)]\tLoss: 12972108.578199\n",
      "Train Epoch: 20 [28160/50981 (55%)]\tLoss: 12964827.000000\n",
      "Train Epoch: 20 [29440/50981 (58%)]\tLoss: 12972137.961039\n",
      "Train Epoch: 20 [30720/50981 (60%)]\tLoss: 12969383.481328\n",
      "Train Epoch: 20 [32000/50981 (63%)]\tLoss: 12974652.733068\n",
      "Train Epoch: 20 [33280/50981 (65%)]\tLoss: 12978008.494253\n",
      "Train Epoch: 20 [34560/50981 (68%)]\tLoss: 12984605.789668\n",
      "Train Epoch: 20 [35840/50981 (70%)]\tLoss: 12983947.366548\n",
      "Train Epoch: 20 [37120/50981 (73%)]\tLoss: 12996882.367698\n",
      "Train Epoch: 20 [38400/50981 (75%)]\tLoss: 12991636.674419\n",
      "Train Epoch: 20 [39680/50981 (78%)]\tLoss: 12996850.672026\n",
      "Train Epoch: 20 [40960/50981 (80%)]\tLoss: 12994537.065421\n",
      "Train Epoch: 20 [42240/50981 (83%)]\tLoss: 13009940.972810\n",
      "Train Epoch: 20 [43520/50981 (85%)]\tLoss: 13020925.032258\n",
      "Train Epoch: 20 [44800/50981 (88%)]\tLoss: 13018418.911681\n",
      "Train Epoch: 20 [46080/50981 (90%)]\tLoss: 13026897.728532\n",
      "Train Epoch: 20 [47360/50981 (93%)]\tLoss: 13038079.309973\n",
      "Train Epoch: 20 [48640/50981 (95%)]\tLoss: 13039107.367454\n",
      "Train Epoch: 20 [49920/50981 (98%)]\tLoss: 13040862.959079\n",
      "Train Epoch: 21 [0/50981 (0%)]\tLoss: 13343346.000000\n",
      "Train Epoch: 21 [1280/50981 (3%)]\tLoss: 12667718.090909\n",
      "Train Epoch: 21 [2560/50981 (5%)]\tLoss: 12454372.571429\n",
      "Train Epoch: 21 [3840/50981 (8%)]\tLoss: 12486802.290323\n",
      "Train Epoch: 21 [5120/50981 (10%)]\tLoss: 12562305.097561\n",
      "Train Epoch: 21 [6400/50981 (13%)]\tLoss: 12515056.058824\n",
      "Train Epoch: 21 [7680/50981 (15%)]\tLoss: 12576051.524590\n",
      "Train Epoch: 21 [8960/50981 (18%)]\tLoss: 12602121.014085\n",
      "Train Epoch: 21 [10240/50981 (20%)]\tLoss: 12601030.135802\n",
      "Train Epoch: 21 [11520/50981 (23%)]\tLoss: 12607740.461538\n",
      "Train Epoch: 21 [12800/50981 (25%)]\tLoss: 12616965.405941\n",
      "Train Epoch: 21 [14080/50981 (28%)]\tLoss: 12613302.090090\n",
      "Train Epoch: 21 [15360/50981 (30%)]\tLoss: 12600708.099174\n",
      "Train Epoch: 21 [16640/50981 (33%)]\tLoss: 12594499.045802\n",
      "Train Epoch: 21 [17920/50981 (35%)]\tLoss: 12616089.411348\n",
      "Train Epoch: 21 [19200/50981 (38%)]\tLoss: 12604843.556291\n",
      "Train Epoch: 21 [20480/50981 (40%)]\tLoss: 12614169.639752\n",
      "Train Epoch: 21 [21760/50981 (43%)]\tLoss: 12601235.081871\n",
      "Train Epoch: 21 [23040/50981 (45%)]\tLoss: 12612297.773481\n",
      "Train Epoch: 21 [24320/50981 (48%)]\tLoss: 12634152.020942\n",
      "Train Epoch: 21 [25600/50981 (50%)]\tLoss: 12638175.895522\n",
      "Train Epoch: 21 [26880/50981 (53%)]\tLoss: 12654745.781991\n",
      "Train Epoch: 21 [28160/50981 (55%)]\tLoss: 12665355.162896\n",
      "Train Epoch: 21 [29440/50981 (58%)]\tLoss: 12674726.982684\n",
      "Train Epoch: 21 [30720/50981 (60%)]\tLoss: 12688206.195021\n",
      "Train Epoch: 21 [32000/50981 (63%)]\tLoss: 12690272.597610\n",
      "Train Epoch: 21 [33280/50981 (65%)]\tLoss: 12706708.486590\n",
      "Train Epoch: 21 [34560/50981 (68%)]\tLoss: 12715275.848708\n",
      "Train Epoch: 21 [35840/50981 (70%)]\tLoss: 12717575.718861\n",
      "Train Epoch: 21 [37120/50981 (73%)]\tLoss: 12724160.329897\n",
      "Train Epoch: 21 [38400/50981 (75%)]\tLoss: 12732118.205980\n",
      "Train Epoch: 21 [39680/50981 (78%)]\tLoss: 12739045.366559\n",
      "Train Epoch: 21 [40960/50981 (80%)]\tLoss: 12754421.345794\n",
      "Train Epoch: 21 [42240/50981 (83%)]\tLoss: 12759072.329305\n",
      "Train Epoch: 21 [43520/50981 (85%)]\tLoss: 12764673.202346\n",
      "Train Epoch: 21 [44800/50981 (88%)]\tLoss: 12764669.846154\n",
      "Train Epoch: 21 [46080/50981 (90%)]\tLoss: 12772082.335180\n",
      "Train Epoch: 21 [47360/50981 (93%)]\tLoss: 12776498.625337\n",
      "Train Epoch: 21 [48640/50981 (95%)]\tLoss: 12790222.640420\n",
      "Train Epoch: 21 [49920/50981 (98%)]\tLoss: 12797627.923274\n",
      "Train Epoch: 22 [0/50981 (0%)]\tLoss: 12753902.000000\n",
      "Train Epoch: 22 [1280/50981 (3%)]\tLoss: 12266053.181818\n",
      "Train Epoch: 22 [2560/50981 (5%)]\tLoss: 12467989.619048\n",
      "Train Epoch: 22 [3840/50981 (8%)]\tLoss: 12444640.709677\n",
      "Train Epoch: 22 [5120/50981 (10%)]\tLoss: 12407783.219512\n",
      "Train Epoch: 22 [6400/50981 (13%)]\tLoss: 12469713.392157\n",
      "Train Epoch: 22 [7680/50981 (15%)]\tLoss: 12459497.377049\n",
      "Train Epoch: 22 [8960/50981 (18%)]\tLoss: 12461912.591549\n",
      "Train Epoch: 22 [10240/50981 (20%)]\tLoss: 12470558.913580\n",
      "Train Epoch: 22 [11520/50981 (23%)]\tLoss: 12465039.483516\n",
      "Train Epoch: 22 [12800/50981 (25%)]\tLoss: 12456245.158416\n",
      "Train Epoch: 22 [14080/50981 (28%)]\tLoss: 12445190.108108\n",
      "Train Epoch: 22 [15360/50981 (30%)]\tLoss: 12462643.057851\n",
      "Train Epoch: 22 [16640/50981 (33%)]\tLoss: 12478445.519084\n",
      "Train Epoch: 22 [17920/50981 (35%)]\tLoss: 12483408.446809\n",
      "Train Epoch: 22 [19200/50981 (38%)]\tLoss: 12494992.688742\n",
      "Train Epoch: 22 [20480/50981 (40%)]\tLoss: 12505689.583851\n",
      "Train Epoch: 22 [21760/50981 (43%)]\tLoss: 12495153.064327\n",
      "Train Epoch: 22 [23040/50981 (45%)]\tLoss: 12496014.513812\n",
      "Train Epoch: 22 [24320/50981 (48%)]\tLoss: 12495517.701571\n",
      "Train Epoch: 22 [25600/50981 (50%)]\tLoss: 12505596.805970\n",
      "Train Epoch: 22 [26880/50981 (53%)]\tLoss: 12496780.199052\n",
      "Train Epoch: 22 [28160/50981 (55%)]\tLoss: 12496914.321267\n",
      "Train Epoch: 22 [29440/50981 (58%)]\tLoss: 12492597.324675\n",
      "Train Epoch: 22 [30720/50981 (60%)]\tLoss: 12500776.622407\n",
      "Train Epoch: 22 [32000/50981 (63%)]\tLoss: 12509374.653386\n",
      "Train Epoch: 22 [33280/50981 (65%)]\tLoss: 12511025.869732\n",
      "Train Epoch: 22 [34560/50981 (68%)]\tLoss: 12516488.472325\n",
      "Train Epoch: 22 [35840/50981 (70%)]\tLoss: 12519584.263345\n",
      "Train Epoch: 22 [37120/50981 (73%)]\tLoss: 12530483.034364\n",
      "Train Epoch: 22 [38400/50981 (75%)]\tLoss: 12537212.318937\n",
      "Train Epoch: 22 [39680/50981 (78%)]\tLoss: 12554432.501608\n",
      "Train Epoch: 22 [40960/50981 (80%)]\tLoss: 12559208.841121\n",
      "Train Epoch: 22 [42240/50981 (83%)]\tLoss: 12565727.604230\n",
      "Train Epoch: 22 [43520/50981 (85%)]\tLoss: 12569724.985337\n",
      "Train Epoch: 22 [44800/50981 (88%)]\tLoss: 12572120.985755\n",
      "Train Epoch: 22 [46080/50981 (90%)]\tLoss: 12576031.390582\n",
      "Train Epoch: 22 [47360/50981 (93%)]\tLoss: 12583426.105121\n",
      "Train Epoch: 22 [48640/50981 (95%)]\tLoss: 12581875.238845\n",
      "Train Epoch: 22 [49920/50981 (98%)]\tLoss: 12586882.281330\n",
      "Train Epoch: 23 [0/50981 (0%)]\tLoss: 12967601.000000\n",
      "Train Epoch: 23 [1280/50981 (3%)]\tLoss: 12124579.454545\n",
      "Train Epoch: 23 [2560/50981 (5%)]\tLoss: 12037455.333333\n",
      "Train Epoch: 23 [3840/50981 (8%)]\tLoss: 12138419.483871\n",
      "Train Epoch: 23 [5120/50981 (10%)]\tLoss: 12153941.658537\n",
      "Train Epoch: 23 [6400/50981 (13%)]\tLoss: 12117483.352941\n",
      "Train Epoch: 23 [7680/50981 (15%)]\tLoss: 12120971.885246\n",
      "Train Epoch: 23 [8960/50981 (18%)]\tLoss: 12120828.197183\n",
      "Train Epoch: 23 [10240/50981 (20%)]\tLoss: 12167133.654321\n",
      "Train Epoch: 23 [11520/50981 (23%)]\tLoss: 12216969.593407\n",
      "Train Epoch: 23 [12800/50981 (25%)]\tLoss: 12231715.722772\n",
      "Train Epoch: 23 [14080/50981 (28%)]\tLoss: 12229522.990991\n",
      "Train Epoch: 23 [15360/50981 (30%)]\tLoss: 12225689.793388\n",
      "Train Epoch: 23 [16640/50981 (33%)]\tLoss: 12208412.648855\n",
      "Train Epoch: 23 [17920/50981 (35%)]\tLoss: 12215950.198582\n",
      "Train Epoch: 23 [19200/50981 (38%)]\tLoss: 12210237.251656\n",
      "Train Epoch: 23 [20480/50981 (40%)]\tLoss: 12215346.093168\n",
      "Train Epoch: 23 [21760/50981 (43%)]\tLoss: 12203679.625731\n",
      "Train Epoch: 23 [23040/50981 (45%)]\tLoss: 12208982.436464\n",
      "Train Epoch: 23 [24320/50981 (48%)]\tLoss: 12222343.308901\n",
      "Train Epoch: 23 [25600/50981 (50%)]\tLoss: 12232577.353234\n",
      "Train Epoch: 23 [26880/50981 (53%)]\tLoss: 12243675.345972\n",
      "Train Epoch: 23 [28160/50981 (55%)]\tLoss: 12254027.565611\n",
      "Train Epoch: 23 [29440/50981 (58%)]\tLoss: 12275810.943723\n",
      "Train Epoch: 23 [30720/50981 (60%)]\tLoss: 12283203.813278\n",
      "Train Epoch: 23 [32000/50981 (63%)]\tLoss: 12295990.832669\n",
      "Train Epoch: 23 [33280/50981 (65%)]\tLoss: 12302400.409962\n",
      "Train Epoch: 23 [34560/50981 (68%)]\tLoss: 12296082.601476\n",
      "Train Epoch: 23 [35840/50981 (70%)]\tLoss: 12303349.327402\n",
      "Train Epoch: 23 [37120/50981 (73%)]\tLoss: 12318170.182131\n",
      "Train Epoch: 23 [38400/50981 (75%)]\tLoss: 12333120.564784\n",
      "Train Epoch: 23 [39680/50981 (78%)]\tLoss: 12338491.218650\n",
      "Train Epoch: 23 [40960/50981 (80%)]\tLoss: 12328182.071651\n",
      "Train Epoch: 23 [42240/50981 (83%)]\tLoss: 12335803.305136\n",
      "Train Epoch: 23 [43520/50981 (85%)]\tLoss: 12341638.744868\n",
      "Train Epoch: 23 [44800/50981 (88%)]\tLoss: 12355671.008547\n",
      "Train Epoch: 23 [46080/50981 (90%)]\tLoss: 12360784.290859\n",
      "Train Epoch: 23 [47360/50981 (93%)]\tLoss: 12360710.681941\n",
      "Train Epoch: 23 [48640/50981 (95%)]\tLoss: 12365919.073491\n",
      "Train Epoch: 23 [49920/50981 (98%)]\tLoss: 12373331.690537\n",
      "Train Epoch: 24 [0/50981 (0%)]\tLoss: 12331037.000000\n",
      "Train Epoch: 24 [1280/50981 (3%)]\tLoss: 11829613.727273\n",
      "Train Epoch: 24 [2560/50981 (5%)]\tLoss: 11842662.809524\n",
      "Train Epoch: 24 [3840/50981 (8%)]\tLoss: 11957697.677419\n",
      "Train Epoch: 24 [5120/50981 (10%)]\tLoss: 11994785.609756\n",
      "Train Epoch: 24 [6400/50981 (13%)]\tLoss: 12023718.509804\n",
      "Train Epoch: 24 [7680/50981 (15%)]\tLoss: 12004519.327869\n",
      "Train Epoch: 24 [8960/50981 (18%)]\tLoss: 12022523.873239\n",
      "Train Epoch: 24 [10240/50981 (20%)]\tLoss: 12032425.592593\n",
      "Train Epoch: 24 [11520/50981 (23%)]\tLoss: 12046998.945055\n",
      "Train Epoch: 24 [12800/50981 (25%)]\tLoss: 12058146.990099\n",
      "Train Epoch: 24 [14080/50981 (28%)]\tLoss: 12044336.081081\n",
      "Train Epoch: 24 [15360/50981 (30%)]\tLoss: 12065123.099174\n",
      "Train Epoch: 24 [16640/50981 (33%)]\tLoss: 12088303.099237\n",
      "Train Epoch: 24 [17920/50981 (35%)]\tLoss: 12084575.269504\n",
      "Train Epoch: 24 [19200/50981 (38%)]\tLoss: 12087823.814570\n",
      "Train Epoch: 24 [20480/50981 (40%)]\tLoss: 12092861.913043\n",
      "Train Epoch: 24 [21760/50981 (43%)]\tLoss: 12081094.245614\n",
      "Train Epoch: 24 [23040/50981 (45%)]\tLoss: 12089147.624309\n",
      "Train Epoch: 24 [24320/50981 (48%)]\tLoss: 12078376.968586\n",
      "Train Epoch: 24 [25600/50981 (50%)]\tLoss: 12100652.482587\n",
      "Train Epoch: 24 [26880/50981 (53%)]\tLoss: 12110594.573460\n",
      "Train Epoch: 24 [28160/50981 (55%)]\tLoss: 12118804.158371\n",
      "Train Epoch: 24 [29440/50981 (58%)]\tLoss: 12127781.372294\n",
      "Train Epoch: 24 [30720/50981 (60%)]\tLoss: 12126449.323651\n",
      "Train Epoch: 24 [32000/50981 (63%)]\tLoss: 12118635.215139\n",
      "Train Epoch: 24 [33280/50981 (65%)]\tLoss: 12111557.226054\n",
      "Train Epoch: 24 [34560/50981 (68%)]\tLoss: 12110971.225092\n",
      "Train Epoch: 24 [35840/50981 (70%)]\tLoss: 12119532.359431\n",
      "Train Epoch: 24 [37120/50981 (73%)]\tLoss: 12119706.491409\n",
      "Train Epoch: 24 [38400/50981 (75%)]\tLoss: 12132375.704319\n",
      "Train Epoch: 24 [39680/50981 (78%)]\tLoss: 12140554.382637\n",
      "Train Epoch: 24 [40960/50981 (80%)]\tLoss: 12142173.841121\n",
      "Train Epoch: 24 [42240/50981 (83%)]\tLoss: 12148822.347432\n",
      "Train Epoch: 24 [43520/50981 (85%)]\tLoss: 12152532.076246\n",
      "Train Epoch: 24 [44800/50981 (88%)]\tLoss: 12158274.376068\n",
      "Train Epoch: 24 [46080/50981 (90%)]\tLoss: 12162636.875346\n",
      "Train Epoch: 24 [47360/50981 (93%)]\tLoss: 12165006.469003\n",
      "Train Epoch: 24 [48640/50981 (95%)]\tLoss: 12166079.703412\n",
      "Train Epoch: 24 [49920/50981 (98%)]\tLoss: 12165424.347826\n",
      "Train Epoch: 25 [0/50981 (0%)]\tLoss: 10490794.000000\n",
      "Train Epoch: 25 [1280/50981 (3%)]\tLoss: 11575196.636364\n",
      "Train Epoch: 25 [2560/50981 (5%)]\tLoss: 11565322.380952\n",
      "Train Epoch: 25 [3840/50981 (8%)]\tLoss: 11672708.322581\n",
      "Train Epoch: 25 [5120/50981 (10%)]\tLoss: 11706927.024390\n",
      "Train Epoch: 25 [6400/50981 (13%)]\tLoss: 11709863.843137\n",
      "Train Epoch: 25 [7680/50981 (15%)]\tLoss: 11758769.098361\n",
      "Train Epoch: 25 [8960/50981 (18%)]\tLoss: 11739937.225352\n",
      "Train Epoch: 25 [10240/50981 (20%)]\tLoss: 11750116.407407\n",
      "Train Epoch: 25 [11520/50981 (23%)]\tLoss: 11744672.109890\n",
      "Train Epoch: 25 [12800/50981 (25%)]\tLoss: 11778728.287129\n",
      "Train Epoch: 25 [14080/50981 (28%)]\tLoss: 11788189.351351\n",
      "Train Epoch: 25 [15360/50981 (30%)]\tLoss: 11820255.239669\n",
      "Train Epoch: 25 [16640/50981 (33%)]\tLoss: 11828175.900763\n",
      "Train Epoch: 25 [17920/50981 (35%)]\tLoss: 11825716.113475\n",
      "Train Epoch: 25 [19200/50981 (38%)]\tLoss: 11837617.609272\n",
      "Train Epoch: 25 [20480/50981 (40%)]\tLoss: 11854842.478261\n",
      "Train Epoch: 25 [21760/50981 (43%)]\tLoss: 11862128.532164\n",
      "Train Epoch: 25 [23040/50981 (45%)]\tLoss: 11867419.850829\n",
      "Train Epoch: 25 [24320/50981 (48%)]\tLoss: 11873983.010471\n",
      "Train Epoch: 25 [25600/50981 (50%)]\tLoss: 11878416.268657\n",
      "Train Epoch: 25 [26880/50981 (53%)]\tLoss: 11891821.317536\n",
      "Train Epoch: 25 [28160/50981 (55%)]\tLoss: 11908803.769231\n",
      "Train Epoch: 25 [29440/50981 (58%)]\tLoss: 11922681.658009\n",
      "Train Epoch: 25 [30720/50981 (60%)]\tLoss: 11920963.132780\n",
      "Train Epoch: 25 [32000/50981 (63%)]\tLoss: 11934362.605578\n",
      "Train Epoch: 25 [33280/50981 (65%)]\tLoss: 11929205.908046\n",
      "Train Epoch: 25 [34560/50981 (68%)]\tLoss: 11931581.394834\n",
      "Train Epoch: 25 [35840/50981 (70%)]\tLoss: 11936321.092527\n",
      "Train Epoch: 25 [37120/50981 (73%)]\tLoss: 11937260.924399\n",
      "Train Epoch: 25 [38400/50981 (75%)]\tLoss: 11942525.960133\n",
      "Train Epoch: 25 [39680/50981 (78%)]\tLoss: 11958713.151125\n",
      "Train Epoch: 25 [40960/50981 (80%)]\tLoss: 11962759.367601\n",
      "Train Epoch: 25 [42240/50981 (83%)]\tLoss: 11967910.468278\n",
      "Train Epoch: 25 [43520/50981 (85%)]\tLoss: 11969369.058651\n",
      "Train Epoch: 25 [44800/50981 (88%)]\tLoss: 11965762.512821\n",
      "Train Epoch: 25 [46080/50981 (90%)]\tLoss: 11969758.578947\n",
      "Train Epoch: 25 [47360/50981 (93%)]\tLoss: 11969481.684636\n",
      "Train Epoch: 25 [48640/50981 (95%)]\tLoss: 11968311.593176\n",
      "Train Epoch: 25 [49920/50981 (98%)]\tLoss: 11971285.194373\n",
      "Train Epoch: 26 [0/50981 (0%)]\tLoss: 10875996.000000\n",
      "Train Epoch: 26 [1280/50981 (3%)]\tLoss: 11573004.181818\n",
      "Train Epoch: 26 [2560/50981 (5%)]\tLoss: 11535272.142857\n",
      "Train Epoch: 26 [3840/50981 (8%)]\tLoss: 11453156.870968\n",
      "Train Epoch: 26 [5120/50981 (10%)]\tLoss: 11458677.268293\n",
      "Train Epoch: 26 [6400/50981 (13%)]\tLoss: 11523586.549020\n",
      "Train Epoch: 26 [7680/50981 (15%)]\tLoss: 11570111.213115\n",
      "Train Epoch: 26 [8960/50981 (18%)]\tLoss: 11537406.028169\n",
      "Train Epoch: 26 [10240/50981 (20%)]\tLoss: 11574846.419753\n",
      "Train Epoch: 26 [11520/50981 (23%)]\tLoss: 11586553.010989\n",
      "Train Epoch: 26 [12800/50981 (25%)]\tLoss: 11604926.287129\n",
      "Train Epoch: 26 [14080/50981 (28%)]\tLoss: 11628613.090090\n",
      "Train Epoch: 26 [15360/50981 (30%)]\tLoss: 11659454.900826\n",
      "Train Epoch: 26 [16640/50981 (33%)]\tLoss: 11664255.412214\n",
      "Train Epoch: 26 [17920/50981 (35%)]\tLoss: 11652513.517730\n",
      "Train Epoch: 26 [19200/50981 (38%)]\tLoss: 11668278.602649\n",
      "Train Epoch: 26 [20480/50981 (40%)]\tLoss: 11661783.422360\n",
      "Train Epoch: 26 [21760/50981 (43%)]\tLoss: 11668473.964912\n",
      "Train Epoch: 26 [23040/50981 (45%)]\tLoss: 11688877.613260\n",
      "Train Epoch: 26 [24320/50981 (48%)]\tLoss: 11704337.340314\n",
      "Train Epoch: 26 [25600/50981 (50%)]\tLoss: 11699164.711443\n",
      "Train Epoch: 26 [26880/50981 (53%)]\tLoss: 11698506.199052\n",
      "Train Epoch: 26 [28160/50981 (55%)]\tLoss: 11702403.981900\n",
      "Train Epoch: 26 [29440/50981 (58%)]\tLoss: 11704846.251082\n",
      "Train Epoch: 26 [30720/50981 (60%)]\tLoss: 11712004.717842\n",
      "Train Epoch: 26 [32000/50981 (63%)]\tLoss: 11708530.697211\n",
      "Train Epoch: 26 [33280/50981 (65%)]\tLoss: 11702014.513410\n",
      "Train Epoch: 26 [34560/50981 (68%)]\tLoss: 11704770.014760\n",
      "Train Epoch: 26 [35840/50981 (70%)]\tLoss: 11711556.323843\n",
      "Train Epoch: 26 [37120/50981 (73%)]\tLoss: 11719422.037801\n",
      "Train Epoch: 26 [38400/50981 (75%)]\tLoss: 11740449.318937\n",
      "Train Epoch: 26 [39680/50981 (78%)]\tLoss: 11749765.003215\n",
      "Train Epoch: 26 [40960/50981 (80%)]\tLoss: 11755475.436137\n",
      "Train Epoch: 26 [42240/50981 (83%)]\tLoss: 11769130.075529\n",
      "Train Epoch: 26 [43520/50981 (85%)]\tLoss: 11777302.580645\n",
      "Train Epoch: 26 [44800/50981 (88%)]\tLoss: 11788682.247863\n",
      "Train Epoch: 26 [46080/50981 (90%)]\tLoss: 11795891.858726\n",
      "Train Epoch: 26 [47360/50981 (93%)]\tLoss: 11799359.859838\n",
      "Train Epoch: 26 [48640/50981 (95%)]\tLoss: 11809995.110236\n",
      "Train Epoch: 26 [49920/50981 (98%)]\tLoss: 11816525.654731\n",
      "Train Epoch: 27 [0/50981 (0%)]\tLoss: 12005366.000000\n",
      "Train Epoch: 27 [1280/50981 (3%)]\tLoss: 11665929.272727\n",
      "Train Epoch: 27 [2560/50981 (5%)]\tLoss: 11554631.000000\n",
      "Train Epoch: 27 [3840/50981 (8%)]\tLoss: 11556085.870968\n",
      "Train Epoch: 27 [5120/50981 (10%)]\tLoss: 11441118.682927\n",
      "Train Epoch: 27 [6400/50981 (13%)]\tLoss: 11402611.803922\n",
      "Train Epoch: 27 [7680/50981 (15%)]\tLoss: 11395293.180328\n",
      "Train Epoch: 27 [8960/50981 (18%)]\tLoss: 11412518.225352\n",
      "Train Epoch: 27 [10240/50981 (20%)]\tLoss: 11423022.172840\n",
      "Train Epoch: 27 [11520/50981 (23%)]\tLoss: 11476596.263736\n",
      "Train Epoch: 27 [12800/50981 (25%)]\tLoss: 11477949.217822\n",
      "Train Epoch: 27 [14080/50981 (28%)]\tLoss: 11479722.423423\n",
      "Train Epoch: 27 [15360/50981 (30%)]\tLoss: 11511196.595041\n",
      "Train Epoch: 27 [16640/50981 (33%)]\tLoss: 11487157.442748\n",
      "Train Epoch: 27 [17920/50981 (35%)]\tLoss: 11498360.177305\n",
      "Train Epoch: 27 [19200/50981 (38%)]\tLoss: 11493517.152318\n",
      "Train Epoch: 27 [20480/50981 (40%)]\tLoss: 11510356.291925\n",
      "Train Epoch: 27 [21760/50981 (43%)]\tLoss: 11527498.251462\n",
      "Train Epoch: 27 [23040/50981 (45%)]\tLoss: 11527073.337017\n",
      "Train Epoch: 27 [24320/50981 (48%)]\tLoss: 11514255.780105\n",
      "Train Epoch: 27 [25600/50981 (50%)]\tLoss: 11528584.099502\n",
      "Train Epoch: 27 [26880/50981 (53%)]\tLoss: 11531167.507109\n",
      "Train Epoch: 27 [28160/50981 (55%)]\tLoss: 11546270.402715\n",
      "Train Epoch: 27 [29440/50981 (58%)]\tLoss: 11551356.982684\n",
      "Train Epoch: 27 [30720/50981 (60%)]\tLoss: 11571082.995851\n",
      "Train Epoch: 27 [32000/50981 (63%)]\tLoss: 11581098.023904\n",
      "Train Epoch: 27 [33280/50981 (65%)]\tLoss: 11593676.973180\n",
      "Train Epoch: 27 [34560/50981 (68%)]\tLoss: 11597022.487085\n",
      "Train Epoch: 27 [35840/50981 (70%)]\tLoss: 11597606.124555\n",
      "Train Epoch: 27 [37120/50981 (73%)]\tLoss: 11605349.780069\n",
      "Train Epoch: 27 [38400/50981 (75%)]\tLoss: 11612319.355482\n",
      "Train Epoch: 27 [39680/50981 (78%)]\tLoss: 11615109.385852\n",
      "Train Epoch: 27 [40960/50981 (80%)]\tLoss: 11617335.710280\n",
      "Train Epoch: 27 [42240/50981 (83%)]\tLoss: 11620105.845921\n",
      "Train Epoch: 27 [43520/50981 (85%)]\tLoss: 11623024.747801\n",
      "Train Epoch: 27 [44800/50981 (88%)]\tLoss: 11618943.279202\n",
      "Train Epoch: 27 [46080/50981 (90%)]\tLoss: 11627328.686981\n",
      "Train Epoch: 27 [47360/50981 (93%)]\tLoss: 11640241.698113\n",
      "Train Epoch: 27 [48640/50981 (95%)]\tLoss: 11648608.144357\n",
      "Train Epoch: 27 [49920/50981 (98%)]\tLoss: 11652102.700767\n",
      "Train Epoch: 28 [0/50981 (0%)]\tLoss: 10844724.000000\n",
      "Train Epoch: 28 [1280/50981 (3%)]\tLoss: 11262385.272727\n",
      "Train Epoch: 28 [2560/50981 (5%)]\tLoss: 11320897.523810\n",
      "Train Epoch: 28 [3840/50981 (8%)]\tLoss: 11359538.000000\n",
      "Train Epoch: 28 [5120/50981 (10%)]\tLoss: 11433035.731707\n",
      "Train Epoch: 28 [6400/50981 (13%)]\tLoss: 11376666.470588\n",
      "Train Epoch: 28 [7680/50981 (15%)]\tLoss: 11385168.852459\n",
      "Train Epoch: 28 [8960/50981 (18%)]\tLoss: 11375710.901408\n",
      "Train Epoch: 28 [10240/50981 (20%)]\tLoss: 11357837.098765\n",
      "Train Epoch: 28 [11520/50981 (23%)]\tLoss: 11356830.527473\n",
      "Train Epoch: 28 [12800/50981 (25%)]\tLoss: 11354314.831683\n",
      "Train Epoch: 28 [14080/50981 (28%)]\tLoss: 11350787.414414\n",
      "Train Epoch: 28 [15360/50981 (30%)]\tLoss: 11349986.595041\n",
      "Train Epoch: 28 [16640/50981 (33%)]\tLoss: 11361498.030534\n",
      "Train Epoch: 28 [17920/50981 (35%)]\tLoss: 11371025.085106\n",
      "Train Epoch: 28 [19200/50981 (38%)]\tLoss: 11367917.794702\n",
      "Train Epoch: 28 [20480/50981 (40%)]\tLoss: 11390765.385093\n",
      "Train Epoch: 28 [21760/50981 (43%)]\tLoss: 11414912.684211\n",
      "Train Epoch: 28 [23040/50981 (45%)]\tLoss: 11430651.000000\n",
      "Train Epoch: 28 [24320/50981 (48%)]\tLoss: 11428036.973822\n",
      "Train Epoch: 28 [25600/50981 (50%)]\tLoss: 11426132.009950\n",
      "Train Epoch: 28 [26880/50981 (53%)]\tLoss: 11427387.047393\n",
      "Train Epoch: 28 [28160/50981 (55%)]\tLoss: 11425071.515837\n",
      "Train Epoch: 28 [29440/50981 (58%)]\tLoss: 11418442.969697\n",
      "Train Epoch: 28 [30720/50981 (60%)]\tLoss: 11430251.435685\n",
      "Train Epoch: 28 [32000/50981 (63%)]\tLoss: 11433614.075697\n",
      "Train Epoch: 28 [33280/50981 (65%)]\tLoss: 11436793.191571\n",
      "Train Epoch: 28 [34560/50981 (68%)]\tLoss: 11443636.055351\n",
      "Train Epoch: 28 [35840/50981 (70%)]\tLoss: 11440252.950178\n",
      "Train Epoch: 28 [37120/50981 (73%)]\tLoss: 11445463.735395\n",
      "Train Epoch: 28 [38400/50981 (75%)]\tLoss: 11446219.112957\n",
      "Train Epoch: 28 [39680/50981 (78%)]\tLoss: 11447142.115756\n",
      "Train Epoch: 28 [40960/50981 (80%)]\tLoss: 11449931.679128\n",
      "Train Epoch: 28 [42240/50981 (83%)]\tLoss: 11454897.972810\n",
      "Train Epoch: 28 [43520/50981 (85%)]\tLoss: 11456467.563050\n",
      "Train Epoch: 28 [44800/50981 (88%)]\tLoss: 11466100.527066\n",
      "Train Epoch: 28 [46080/50981 (90%)]\tLoss: 11478716.789474\n",
      "Train Epoch: 28 [47360/50981 (93%)]\tLoss: 11480837.819407\n",
      "Train Epoch: 28 [48640/50981 (95%)]\tLoss: 11492477.779528\n",
      "Train Epoch: 28 [49920/50981 (98%)]\tLoss: 11497822.534527\n",
      "Train Epoch: 29 [0/50981 (0%)]\tLoss: 11007097.000000\n",
      "Train Epoch: 29 [1280/50981 (3%)]\tLoss: 11021866.090909\n",
      "Train Epoch: 29 [2560/50981 (5%)]\tLoss: 11096186.380952\n",
      "Train Epoch: 29 [3840/50981 (8%)]\tLoss: 11053763.064516\n",
      "Train Epoch: 29 [5120/50981 (10%)]\tLoss: 11069440.512195\n",
      "Train Epoch: 29 [6400/50981 (13%)]\tLoss: 11136921.333333\n",
      "Train Epoch: 29 [7680/50981 (15%)]\tLoss: 11120872.590164\n",
      "Train Epoch: 29 [8960/50981 (18%)]\tLoss: 11112386.985915\n",
      "Train Epoch: 29 [10240/50981 (20%)]\tLoss: 11128034.308642\n",
      "Train Epoch: 29 [11520/50981 (23%)]\tLoss: 11106545.439560\n",
      "Train Epoch: 29 [12800/50981 (25%)]\tLoss: 11141788.910891\n",
      "Train Epoch: 29 [14080/50981 (28%)]\tLoss: 11135468.954955\n",
      "Train Epoch: 29 [15360/50981 (30%)]\tLoss: 11149878.214876\n",
      "Train Epoch: 29 [16640/50981 (33%)]\tLoss: 11136790.068702\n",
      "Train Epoch: 29 [17920/50981 (35%)]\tLoss: 11150045.546099\n",
      "Train Epoch: 29 [19200/50981 (38%)]\tLoss: 11154945.602649\n",
      "Train Epoch: 29 [20480/50981 (40%)]\tLoss: 11162593.844720\n",
      "Train Epoch: 29 [21760/50981 (43%)]\tLoss: 11175944.783626\n",
      "Train Epoch: 29 [23040/50981 (45%)]\tLoss: 11178384.254144\n",
      "Train Epoch: 29 [24320/50981 (48%)]\tLoss: 11182145.628272\n",
      "Train Epoch: 29 [25600/50981 (50%)]\tLoss: 11187426.925373\n",
      "Train Epoch: 29 [26880/50981 (53%)]\tLoss: 11196304.516588\n",
      "Train Epoch: 29 [28160/50981 (55%)]\tLoss: 11201895.981900\n",
      "Train Epoch: 29 [29440/50981 (58%)]\tLoss: 11215139.424242\n",
      "Train Epoch: 29 [30720/50981 (60%)]\tLoss: 11227804.468880\n",
      "Train Epoch: 29 [32000/50981 (63%)]\tLoss: 11232357.294821\n",
      "Train Epoch: 29 [33280/50981 (65%)]\tLoss: 11243060.708812\n",
      "Train Epoch: 29 [34560/50981 (68%)]\tLoss: 11250999.361624\n",
      "Train Epoch: 29 [35840/50981 (70%)]\tLoss: 11255334.708185\n",
      "Train Epoch: 29 [37120/50981 (73%)]\tLoss: 11263961.721649\n",
      "Train Epoch: 29 [38400/50981 (75%)]\tLoss: 11274546.029900\n",
      "Train Epoch: 29 [39680/50981 (78%)]\tLoss: 11282551.874598\n",
      "Train Epoch: 29 [40960/50981 (80%)]\tLoss: 11285724.850467\n",
      "Train Epoch: 29 [42240/50981 (83%)]\tLoss: 11295507.731118\n",
      "Train Epoch: 29 [43520/50981 (85%)]\tLoss: 11300042.771261\n",
      "Train Epoch: 29 [44800/50981 (88%)]\tLoss: 11307496.840456\n",
      "Train Epoch: 29 [46080/50981 (90%)]\tLoss: 11319847.487535\n",
      "Train Epoch: 29 [47360/50981 (93%)]\tLoss: 11327435.843666\n",
      "Train Epoch: 29 [48640/50981 (95%)]\tLoss: 11337309.204724\n",
      "Train Epoch: 29 [49920/50981 (98%)]\tLoss: 11346254.874680\n",
      "Train Epoch: 30 [0/50981 (0%)]\tLoss: 10916335.000000\n",
      "Train Epoch: 30 [1280/50981 (3%)]\tLoss: 11000466.545455\n",
      "Train Epoch: 30 [2560/50981 (5%)]\tLoss: 10881946.952381\n",
      "Train Epoch: 30 [3840/50981 (8%)]\tLoss: 10953071.419355\n",
      "Train Epoch: 30 [5120/50981 (10%)]\tLoss: 10920999.585366\n",
      "Train Epoch: 30 [6400/50981 (13%)]\tLoss: 10945605.000000\n",
      "Train Epoch: 30 [7680/50981 (15%)]\tLoss: 10936447.524590\n",
      "Train Epoch: 30 [8960/50981 (18%)]\tLoss: 10940729.746479\n",
      "Train Epoch: 30 [10240/50981 (20%)]\tLoss: 10942281.493827\n",
      "Train Epoch: 30 [11520/50981 (23%)]\tLoss: 10960313.439560\n",
      "Train Epoch: 30 [12800/50981 (25%)]\tLoss: 10973158.801980\n",
      "Train Epoch: 30 [14080/50981 (28%)]\tLoss: 10978247.819820\n",
      "Train Epoch: 30 [15360/50981 (30%)]\tLoss: 10980589.396694\n",
      "Train Epoch: 30 [16640/50981 (33%)]\tLoss: 10999252.343511\n",
      "Train Epoch: 30 [17920/50981 (35%)]\tLoss: 11027730.609929\n",
      "Train Epoch: 30 [19200/50981 (38%)]\tLoss: 11053150.490066\n",
      "Train Epoch: 30 [20480/50981 (40%)]\tLoss: 11069491.273292\n",
      "Train Epoch: 30 [21760/50981 (43%)]\tLoss: 11080745.789474\n",
      "Train Epoch: 30 [23040/50981 (45%)]\tLoss: 11087666.342541\n",
      "Train Epoch: 30 [24320/50981 (48%)]\tLoss: 11095349.691099\n",
      "Train Epoch: 30 [25600/50981 (50%)]\tLoss: 11106648.293532\n",
      "Train Epoch: 30 [26880/50981 (53%)]\tLoss: 11105239.379147\n",
      "Train Epoch: 30 [28160/50981 (55%)]\tLoss: 11121787.307692\n",
      "Train Epoch: 30 [29440/50981 (58%)]\tLoss: 11124736.428571\n",
      "Train Epoch: 30 [30720/50981 (60%)]\tLoss: 11145441.746888\n",
      "Train Epoch: 30 [32000/50981 (63%)]\tLoss: 11151175.569721\n",
      "Train Epoch: 30 [33280/50981 (65%)]\tLoss: 11161344.678161\n",
      "Train Epoch: 30 [34560/50981 (68%)]\tLoss: 11162015.896679\n",
      "Train Epoch: 30 [35840/50981 (70%)]\tLoss: 11170401.790036\n",
      "Train Epoch: 30 [37120/50981 (73%)]\tLoss: 11173486.673540\n",
      "Train Epoch: 30 [38400/50981 (75%)]\tLoss: 11174534.524917\n",
      "Train Epoch: 30 [39680/50981 (78%)]\tLoss: 11178179.122186\n",
      "Train Epoch: 30 [40960/50981 (80%)]\tLoss: 11180709.261682\n",
      "Train Epoch: 30 [42240/50981 (83%)]\tLoss: 11187595.830816\n",
      "Train Epoch: 30 [43520/50981 (85%)]\tLoss: 11192585.422287\n",
      "Train Epoch: 30 [44800/50981 (88%)]\tLoss: 11193835.826211\n",
      "Train Epoch: 30 [46080/50981 (90%)]\tLoss: 11197001.681440\n",
      "Train Epoch: 30 [47360/50981 (93%)]\tLoss: 11198827.873315\n",
      "Train Epoch: 30 [48640/50981 (95%)]\tLoss: 11206905.832021\n",
      "Train Epoch: 30 [49920/50981 (98%)]\tLoss: 11210849.785166\n",
      "Train Epoch: 31 [0/50981 (0%)]\tLoss: 11191369.000000\n",
      "Train Epoch: 31 [1280/50981 (3%)]\tLoss: 11040381.818182\n",
      "Train Epoch: 31 [2560/50981 (5%)]\tLoss: 10866672.761905\n",
      "Train Epoch: 31 [3840/50981 (8%)]\tLoss: 10869466.838710\n",
      "Train Epoch: 31 [5120/50981 (10%)]\tLoss: 10919368.707317\n",
      "Train Epoch: 31 [6400/50981 (13%)]\tLoss: 10860358.431373\n",
      "Train Epoch: 31 [7680/50981 (15%)]\tLoss: 10861172.704918\n",
      "Train Epoch: 31 [8960/50981 (18%)]\tLoss: 10887063.267606\n",
      "Train Epoch: 31 [10240/50981 (20%)]\tLoss: 10914735.987654\n",
      "Train Epoch: 31 [11520/50981 (23%)]\tLoss: 10891939.857143\n",
      "Train Epoch: 31 [12800/50981 (25%)]\tLoss: 10894051.049505\n",
      "Train Epoch: 31 [14080/50981 (28%)]\tLoss: 10906960.594595\n",
      "Train Epoch: 31 [15360/50981 (30%)]\tLoss: 10919811.454545\n",
      "Train Epoch: 31 [16640/50981 (33%)]\tLoss: 10919180.885496\n",
      "Train Epoch: 31 [17920/50981 (35%)]\tLoss: 10912247.531915\n",
      "Train Epoch: 31 [19200/50981 (38%)]\tLoss: 10903540.125828\n",
      "Train Epoch: 31 [20480/50981 (40%)]\tLoss: 10919239.086957\n",
      "Train Epoch: 31 [21760/50981 (43%)]\tLoss: 10922680.836257\n",
      "Train Epoch: 31 [23040/50981 (45%)]\tLoss: 10931734.281768\n",
      "Train Epoch: 31 [24320/50981 (48%)]\tLoss: 10950702.492147\n",
      "Train Epoch: 31 [25600/50981 (50%)]\tLoss: 10951731.721393\n",
      "Train Epoch: 31 [26880/50981 (53%)]\tLoss: 10970145.071090\n",
      "Train Epoch: 31 [28160/50981 (55%)]\tLoss: 10981997.751131\n",
      "Train Epoch: 31 [29440/50981 (58%)]\tLoss: 10997042.082251\n",
      "Train Epoch: 31 [30720/50981 (60%)]\tLoss: 10996736.195021\n",
      "Train Epoch: 31 [32000/50981 (63%)]\tLoss: 11003077.964143\n",
      "Train Epoch: 31 [33280/50981 (65%)]\tLoss: 11001435.747126\n",
      "Train Epoch: 31 [34560/50981 (68%)]\tLoss: 11013019.682657\n",
      "Train Epoch: 31 [35840/50981 (70%)]\tLoss: 11018105.619217\n",
      "Train Epoch: 31 [37120/50981 (73%)]\tLoss: 11023789.257732\n",
      "Train Epoch: 31 [38400/50981 (75%)]\tLoss: 11026111.740864\n",
      "Train Epoch: 31 [39680/50981 (78%)]\tLoss: 11027713.813505\n",
      "Train Epoch: 31 [40960/50981 (80%)]\tLoss: 11018395.912773\n",
      "Train Epoch: 31 [42240/50981 (83%)]\tLoss: 11020492.583082\n",
      "Train Epoch: 31 [43520/50981 (85%)]\tLoss: 11029035.530792\n",
      "Train Epoch: 31 [44800/50981 (88%)]\tLoss: 11044725.122507\n",
      "Train Epoch: 31 [46080/50981 (90%)]\tLoss: 11050966.263158\n",
      "Train Epoch: 31 [47360/50981 (93%)]\tLoss: 11054267.611860\n",
      "Train Epoch: 31 [48640/50981 (95%)]\tLoss: 11066019.094488\n",
      "Train Epoch: 31 [49920/50981 (98%)]\tLoss: 11072690.503836\n",
      "Train Epoch: 32 [0/50981 (0%)]\tLoss: 10375908.000000\n",
      "Train Epoch: 32 [1280/50981 (3%)]\tLoss: 10762471.000000\n",
      "Train Epoch: 32 [2560/50981 (5%)]\tLoss: 10779247.523810\n",
      "Train Epoch: 32 [3840/50981 (8%)]\tLoss: 10742228.290323\n",
      "Train Epoch: 32 [5120/50981 (10%)]\tLoss: 10729906.341463\n",
      "Train Epoch: 32 [6400/50981 (13%)]\tLoss: 10722899.764706\n",
      "Train Epoch: 32 [7680/50981 (15%)]\tLoss: 10701062.918033\n",
      "Train Epoch: 32 [8960/50981 (18%)]\tLoss: 10730394.915493\n",
      "Train Epoch: 32 [10240/50981 (20%)]\tLoss: 10754031.641975\n",
      "Train Epoch: 32 [11520/50981 (23%)]\tLoss: 10743675.659341\n",
      "Train Epoch: 32 [12800/50981 (25%)]\tLoss: 10754956.465347\n",
      "Train Epoch: 32 [14080/50981 (28%)]\tLoss: 10755169.639640\n",
      "Train Epoch: 32 [15360/50981 (30%)]\tLoss: 10783248.537190\n",
      "Train Epoch: 32 [16640/50981 (33%)]\tLoss: 10794854.702290\n",
      "Train Epoch: 32 [17920/50981 (35%)]\tLoss: 10790274.382979\n",
      "Train Epoch: 32 [19200/50981 (38%)]\tLoss: 10814092.006623\n",
      "Train Epoch: 32 [20480/50981 (40%)]\tLoss: 10829138.229814\n",
      "Train Epoch: 32 [21760/50981 (43%)]\tLoss: 10826366.350877\n",
      "Train Epoch: 32 [23040/50981 (45%)]\tLoss: 10839654.646409\n",
      "Train Epoch: 32 [24320/50981 (48%)]\tLoss: 10841687.020942\n",
      "Train Epoch: 32 [25600/50981 (50%)]\tLoss: 10848438.393035\n",
      "Train Epoch: 32 [26880/50981 (53%)]\tLoss: 10876373.777251\n",
      "Train Epoch: 32 [28160/50981 (55%)]\tLoss: 10879684.615385\n",
      "Train Epoch: 32 [29440/50981 (58%)]\tLoss: 10893656.238095\n",
      "Train Epoch: 32 [30720/50981 (60%)]\tLoss: 10903373.112033\n",
      "Train Epoch: 32 [32000/50981 (63%)]\tLoss: 10901064.860558\n",
      "Train Epoch: 32 [33280/50981 (65%)]\tLoss: 10911465.517241\n",
      "Train Epoch: 32 [34560/50981 (68%)]\tLoss: 10913333.771218\n",
      "Train Epoch: 32 [35840/50981 (70%)]\tLoss: 10918991.661922\n",
      "Train Epoch: 32 [37120/50981 (73%)]\tLoss: 10922522.828179\n",
      "Train Epoch: 32 [38400/50981 (75%)]\tLoss: 10924836.209302\n",
      "Train Epoch: 32 [39680/50981 (78%)]\tLoss: 10931415.549839\n",
      "Train Epoch: 32 [40960/50981 (80%)]\tLoss: 10941417.763240\n",
      "Train Epoch: 32 [42240/50981 (83%)]\tLoss: 10946027.528701\n",
      "Train Epoch: 32 [43520/50981 (85%)]\tLoss: 10950716.202346\n",
      "Train Epoch: 32 [44800/50981 (88%)]\tLoss: 10955290.669516\n",
      "Train Epoch: 32 [46080/50981 (90%)]\tLoss: 10951902.157895\n",
      "Train Epoch: 32 [47360/50981 (93%)]\tLoss: 10957127.698113\n",
      "Train Epoch: 32 [48640/50981 (95%)]\tLoss: 10965499.834646\n",
      "Train Epoch: 32 [49920/50981 (98%)]\tLoss: 10968883.913043\n",
      "Train Epoch: 33 [0/50981 (0%)]\tLoss: 9989574.000000\n",
      "Train Epoch: 33 [1280/50981 (3%)]\tLoss: 10485863.272727\n",
      "Train Epoch: 33 [2560/50981 (5%)]\tLoss: 10615350.000000\n",
      "Train Epoch: 33 [3840/50981 (8%)]\tLoss: 10726775.096774\n",
      "Train Epoch: 33 [5120/50981 (10%)]\tLoss: 10722915.878049\n",
      "Train Epoch: 33 [6400/50981 (13%)]\tLoss: 10714516.254902\n",
      "Train Epoch: 33 [7680/50981 (15%)]\tLoss: 10703446.229508\n",
      "Train Epoch: 33 [8960/50981 (18%)]\tLoss: 10704309.830986\n",
      "Train Epoch: 33 [10240/50981 (20%)]\tLoss: 10715715.061728\n",
      "Train Epoch: 33 [11520/50981 (23%)]\tLoss: 10710616.538462\n",
      "Train Epoch: 33 [12800/50981 (25%)]\tLoss: 10704118.425743\n",
      "Train Epoch: 33 [14080/50981 (28%)]\tLoss: 10694194.963964\n",
      "Train Epoch: 33 [15360/50981 (30%)]\tLoss: 10693384.504132\n",
      "Train Epoch: 33 [16640/50981 (33%)]\tLoss: 10708590.809160\n",
      "Train Epoch: 33 [17920/50981 (35%)]\tLoss: 10711138.070922\n",
      "Train Epoch: 33 [19200/50981 (38%)]\tLoss: 10721695.516556\n",
      "Train Epoch: 33 [20480/50981 (40%)]\tLoss: 10734649.602484\n",
      "Train Epoch: 33 [21760/50981 (43%)]\tLoss: 10738020.438596\n",
      "Train Epoch: 33 [23040/50981 (45%)]\tLoss: 10738135.712707\n",
      "Train Epoch: 33 [24320/50981 (48%)]\tLoss: 10757951.780105\n",
      "Train Epoch: 33 [25600/50981 (50%)]\tLoss: 10767348.915423\n",
      "Train Epoch: 33 [26880/50981 (53%)]\tLoss: 10776545.222749\n",
      "Train Epoch: 33 [28160/50981 (55%)]\tLoss: 10775732.466063\n",
      "Train Epoch: 33 [29440/50981 (58%)]\tLoss: 10771338.320346\n",
      "Train Epoch: 33 [30720/50981 (60%)]\tLoss: 10776328.551867\n",
      "Train Epoch: 33 [32000/50981 (63%)]\tLoss: 10779890.402390\n",
      "Train Epoch: 33 [33280/50981 (65%)]\tLoss: 10792687.651341\n",
      "Train Epoch: 33 [34560/50981 (68%)]\tLoss: 10800175.837638\n",
      "Train Epoch: 33 [35840/50981 (70%)]\tLoss: 10802166.377224\n",
      "Train Epoch: 33 [37120/50981 (73%)]\tLoss: 10807888.862543\n",
      "Train Epoch: 33 [38400/50981 (75%)]\tLoss: 10813669.966777\n",
      "Train Epoch: 33 [39680/50981 (78%)]\tLoss: 10820493.151125\n",
      "Train Epoch: 33 [40960/50981 (80%)]\tLoss: 10829412.965732\n",
      "Train Epoch: 33 [42240/50981 (83%)]\tLoss: 10834845.885196\n",
      "Train Epoch: 33 [43520/50981 (85%)]\tLoss: 10838367.193548\n",
      "Train Epoch: 33 [44800/50981 (88%)]\tLoss: 10847187.424501\n",
      "Train Epoch: 33 [46080/50981 (90%)]\tLoss: 10850572.501385\n",
      "Train Epoch: 33 [47360/50981 (93%)]\tLoss: 10851588.345013\n",
      "Train Epoch: 33 [48640/50981 (95%)]\tLoss: 10857572.929134\n",
      "Train Epoch: 33 [49920/50981 (98%)]\tLoss: 10856282.235294\n",
      "Train Epoch: 34 [0/50981 (0%)]\tLoss: 10586422.000000\n",
      "Train Epoch: 34 [1280/50981 (3%)]\tLoss: 10482696.000000\n",
      "Train Epoch: 34 [2560/50981 (5%)]\tLoss: 10491372.523810\n",
      "Train Epoch: 34 [3840/50981 (8%)]\tLoss: 10550406.290323\n",
      "Train Epoch: 34 [5120/50981 (10%)]\tLoss: 10546997.682927\n",
      "Train Epoch: 34 [6400/50981 (13%)]\tLoss: 10576727.235294\n",
      "Train Epoch: 34 [7680/50981 (15%)]\tLoss: 10588617.688525\n",
      "Train Epoch: 34 [8960/50981 (18%)]\tLoss: 10604322.591549\n",
      "Train Epoch: 34 [10240/50981 (20%)]\tLoss: 10590280.703704\n",
      "Train Epoch: 34 [11520/50981 (23%)]\tLoss: 10601245.263736\n",
      "Train Epoch: 34 [12800/50981 (25%)]\tLoss: 10572595.336634\n",
      "Train Epoch: 34 [14080/50981 (28%)]\tLoss: 10574954.369369\n",
      "Train Epoch: 34 [15360/50981 (30%)]\tLoss: 10560797.305785\n",
      "Train Epoch: 34 [16640/50981 (33%)]\tLoss: 10554629.244275\n",
      "Train Epoch: 34 [17920/50981 (35%)]\tLoss: 10565835.120567\n",
      "Train Epoch: 34 [19200/50981 (38%)]\tLoss: 10576778.947020\n",
      "Train Epoch: 34 [20480/50981 (40%)]\tLoss: 10586059.335404\n",
      "Train Epoch: 34 [21760/50981 (43%)]\tLoss: 10601750.941520\n",
      "Train Epoch: 34 [23040/50981 (45%)]\tLoss: 10616509.640884\n",
      "Train Epoch: 34 [24320/50981 (48%)]\tLoss: 10618491.094241\n",
      "Train Epoch: 34 [25600/50981 (50%)]\tLoss: 10644500.945274\n",
      "Train Epoch: 34 [26880/50981 (53%)]\tLoss: 10655211.000000\n",
      "Train Epoch: 34 [28160/50981 (55%)]\tLoss: 10650759.728507\n",
      "Train Epoch: 34 [29440/50981 (58%)]\tLoss: 10658337.956710\n",
      "Train Epoch: 34 [30720/50981 (60%)]\tLoss: 10662430.352697\n",
      "Train Epoch: 34 [32000/50981 (63%)]\tLoss: 10678070.944223\n",
      "Train Epoch: 34 [33280/50981 (65%)]\tLoss: 10686372.206897\n",
      "Train Epoch: 34 [34560/50981 (68%)]\tLoss: 10682707.793358\n",
      "Train Epoch: 34 [35840/50981 (70%)]\tLoss: 10683399.024911\n",
      "Train Epoch: 34 [37120/50981 (73%)]\tLoss: 10685321.288660\n",
      "Train Epoch: 34 [38400/50981 (75%)]\tLoss: 10693341.305648\n",
      "Train Epoch: 34 [39680/50981 (78%)]\tLoss: 10693920.350482\n",
      "Train Epoch: 34 [40960/50981 (80%)]\tLoss: 10695834.249221\n",
      "Train Epoch: 34 [42240/50981 (83%)]\tLoss: 10705198.637462\n",
      "Train Epoch: 34 [43520/50981 (85%)]\tLoss: 10702879.656891\n",
      "Train Epoch: 34 [44800/50981 (88%)]\tLoss: 10707655.971510\n",
      "Train Epoch: 34 [46080/50981 (90%)]\tLoss: 10710292.642659\n",
      "Train Epoch: 34 [47360/50981 (93%)]\tLoss: 10719343.442049\n",
      "Train Epoch: 34 [48640/50981 (95%)]\tLoss: 10721597.929134\n",
      "Train Epoch: 34 [49920/50981 (98%)]\tLoss: 10736201.156010\n",
      "Train Epoch: 35 [0/50981 (0%)]\tLoss: 10344760.000000\n",
      "Train Epoch: 35 [1280/50981 (3%)]\tLoss: 10321119.636364\n",
      "Train Epoch: 35 [2560/50981 (5%)]\tLoss: 10359690.904762\n",
      "Train Epoch: 35 [3840/50981 (8%)]\tLoss: 10406039.000000\n",
      "Train Epoch: 35 [5120/50981 (10%)]\tLoss: 10371319.219512\n",
      "Train Epoch: 35 [6400/50981 (13%)]\tLoss: 10396451.078431\n",
      "Train Epoch: 35 [7680/50981 (15%)]\tLoss: 10415537.885246\n",
      "Train Epoch: 35 [8960/50981 (18%)]\tLoss: 10407047.943662\n",
      "Train Epoch: 35 [10240/50981 (20%)]\tLoss: 10420826.753086\n",
      "Train Epoch: 35 [11520/50981 (23%)]\tLoss: 10454364.351648\n",
      "Train Epoch: 35 [12800/50981 (25%)]\tLoss: 10442021.198020\n",
      "Train Epoch: 35 [14080/50981 (28%)]\tLoss: 10450971.081081\n",
      "Train Epoch: 35 [15360/50981 (30%)]\tLoss: 10451750.909091\n",
      "Train Epoch: 35 [16640/50981 (33%)]\tLoss: 10444908.549618\n",
      "Train Epoch: 35 [17920/50981 (35%)]\tLoss: 10463104.687943\n",
      "Train Epoch: 35 [19200/50981 (38%)]\tLoss: 10463913.264901\n",
      "Train Epoch: 35 [20480/50981 (40%)]\tLoss: 10470704.024845\n",
      "Train Epoch: 35 [21760/50981 (43%)]\tLoss: 10471492.292398\n",
      "Train Epoch: 35 [23040/50981 (45%)]\tLoss: 10485590.961326\n",
      "Train Epoch: 35 [24320/50981 (48%)]\tLoss: 10499131.010471\n",
      "Train Epoch: 35 [25600/50981 (50%)]\tLoss: 10509530.174129\n",
      "Train Epoch: 35 [26880/50981 (53%)]\tLoss: 10514910.668246\n",
      "Train Epoch: 35 [28160/50981 (55%)]\tLoss: 10523790.877828\n",
      "Train Epoch: 35 [29440/50981 (58%)]\tLoss: 10532523.043290\n",
      "Train Epoch: 35 [30720/50981 (60%)]\tLoss: 10537986.825726\n",
      "Train Epoch: 35 [32000/50981 (63%)]\tLoss: 10540256.685259\n",
      "Train Epoch: 35 [33280/50981 (65%)]\tLoss: 10537837.597701\n",
      "Train Epoch: 35 [34560/50981 (68%)]\tLoss: 10541609.206642\n",
      "Train Epoch: 35 [35840/50981 (70%)]\tLoss: 10550492.676157\n",
      "Train Epoch: 35 [37120/50981 (73%)]\tLoss: 10563462.762887\n",
      "Train Epoch: 35 [38400/50981 (75%)]\tLoss: 10566167.976744\n",
      "Train Epoch: 35 [39680/50981 (78%)]\tLoss: 10574953.305466\n",
      "Train Epoch: 35 [40960/50981 (80%)]\tLoss: 10575749.158879\n",
      "Train Epoch: 35 [42240/50981 (83%)]\tLoss: 10575838.308157\n",
      "Train Epoch: 35 [43520/50981 (85%)]\tLoss: 10580686.390029\n",
      "Train Epoch: 35 [44800/50981 (88%)]\tLoss: 10581421.800570\n",
      "Train Epoch: 35 [46080/50981 (90%)]\tLoss: 10588570.914127\n",
      "Train Epoch: 35 [47360/50981 (93%)]\tLoss: 10594800.830189\n",
      "Train Epoch: 35 [48640/50981 (95%)]\tLoss: 10608950.081365\n",
      "Train Epoch: 35 [49920/50981 (98%)]\tLoss: 10618748.491049\n",
      "Train Epoch: 36 [0/50981 (0%)]\tLoss: 10059514.000000\n",
      "Train Epoch: 36 [1280/50981 (3%)]\tLoss: 10079661.454545\n",
      "Train Epoch: 36 [2560/50981 (5%)]\tLoss: 10188859.523810\n",
      "Train Epoch: 36 [3840/50981 (8%)]\tLoss: 10171204.838710\n",
      "Train Epoch: 36 [5120/50981 (10%)]\tLoss: 10228778.902439\n",
      "Train Epoch: 36 [6400/50981 (13%)]\tLoss: 10243477.058824\n",
      "Train Epoch: 36 [7680/50981 (15%)]\tLoss: 10260491.344262\n",
      "Train Epoch: 36 [8960/50981 (18%)]\tLoss: 10263346.295775\n",
      "Train Epoch: 36 [10240/50981 (20%)]\tLoss: 10294745.962963\n",
      "Train Epoch: 36 [11520/50981 (23%)]\tLoss: 10290784.087912\n",
      "Train Epoch: 36 [12800/50981 (25%)]\tLoss: 10309293.584158\n",
      "Train Epoch: 36 [14080/50981 (28%)]\tLoss: 10335376.648649\n",
      "Train Epoch: 36 [15360/50981 (30%)]\tLoss: 10324599.578512\n",
      "Train Epoch: 36 [16640/50981 (33%)]\tLoss: 10338747.068702\n",
      "Train Epoch: 36 [17920/50981 (35%)]\tLoss: 10353265.744681\n",
      "Train Epoch: 36 [19200/50981 (38%)]\tLoss: 10363535.192053\n",
      "Train Epoch: 36 [20480/50981 (40%)]\tLoss: 10372671.757764\n",
      "Train Epoch: 36 [21760/50981 (43%)]\tLoss: 10382224.538012\n",
      "Train Epoch: 36 [23040/50981 (45%)]\tLoss: 10400017.254144\n",
      "Train Epoch: 36 [24320/50981 (48%)]\tLoss: 10401178.869110\n",
      "Train Epoch: 36 [25600/50981 (50%)]\tLoss: 10426619.417910\n",
      "Train Epoch: 36 [26880/50981 (53%)]\tLoss: 10433046.857820\n",
      "Train Epoch: 36 [28160/50981 (55%)]\tLoss: 10434832.864253\n",
      "Train Epoch: 36 [29440/50981 (58%)]\tLoss: 10431847.770563\n",
      "Train Epoch: 36 [30720/50981 (60%)]\tLoss: 10431528.900415\n",
      "Train Epoch: 36 [32000/50981 (63%)]\tLoss: 10441491.063745\n",
      "Train Epoch: 36 [33280/50981 (65%)]\tLoss: 10447879.398467\n",
      "Train Epoch: 36 [34560/50981 (68%)]\tLoss: 10463105.250923\n",
      "Train Epoch: 36 [35840/50981 (70%)]\tLoss: 10471698.402135\n",
      "Train Epoch: 36 [37120/50981 (73%)]\tLoss: 10480257.254296\n",
      "Train Epoch: 36 [38400/50981 (75%)]\tLoss: 10491204.973422\n",
      "Train Epoch: 36 [39680/50981 (78%)]\tLoss: 10497212.553055\n",
      "Train Epoch: 36 [40960/50981 (80%)]\tLoss: 10502038.707165\n",
      "Train Epoch: 36 [42240/50981 (83%)]\tLoss: 10509151.543807\n",
      "Train Epoch: 36 [43520/50981 (85%)]\tLoss: 10515330.633431\n",
      "Train Epoch: 36 [44800/50981 (88%)]\tLoss: 10512692.156695\n",
      "Train Epoch: 36 [46080/50981 (90%)]\tLoss: 10522486.418283\n",
      "Train Epoch: 36 [47360/50981 (93%)]\tLoss: 10528540.393531\n",
      "Train Epoch: 36 [48640/50981 (95%)]\tLoss: 10530265.734908\n",
      "Train Epoch: 36 [49920/50981 (98%)]\tLoss: 10524646.470588\n",
      "Train Epoch: 37 [0/50981 (0%)]\tLoss: 10470721.000000\n",
      "Train Epoch: 37 [1280/50981 (3%)]\tLoss: 10193863.272727\n",
      "Train Epoch: 37 [2560/50981 (5%)]\tLoss: 10083823.000000\n",
      "Train Epoch: 37 [3840/50981 (8%)]\tLoss: 10128965.258065\n",
      "Train Epoch: 37 [5120/50981 (10%)]\tLoss: 10151804.707317\n",
      "Train Epoch: 37 [6400/50981 (13%)]\tLoss: 10180927.156863\n",
      "Train Epoch: 37 [7680/50981 (15%)]\tLoss: 10221241.557377\n",
      "Train Epoch: 37 [8960/50981 (18%)]\tLoss: 10263314.549296\n",
      "Train Epoch: 37 [10240/50981 (20%)]\tLoss: 10245355.296296\n",
      "Train Epoch: 37 [11520/50981 (23%)]\tLoss: 10246635.824176\n",
      "Train Epoch: 37 [12800/50981 (25%)]\tLoss: 10254971.435644\n",
      "Train Epoch: 37 [14080/50981 (28%)]\tLoss: 10270914.000000\n",
      "Train Epoch: 37 [15360/50981 (30%)]\tLoss: 10266186.834711\n",
      "Train Epoch: 37 [16640/50981 (33%)]\tLoss: 10272131.984733\n",
      "Train Epoch: 37 [17920/50981 (35%)]\tLoss: 10270058.971631\n",
      "Train Epoch: 37 [19200/50981 (38%)]\tLoss: 10279880.933775\n",
      "Train Epoch: 37 [20480/50981 (40%)]\tLoss: 10286761.503106\n",
      "Train Epoch: 37 [21760/50981 (43%)]\tLoss: 10302056.479532\n",
      "Train Epoch: 37 [23040/50981 (45%)]\tLoss: 10316255.861878\n",
      "Train Epoch: 37 [24320/50981 (48%)]\tLoss: 10324462.272251\n",
      "Train Epoch: 37 [25600/50981 (50%)]\tLoss: 10337772.084577\n",
      "Train Epoch: 37 [26880/50981 (53%)]\tLoss: 10341563.644550\n",
      "Train Epoch: 37 [28160/50981 (55%)]\tLoss: 10350961.067873\n",
      "Train Epoch: 37 [29440/50981 (58%)]\tLoss: 10354019.883117\n",
      "Train Epoch: 37 [30720/50981 (60%)]\tLoss: 10359412.352697\n",
      "Train Epoch: 37 [32000/50981 (63%)]\tLoss: 10361848.860558\n",
      "Train Epoch: 37 [33280/50981 (65%)]\tLoss: 10372003.352490\n",
      "Train Epoch: 37 [34560/50981 (68%)]\tLoss: 10374986.195572\n",
      "Train Epoch: 37 [35840/50981 (70%)]\tLoss: 10380838.145907\n",
      "Train Epoch: 37 [37120/50981 (73%)]\tLoss: 10383953.065292\n",
      "Train Epoch: 37 [38400/50981 (75%)]\tLoss: 10391059.578073\n",
      "Train Epoch: 37 [39680/50981 (78%)]\tLoss: 10394764.070740\n",
      "Train Epoch: 37 [40960/50981 (80%)]\tLoss: 10401458.722741\n",
      "Train Epoch: 37 [42240/50981 (83%)]\tLoss: 10402639.522659\n",
      "Train Epoch: 37 [43520/50981 (85%)]\tLoss: 10408964.859238\n",
      "Train Epoch: 37 [44800/50981 (88%)]\tLoss: 10417878.284900\n",
      "Train Epoch: 37 [46080/50981 (90%)]\tLoss: 10424923.994460\n",
      "Train Epoch: 37 [47360/50981 (93%)]\tLoss: 10429055.787062\n",
      "Train Epoch: 37 [48640/50981 (95%)]\tLoss: 10438370.839895\n",
      "Train Epoch: 37 [49920/50981 (98%)]\tLoss: 10437588.386189\n",
      "Train Epoch: 38 [0/50981 (0%)]\tLoss: 10422191.000000\n",
      "Train Epoch: 38 [1280/50981 (3%)]\tLoss: 10139391.909091\n",
      "Train Epoch: 38 [2560/50981 (5%)]\tLoss: 9966986.666667\n",
      "Train Epoch: 38 [3840/50981 (8%)]\tLoss: 9904121.258065\n",
      "Train Epoch: 38 [5120/50981 (10%)]\tLoss: 9932448.219512\n",
      "Train Epoch: 38 [6400/50981 (13%)]\tLoss: 9960001.039216\n",
      "Train Epoch: 38 [7680/50981 (15%)]\tLoss: 10006003.721311\n",
      "Train Epoch: 38 [8960/50981 (18%)]\tLoss: 10067852.887324\n",
      "Train Epoch: 38 [10240/50981 (20%)]\tLoss: 10096166.098765\n",
      "Train Epoch: 38 [11520/50981 (23%)]\tLoss: 10090112.714286\n",
      "Train Epoch: 38 [12800/50981 (25%)]\tLoss: 10083377.089109\n",
      "Train Epoch: 38 [14080/50981 (28%)]\tLoss: 10094528.288288\n",
      "Train Epoch: 38 [15360/50981 (30%)]\tLoss: 10091038.909091\n",
      "Train Epoch: 38 [16640/50981 (33%)]\tLoss: 10112527.061069\n",
      "Train Epoch: 38 [17920/50981 (35%)]\tLoss: 10133136.361702\n",
      "Train Epoch: 38 [19200/50981 (38%)]\tLoss: 10146594.874172\n",
      "Train Epoch: 38 [20480/50981 (40%)]\tLoss: 10147174.857143\n",
      "Train Epoch: 38 [21760/50981 (43%)]\tLoss: 10161574.672515\n",
      "Train Epoch: 38 [23040/50981 (45%)]\tLoss: 10173820.254144\n",
      "Train Epoch: 38 [24320/50981 (48%)]\tLoss: 10181861.314136\n",
      "Train Epoch: 38 [25600/50981 (50%)]\tLoss: 10196175.885572\n",
      "Train Epoch: 38 [26880/50981 (53%)]\tLoss: 10197154.744076\n",
      "Train Epoch: 38 [28160/50981 (55%)]\tLoss: 10208426.742081\n",
      "Train Epoch: 38 [29440/50981 (58%)]\tLoss: 10224457.601732\n",
      "Train Epoch: 38 [30720/50981 (60%)]\tLoss: 10228727.809129\n",
      "Train Epoch: 38 [32000/50981 (63%)]\tLoss: 10240319.003984\n",
      "Train Epoch: 38 [33280/50981 (65%)]\tLoss: 10248377.681992\n",
      "Train Epoch: 38 [34560/50981 (68%)]\tLoss: 10257466.394834\n",
      "Train Epoch: 38 [35840/50981 (70%)]\tLoss: 10276906.088968\n",
      "Train Epoch: 38 [37120/50981 (73%)]\tLoss: 10277666.127148\n",
      "Train Epoch: 38 [38400/50981 (75%)]\tLoss: 10284500.249169\n",
      "Train Epoch: 38 [39680/50981 (78%)]\tLoss: 10292636.475884\n",
      "Train Epoch: 38 [40960/50981 (80%)]\tLoss: 10302622.327103\n",
      "Train Epoch: 38 [42240/50981 (83%)]\tLoss: 10308743.604230\n",
      "Train Epoch: 38 [43520/50981 (85%)]\tLoss: 10314075.648094\n",
      "Train Epoch: 38 [44800/50981 (88%)]\tLoss: 10320435.327635\n",
      "Train Epoch: 38 [46080/50981 (90%)]\tLoss: 10325719.617729\n",
      "Train Epoch: 38 [47360/50981 (93%)]\tLoss: 10331335.485175\n",
      "Train Epoch: 38 [48640/50981 (95%)]\tLoss: 10338949.908136\n",
      "Train Epoch: 38 [49920/50981 (98%)]\tLoss: 10342357.434783\n",
      "Train Epoch: 39 [0/50981 (0%)]\tLoss: 9687370.000000\n",
      "Train Epoch: 39 [1280/50981 (3%)]\tLoss: 10173201.909091\n",
      "Train Epoch: 39 [2560/50981 (5%)]\tLoss: 10068597.285714\n",
      "Train Epoch: 39 [3840/50981 (8%)]\tLoss: 10053410.064516\n",
      "Train Epoch: 39 [5120/50981 (10%)]\tLoss: 10011258.000000\n",
      "Train Epoch: 39 [6400/50981 (13%)]\tLoss: 10064544.254902\n",
      "Train Epoch: 39 [7680/50981 (15%)]\tLoss: 10063029.377049\n",
      "Train Epoch: 39 [8960/50981 (18%)]\tLoss: 10064732.239437\n",
      "Train Epoch: 39 [10240/50981 (20%)]\tLoss: 10102391.506173\n",
      "Train Epoch: 39 [11520/50981 (23%)]\tLoss: 10122737.087912\n",
      "Train Epoch: 39 [12800/50981 (25%)]\tLoss: 10129868.920792\n",
      "Train Epoch: 39 [14080/50981 (28%)]\tLoss: 10141921.846847\n",
      "Train Epoch: 39 [15360/50981 (30%)]\tLoss: 10151814.173554\n",
      "Train Epoch: 39 [16640/50981 (33%)]\tLoss: 10145858.595420\n",
      "Train Epoch: 39 [17920/50981 (35%)]\tLoss: 10152453.333333\n",
      "Train Epoch: 39 [19200/50981 (38%)]\tLoss: 10158223.821192\n",
      "Train Epoch: 39 [20480/50981 (40%)]\tLoss: 10157126.559006\n",
      "Train Epoch: 39 [21760/50981 (43%)]\tLoss: 10152858.994152\n",
      "Train Epoch: 39 [23040/50981 (45%)]\tLoss: 10156294.469613\n",
      "Train Epoch: 39 [24320/50981 (48%)]\tLoss: 10161278.371728\n",
      "Train Epoch: 39 [25600/50981 (50%)]\tLoss: 10180522.343284\n",
      "Train Epoch: 39 [26880/50981 (53%)]\tLoss: 10186527.118483\n",
      "Train Epoch: 39 [28160/50981 (55%)]\tLoss: 10197770.126697\n",
      "Train Epoch: 39 [29440/50981 (58%)]\tLoss: 10207939.090909\n",
      "Train Epoch: 39 [30720/50981 (60%)]\tLoss: 10216351.058091\n",
      "Train Epoch: 39 [32000/50981 (63%)]\tLoss: 10220092.482072\n",
      "Train Epoch: 39 [33280/50981 (65%)]\tLoss: 10225080.655172\n",
      "Train Epoch: 39 [34560/50981 (68%)]\tLoss: 10228669.195572\n",
      "Train Epoch: 39 [35840/50981 (70%)]\tLoss: 10232539.380783\n",
      "Train Epoch: 39 [37120/50981 (73%)]\tLoss: 10237540.718213\n",
      "Train Epoch: 39 [38400/50981 (75%)]\tLoss: 10234240.684385\n",
      "Train Epoch: 39 [39680/50981 (78%)]\tLoss: 10237997.890675\n",
      "Train Epoch: 39 [40960/50981 (80%)]\tLoss: 10237169.342679\n",
      "Train Epoch: 39 [42240/50981 (83%)]\tLoss: 10245007.347432\n",
      "Train Epoch: 39 [43520/50981 (85%)]\tLoss: 10254810.659824\n",
      "Train Epoch: 39 [44800/50981 (88%)]\tLoss: 10263846.014245\n",
      "Train Epoch: 39 [46080/50981 (90%)]\tLoss: 10264622.160665\n",
      "Train Epoch: 39 [47360/50981 (93%)]\tLoss: 10264776.094340\n",
      "Train Epoch: 39 [48640/50981 (95%)]\tLoss: 10270108.703412\n",
      "Train Epoch: 39 [49920/50981 (98%)]\tLoss: 10271620.946292\n",
      "Train Epoch: 40 [0/50981 (0%)]\tLoss: 9847853.000000\n",
      "Train Epoch: 40 [1280/50981 (3%)]\tLoss: 9800272.272727\n",
      "Train Epoch: 40 [2560/50981 (5%)]\tLoss: 9845711.904762\n",
      "Train Epoch: 40 [3840/50981 (8%)]\tLoss: 9811922.451613\n",
      "Train Epoch: 40 [5120/50981 (10%)]\tLoss: 9826607.024390\n",
      "Train Epoch: 40 [6400/50981 (13%)]\tLoss: 9835879.156863\n",
      "Train Epoch: 40 [7680/50981 (15%)]\tLoss: 9849533.967213\n",
      "Train Epoch: 40 [8960/50981 (18%)]\tLoss: 9881349.154930\n",
      "Train Epoch: 40 [10240/50981 (20%)]\tLoss: 9901904.543210\n",
      "Train Epoch: 40 [11520/50981 (23%)]\tLoss: 9930646.120879\n",
      "Train Epoch: 40 [12800/50981 (25%)]\tLoss: 9949320.326733\n",
      "Train Epoch: 40 [14080/50981 (28%)]\tLoss: 9956311.459459\n",
      "Train Epoch: 40 [15360/50981 (30%)]\tLoss: 9958039.958678\n",
      "Train Epoch: 40 [16640/50981 (33%)]\tLoss: 9972905.183206\n",
      "Train Epoch: 40 [17920/50981 (35%)]\tLoss: 9997192.929078\n",
      "Train Epoch: 40 [19200/50981 (38%)]\tLoss: 10011958.377483\n",
      "Train Epoch: 40 [20480/50981 (40%)]\tLoss: 10020728.782609\n",
      "Train Epoch: 40 [21760/50981 (43%)]\tLoss: 10036015.801170\n",
      "Train Epoch: 40 [23040/50981 (45%)]\tLoss: 10051736.817680\n",
      "Train Epoch: 40 [24320/50981 (48%)]\tLoss: 10052178.869110\n",
      "Train Epoch: 40 [25600/50981 (50%)]\tLoss: 10073844.935323\n",
      "Train Epoch: 40 [26880/50981 (53%)]\tLoss: 10080437.554502\n",
      "Train Epoch: 40 [28160/50981 (55%)]\tLoss: 10081352.895928\n",
      "Train Epoch: 40 [29440/50981 (58%)]\tLoss: 10087830.796537\n",
      "Train Epoch: 40 [30720/50981 (60%)]\tLoss: 10094710.502075\n",
      "Train Epoch: 40 [32000/50981 (63%)]\tLoss: 10097212.310757\n",
      "Train Epoch: 40 [33280/50981 (65%)]\tLoss: 10118847.582375\n",
      "Train Epoch: 40 [34560/50981 (68%)]\tLoss: 10129233.501845\n",
      "Train Epoch: 40 [35840/50981 (70%)]\tLoss: 10139077.953737\n",
      "Train Epoch: 40 [37120/50981 (73%)]\tLoss: 10145604.580756\n",
      "Train Epoch: 40 [38400/50981 (75%)]\tLoss: 10156390.365449\n",
      "Train Epoch: 40 [39680/50981 (78%)]\tLoss: 10168667.845659\n",
      "Train Epoch: 40 [40960/50981 (80%)]\tLoss: 10169722.482866\n",
      "Train Epoch: 40 [42240/50981 (83%)]\tLoss: 10162771.719033\n",
      "Train Epoch: 40 [43520/50981 (85%)]\tLoss: 10168393.824047\n",
      "Train Epoch: 40 [44800/50981 (88%)]\tLoss: 10176464.179487\n",
      "Train Epoch: 40 [46080/50981 (90%)]\tLoss: 10179500.952909\n",
      "Train Epoch: 40 [47360/50981 (93%)]\tLoss: 10179428.549865\n",
      "Train Epoch: 40 [48640/50981 (95%)]\tLoss: 10176641.769029\n",
      "Train Epoch: 40 [49920/50981 (98%)]\tLoss: 10182258.984655\n",
      "Train Epoch: 41 [0/50981 (0%)]\tLoss: 9376871.000000\n",
      "Train Epoch: 41 [1280/50981 (3%)]\tLoss: 9847419.000000\n",
      "Train Epoch: 41 [2560/50981 (5%)]\tLoss: 9777466.000000\n",
      "Train Epoch: 41 [3840/50981 (8%)]\tLoss: 9758771.322581\n",
      "Train Epoch: 41 [5120/50981 (10%)]\tLoss: 9830012.829268\n",
      "Train Epoch: 41 [6400/50981 (13%)]\tLoss: 9792232.980392\n",
      "Train Epoch: 41 [7680/50981 (15%)]\tLoss: 9849762.475410\n",
      "Train Epoch: 41 [8960/50981 (18%)]\tLoss: 9880451.915493\n",
      "Train Epoch: 41 [10240/50981 (20%)]\tLoss: 9880221.888889\n",
      "Train Epoch: 41 [11520/50981 (23%)]\tLoss: 9895061.989011\n",
      "Train Epoch: 41 [12800/50981 (25%)]\tLoss: 9919331.465347\n",
      "Train Epoch: 41 [14080/50981 (28%)]\tLoss: 9914485.963964\n",
      "Train Epoch: 41 [15360/50981 (30%)]\tLoss: 9920030.214876\n",
      "Train Epoch: 41 [16640/50981 (33%)]\tLoss: 9931331.351145\n",
      "Train Epoch: 41 [17920/50981 (35%)]\tLoss: 9943402.617021\n",
      "Train Epoch: 41 [19200/50981 (38%)]\tLoss: 9941550.192053\n",
      "Train Epoch: 41 [20480/50981 (40%)]\tLoss: 9946673.614907\n",
      "Train Epoch: 41 [21760/50981 (43%)]\tLoss: 9956609.654971\n",
      "Train Epoch: 41 [23040/50981 (45%)]\tLoss: 9965626.972376\n",
      "Train Epoch: 41 [24320/50981 (48%)]\tLoss: 9976638.382199\n",
      "Train Epoch: 41 [25600/50981 (50%)]\tLoss: 9985912.781095\n",
      "Train Epoch: 41 [26880/50981 (53%)]\tLoss: 9986654.184834\n",
      "Train Epoch: 41 [28160/50981 (55%)]\tLoss: 9990670.850679\n",
      "Train Epoch: 41 [29440/50981 (58%)]\tLoss: 10006476.151515\n",
      "Train Epoch: 41 [30720/50981 (60%)]\tLoss: 10017774.091286\n",
      "Train Epoch: 41 [32000/50981 (63%)]\tLoss: 10023962.270916\n",
      "Train Epoch: 41 [33280/50981 (65%)]\tLoss: 10026374.363985\n",
      "Train Epoch: 41 [34560/50981 (68%)]\tLoss: 10038804.704797\n",
      "Train Epoch: 41 [35840/50981 (70%)]\tLoss: 10044055.007117\n",
      "Train Epoch: 41 [37120/50981 (73%)]\tLoss: 10052945.941581\n",
      "Train Epoch: 41 [38400/50981 (75%)]\tLoss: 10056169.338870\n",
      "Train Epoch: 41 [39680/50981 (78%)]\tLoss: 10062789.151125\n",
      "Train Epoch: 41 [40960/50981 (80%)]\tLoss: 10061001.249221\n",
      "Train Epoch: 41 [42240/50981 (83%)]\tLoss: 10074965.951662\n",
      "Train Epoch: 41 [43520/50981 (85%)]\tLoss: 10077649.255132\n",
      "Train Epoch: 41 [44800/50981 (88%)]\tLoss: 10081857.703704\n",
      "Train Epoch: 41 [46080/50981 (90%)]\tLoss: 10085567.997230\n",
      "Train Epoch: 41 [47360/50981 (93%)]\tLoss: 10090339.676550\n",
      "Train Epoch: 41 [48640/50981 (95%)]\tLoss: 10095815.706037\n",
      "Train Epoch: 41 [49920/50981 (98%)]\tLoss: 10101222.475703\n",
      "Train Epoch: 42 [0/50981 (0%)]\tLoss: 9391592.000000\n",
      "Train Epoch: 42 [1280/50981 (3%)]\tLoss: 9870981.636364\n",
      "Train Epoch: 42 [2560/50981 (5%)]\tLoss: 9832325.285714\n",
      "Train Epoch: 42 [3840/50981 (8%)]\tLoss: 9786050.258065\n",
      "Train Epoch: 42 [5120/50981 (10%)]\tLoss: 9790327.756098\n",
      "Train Epoch: 42 [6400/50981 (13%)]\tLoss: 9833005.803922\n",
      "Train Epoch: 42 [7680/50981 (15%)]\tLoss: 9852663.049180\n",
      "Train Epoch: 42 [8960/50981 (18%)]\tLoss: 9870082.098592\n",
      "Train Epoch: 42 [10240/50981 (20%)]\tLoss: 9877880.024691\n",
      "Train Epoch: 42 [11520/50981 (23%)]\tLoss: 9902218.527473\n",
      "Train Epoch: 42 [12800/50981 (25%)]\tLoss: 9895496.029703\n",
      "Train Epoch: 42 [14080/50981 (28%)]\tLoss: 9909360.387387\n",
      "Train Epoch: 42 [15360/50981 (30%)]\tLoss: 9919049.314050\n",
      "Train Epoch: 42 [16640/50981 (33%)]\tLoss: 9903845.114504\n",
      "Train Epoch: 42 [17920/50981 (35%)]\tLoss: 9909670.787234\n",
      "Train Epoch: 42 [19200/50981 (38%)]\tLoss: 9922467.476821\n",
      "Train Epoch: 42 [20480/50981 (40%)]\tLoss: 9922464.391304\n",
      "Train Epoch: 42 [21760/50981 (43%)]\tLoss: 9926774.760234\n",
      "Train Epoch: 42 [23040/50981 (45%)]\tLoss: 9944149.723757\n",
      "Train Epoch: 42 [24320/50981 (48%)]\tLoss: 9942833.151832\n",
      "Train Epoch: 42 [25600/50981 (50%)]\tLoss: 9946477.691542\n",
      "Train Epoch: 42 [26880/50981 (53%)]\tLoss: 9948721.213270\n",
      "Train Epoch: 42 [28160/50981 (55%)]\tLoss: 9951267.027149\n",
      "Train Epoch: 42 [29440/50981 (58%)]\tLoss: 9963247.173160\n",
      "Train Epoch: 42 [30720/50981 (60%)]\tLoss: 9962834.742739\n",
      "Train Epoch: 42 [32000/50981 (63%)]\tLoss: 9968162.764940\n",
      "Train Epoch: 42 [33280/50981 (65%)]\tLoss: 9967071.689655\n",
      "Train Epoch: 42 [34560/50981 (68%)]\tLoss: 9975316.966790\n",
      "Train Epoch: 42 [35840/50981 (70%)]\tLoss: 9980206.291815\n",
      "Train Epoch: 42 [37120/50981 (73%)]\tLoss: 9986327.907216\n",
      "Train Epoch: 42 [38400/50981 (75%)]\tLoss: 9991372.398671\n",
      "Train Epoch: 42 [39680/50981 (78%)]\tLoss: 9993578.797428\n",
      "Train Epoch: 42 [40960/50981 (80%)]\tLoss: 9998481.934579\n",
      "Train Epoch: 42 [42240/50981 (83%)]\tLoss: 10003708.667674\n",
      "Train Epoch: 42 [43520/50981 (85%)]\tLoss: 10007423.803519\n",
      "Train Epoch: 42 [44800/50981 (88%)]\tLoss: 10009365.575499\n",
      "Train Epoch: 42 [46080/50981 (90%)]\tLoss: 10014644.556787\n",
      "Train Epoch: 42 [47360/50981 (93%)]\tLoss: 10018798.592992\n",
      "Train Epoch: 42 [48640/50981 (95%)]\tLoss: 10022558.931759\n",
      "Train Epoch: 42 [49920/50981 (98%)]\tLoss: 10025533.030691\n",
      "Train Epoch: 43 [0/50981 (0%)]\tLoss: 10408772.000000\n",
      "Train Epoch: 43 [1280/50981 (3%)]\tLoss: 9663357.909091\n",
      "Train Epoch: 43 [2560/50981 (5%)]\tLoss: 9608542.000000\n",
      "Train Epoch: 43 [3840/50981 (8%)]\tLoss: 9675894.451613\n",
      "Train Epoch: 43 [5120/50981 (10%)]\tLoss: 9680636.243902\n",
      "Train Epoch: 43 [6400/50981 (13%)]\tLoss: 9704628.627451\n",
      "Train Epoch: 43 [7680/50981 (15%)]\tLoss: 9705485.819672\n",
      "Train Epoch: 43 [8960/50981 (18%)]\tLoss: 9707803.845070\n",
      "Train Epoch: 43 [10240/50981 (20%)]\tLoss: 9728094.604938\n",
      "Train Epoch: 43 [11520/50981 (23%)]\tLoss: 9751940.571429\n",
      "Train Epoch: 43 [12800/50981 (25%)]\tLoss: 9752661.851485\n",
      "Train Epoch: 43 [14080/50981 (28%)]\tLoss: 9744005.549550\n",
      "Train Epoch: 43 [15360/50981 (30%)]\tLoss: 9767913.066116\n",
      "Train Epoch: 43 [16640/50981 (33%)]\tLoss: 9776501.343511\n",
      "Train Epoch: 43 [17920/50981 (35%)]\tLoss: 9788138.843972\n",
      "Train Epoch: 43 [19200/50981 (38%)]\tLoss: 9814112.933775\n",
      "Train Epoch: 43 [20480/50981 (40%)]\tLoss: 9833806.434783\n",
      "Train Epoch: 43 [21760/50981 (43%)]\tLoss: 9841009.046784\n",
      "Train Epoch: 43 [23040/50981 (45%)]\tLoss: 9841717.314917\n",
      "Train Epoch: 43 [24320/50981 (48%)]\tLoss: 9850187.015707\n",
      "Train Epoch: 43 [25600/50981 (50%)]\tLoss: 9858725.875622\n",
      "Train Epoch: 43 [26880/50981 (53%)]\tLoss: 9856683.156398\n",
      "Train Epoch: 43 [28160/50981 (55%)]\tLoss: 9875714.995475\n",
      "Train Epoch: 43 [29440/50981 (58%)]\tLoss: 9881274.255411\n",
      "Train Epoch: 43 [30720/50981 (60%)]\tLoss: 9889163.053942\n",
      "Train Epoch: 43 [32000/50981 (63%)]\tLoss: 9892303.980080\n",
      "Train Epoch: 43 [33280/50981 (65%)]\tLoss: 9895732.111111\n",
      "Train Epoch: 43 [34560/50981 (68%)]\tLoss: 9900804.719557\n",
      "Train Epoch: 43 [35840/50981 (70%)]\tLoss: 9908180.758007\n",
      "Train Epoch: 43 [37120/50981 (73%)]\tLoss: 9915629.790378\n",
      "Train Epoch: 43 [38400/50981 (75%)]\tLoss: 9912193.813953\n",
      "Train Epoch: 43 [39680/50981 (78%)]\tLoss: 9918246.192926\n",
      "Train Epoch: 43 [40960/50981 (80%)]\tLoss: 9924275.725857\n",
      "Train Epoch: 43 [42240/50981 (83%)]\tLoss: 9931002.326284\n",
      "Train Epoch: 43 [43520/50981 (85%)]\tLoss: 9929748.973607\n",
      "Train Epoch: 43 [44800/50981 (88%)]\tLoss: 9937157.564103\n",
      "Train Epoch: 43 [46080/50981 (90%)]\tLoss: 9940405.950139\n",
      "Train Epoch: 43 [47360/50981 (93%)]\tLoss: 9944800.770889\n",
      "Train Epoch: 43 [48640/50981 (95%)]\tLoss: 9949659.829396\n",
      "Train Epoch: 43 [49920/50981 (98%)]\tLoss: 9952601.337596\n",
      "Train Epoch: 44 [0/50981 (0%)]\tLoss: 10041208.000000\n",
      "Train Epoch: 44 [1280/50981 (3%)]\tLoss: 9708402.363636\n",
      "Train Epoch: 44 [2560/50981 (5%)]\tLoss: 9682727.142857\n",
      "Train Epoch: 44 [3840/50981 (8%)]\tLoss: 9661307.451613\n",
      "Train Epoch: 44 [5120/50981 (10%)]\tLoss: 9695542.804878\n",
      "Train Epoch: 44 [6400/50981 (13%)]\tLoss: 9716401.862745\n",
      "Train Epoch: 44 [7680/50981 (15%)]\tLoss: 9698949.426230\n",
      "Train Epoch: 44 [8960/50981 (18%)]\tLoss: 9715880.464789\n",
      "Train Epoch: 44 [10240/50981 (20%)]\tLoss: 9721637.148148\n",
      "Train Epoch: 44 [11520/50981 (23%)]\tLoss: 9725000.527473\n",
      "Train Epoch: 44 [12800/50981 (25%)]\tLoss: 9731838.297030\n",
      "Train Epoch: 44 [14080/50981 (28%)]\tLoss: 9744679.180180\n",
      "Train Epoch: 44 [15360/50981 (30%)]\tLoss: 9718531.041322\n",
      "Train Epoch: 44 [16640/50981 (33%)]\tLoss: 9710429.068702\n",
      "Train Epoch: 44 [17920/50981 (35%)]\tLoss: 9715017.014184\n",
      "Train Epoch: 44 [19200/50981 (38%)]\tLoss: 9728863.370861\n",
      "Train Epoch: 44 [20480/50981 (40%)]\tLoss: 9725663.739130\n",
      "Train Epoch: 44 [21760/50981 (43%)]\tLoss: 9743901.274854\n",
      "Train Epoch: 44 [23040/50981 (45%)]\tLoss: 9761489.143646\n",
      "Train Epoch: 44 [24320/50981 (48%)]\tLoss: 9786617.759162\n",
      "Train Epoch: 44 [25600/50981 (50%)]\tLoss: 9802612.477612\n",
      "Train Epoch: 44 [26880/50981 (53%)]\tLoss: 9803554.222749\n",
      "Train Epoch: 44 [28160/50981 (55%)]\tLoss: 9800195.954751\n",
      "Train Epoch: 44 [29440/50981 (58%)]\tLoss: 9805032.800866\n",
      "Train Epoch: 44 [30720/50981 (60%)]\tLoss: 9811103.622407\n",
      "Train Epoch: 44 [32000/50981 (63%)]\tLoss: 9812173.629482\n",
      "Train Epoch: 44 [33280/50981 (65%)]\tLoss: 9825306.674330\n",
      "Train Epoch: 44 [34560/50981 (68%)]\tLoss: 9826402.346863\n",
      "Train Epoch: 44 [35840/50981 (70%)]\tLoss: 9828823.669039\n",
      "Train Epoch: 44 [37120/50981 (73%)]\tLoss: 9833346.553265\n",
      "Train Epoch: 44 [38400/50981 (75%)]\tLoss: 9836946.249169\n",
      "Train Epoch: 44 [39680/50981 (78%)]\tLoss: 9849867.723473\n",
      "Train Epoch: 44 [40960/50981 (80%)]\tLoss: 9855012.831776\n",
      "Train Epoch: 44 [42240/50981 (83%)]\tLoss: 9859488.731118\n",
      "Train Epoch: 44 [43520/50981 (85%)]\tLoss: 9857858.108504\n",
      "Train Epoch: 44 [44800/50981 (88%)]\tLoss: 9868965.253561\n",
      "Train Epoch: 44 [46080/50981 (90%)]\tLoss: 9870228.714681\n",
      "Train Epoch: 44 [47360/50981 (93%)]\tLoss: 9875245.396226\n",
      "Train Epoch: 44 [48640/50981 (95%)]\tLoss: 9880279.787402\n",
      "Train Epoch: 44 [49920/50981 (98%)]\tLoss: 9888142.861893\n",
      "Train Epoch: 45 [0/50981 (0%)]\tLoss: 9298406.000000\n",
      "Train Epoch: 45 [1280/50981 (3%)]\tLoss: 9577531.454545\n",
      "Train Epoch: 45 [2560/50981 (5%)]\tLoss: 9589147.523810\n",
      "Train Epoch: 45 [3840/50981 (8%)]\tLoss: 9665374.225806\n",
      "Train Epoch: 45 [5120/50981 (10%)]\tLoss: 9700969.219512\n",
      "Train Epoch: 45 [6400/50981 (13%)]\tLoss: 9739914.117647\n",
      "Train Epoch: 45 [7680/50981 (15%)]\tLoss: 9726260.278689\n",
      "Train Epoch: 45 [8960/50981 (18%)]\tLoss: 9696894.492958\n",
      "Train Epoch: 45 [10240/50981 (20%)]\tLoss: 9703258.629630\n",
      "Train Epoch: 45 [11520/50981 (23%)]\tLoss: 9714343.219780\n",
      "Train Epoch: 45 [12800/50981 (25%)]\tLoss: 9705101.623762\n",
      "Train Epoch: 45 [14080/50981 (28%)]\tLoss: 9703337.648649\n",
      "Train Epoch: 45 [15360/50981 (30%)]\tLoss: 9707017.107438\n",
      "Train Epoch: 45 [16640/50981 (33%)]\tLoss: 9700526.122137\n",
      "Train Epoch: 45 [17920/50981 (35%)]\tLoss: 9700449.872340\n",
      "Train Epoch: 45 [19200/50981 (38%)]\tLoss: 9705979.251656\n",
      "Train Epoch: 45 [20480/50981 (40%)]\tLoss: 9707487.608696\n",
      "Train Epoch: 45 [21760/50981 (43%)]\tLoss: 9716449.362573\n",
      "Train Epoch: 45 [23040/50981 (45%)]\tLoss: 9720661.657459\n",
      "Train Epoch: 45 [24320/50981 (48%)]\tLoss: 9728356.455497\n",
      "Train Epoch: 45 [25600/50981 (50%)]\tLoss: 9746049.815920\n",
      "Train Epoch: 45 [26880/50981 (53%)]\tLoss: 9749357.886256\n",
      "Train Epoch: 45 [28160/50981 (55%)]\tLoss: 9757650.339367\n",
      "Train Epoch: 45 [29440/50981 (58%)]\tLoss: 9759089.913420\n",
      "Train Epoch: 45 [30720/50981 (60%)]\tLoss: 9759703.829876\n",
      "Train Epoch: 45 [32000/50981 (63%)]\tLoss: 9760802.107570\n",
      "Train Epoch: 45 [33280/50981 (65%)]\tLoss: 9773688.233716\n",
      "Train Epoch: 45 [34560/50981 (68%)]\tLoss: 9771377.036900\n",
      "Train Epoch: 45 [35840/50981 (70%)]\tLoss: 9775586.829181\n",
      "Train Epoch: 45 [37120/50981 (73%)]\tLoss: 9785448.608247\n",
      "Train Epoch: 45 [38400/50981 (75%)]\tLoss: 9789758.431894\n",
      "Train Epoch: 45 [39680/50981 (78%)]\tLoss: 9791117.643087\n",
      "Train Epoch: 45 [40960/50981 (80%)]\tLoss: 9790014.950156\n",
      "Train Epoch: 45 [42240/50981 (83%)]\tLoss: 9790083.383686\n",
      "Train Epoch: 45 [43520/50981 (85%)]\tLoss: 9795906.912023\n",
      "Train Epoch: 45 [44800/50981 (88%)]\tLoss: 9800824.584046\n",
      "Train Epoch: 45 [46080/50981 (90%)]\tLoss: 9811847.415512\n",
      "Train Epoch: 45 [47360/50981 (93%)]\tLoss: 9820077.784367\n",
      "Train Epoch: 45 [48640/50981 (95%)]\tLoss: 9819129.005249\n",
      "Train Epoch: 45 [49920/50981 (98%)]\tLoss: 9824141.616368\n",
      "Train Epoch: 46 [0/50981 (0%)]\tLoss: 9687325.000000\n",
      "Train Epoch: 46 [1280/50981 (3%)]\tLoss: 9380207.818182\n",
      "Train Epoch: 46 [2560/50981 (5%)]\tLoss: 9514344.238095\n",
      "Train Epoch: 46 [3840/50981 (8%)]\tLoss: 9494743.903226\n",
      "Train Epoch: 46 [5120/50981 (10%)]\tLoss: 9521096.975610\n",
      "Train Epoch: 46 [6400/50981 (13%)]\tLoss: 9562007.705882\n",
      "Train Epoch: 46 [7680/50981 (15%)]\tLoss: 9570259.557377\n",
      "Train Epoch: 46 [8960/50981 (18%)]\tLoss: 9597944.859155\n",
      "Train Epoch: 46 [10240/50981 (20%)]\tLoss: 9616480.358025\n",
      "Train Epoch: 46 [11520/50981 (23%)]\tLoss: 9607995.989011\n",
      "Train Epoch: 46 [12800/50981 (25%)]\tLoss: 9600812.029703\n",
      "Train Epoch: 46 [14080/50981 (28%)]\tLoss: 9574968.468468\n",
      "Train Epoch: 46 [15360/50981 (30%)]\tLoss: 9578791.173554\n",
      "Train Epoch: 46 [16640/50981 (33%)]\tLoss: 9580369.946565\n",
      "Train Epoch: 46 [17920/50981 (35%)]\tLoss: 9577456.375887\n",
      "Train Epoch: 46 [19200/50981 (38%)]\tLoss: 9587096.490066\n",
      "Train Epoch: 46 [20480/50981 (40%)]\tLoss: 9608932.683230\n",
      "Train Epoch: 46 [21760/50981 (43%)]\tLoss: 9610499.146199\n",
      "Train Epoch: 46 [23040/50981 (45%)]\tLoss: 9623583.309392\n",
      "Train Epoch: 46 [24320/50981 (48%)]\tLoss: 9634130.748691\n",
      "Train Epoch: 46 [25600/50981 (50%)]\tLoss: 9637582.646766\n",
      "Train Epoch: 46 [26880/50981 (53%)]\tLoss: 9650078.710900\n",
      "Train Epoch: 46 [28160/50981 (55%)]\tLoss: 9661132.769231\n",
      "Train Epoch: 46 [29440/50981 (58%)]\tLoss: 9671178.571429\n",
      "Train Epoch: 46 [30720/50981 (60%)]\tLoss: 9674075.153527\n",
      "Train Epoch: 46 [32000/50981 (63%)]\tLoss: 9671229.414343\n",
      "Train Epoch: 46 [33280/50981 (65%)]\tLoss: 9676543.689655\n",
      "Train Epoch: 46 [34560/50981 (68%)]\tLoss: 9690744.184502\n",
      "Train Epoch: 46 [35840/50981 (70%)]\tLoss: 9698418.227758\n",
      "Train Epoch: 46 [37120/50981 (73%)]\tLoss: 9701199.948454\n",
      "Train Epoch: 46 [38400/50981 (75%)]\tLoss: 9703684.405316\n",
      "Train Epoch: 46 [39680/50981 (78%)]\tLoss: 9715620.453376\n",
      "Train Epoch: 46 [40960/50981 (80%)]\tLoss: 9721000.380062\n",
      "Train Epoch: 46 [42240/50981 (83%)]\tLoss: 9724446.344411\n",
      "Train Epoch: 46 [43520/50981 (85%)]\tLoss: 9729251.958944\n",
      "Train Epoch: 46 [44800/50981 (88%)]\tLoss: 9734905.495726\n",
      "Train Epoch: 46 [46080/50981 (90%)]\tLoss: 9744757.714681\n",
      "Train Epoch: 46 [47360/50981 (93%)]\tLoss: 9750349.792453\n",
      "Train Epoch: 46 [48640/50981 (95%)]\tLoss: 9757211.779528\n",
      "Train Epoch: 46 [49920/50981 (98%)]\tLoss: 9764762.117647\n",
      "Train Epoch: 47 [0/50981 (0%)]\tLoss: 10017651.000000\n",
      "Train Epoch: 47 [1280/50981 (3%)]\tLoss: 9596092.545455\n",
      "Train Epoch: 47 [2560/50981 (5%)]\tLoss: 9666444.285714\n",
      "Train Epoch: 47 [3840/50981 (8%)]\tLoss: 9569432.677419\n",
      "Train Epoch: 47 [5120/50981 (10%)]\tLoss: 9561255.658537\n",
      "Train Epoch: 47 [6400/50981 (13%)]\tLoss: 9555426.313725\n",
      "Train Epoch: 47 [7680/50981 (15%)]\tLoss: 9574453.704918\n",
      "Train Epoch: 47 [8960/50981 (18%)]\tLoss: 9569890.732394\n",
      "Train Epoch: 47 [10240/50981 (20%)]\tLoss: 9577102.172840\n",
      "Train Epoch: 47 [11520/50981 (23%)]\tLoss: 9591302.725275\n",
      "Train Epoch: 47 [12800/50981 (25%)]\tLoss: 9588893.712871\n",
      "Train Epoch: 47 [14080/50981 (28%)]\tLoss: 9615671.927928\n",
      "Train Epoch: 47 [15360/50981 (30%)]\tLoss: 9628955.520661\n",
      "Train Epoch: 47 [16640/50981 (33%)]\tLoss: 9649516.572519\n",
      "Train Epoch: 47 [17920/50981 (35%)]\tLoss: 9655567.361702\n",
      "Train Epoch: 47 [19200/50981 (38%)]\tLoss: 9658149.807947\n",
      "Train Epoch: 47 [20480/50981 (40%)]\tLoss: 9651419.677019\n",
      "Train Epoch: 47 [21760/50981 (43%)]\tLoss: 9652400.543860\n",
      "Train Epoch: 47 [23040/50981 (45%)]\tLoss: 9672847.972376\n",
      "Train Epoch: 47 [24320/50981 (48%)]\tLoss: 9662974.418848\n",
      "Train Epoch: 47 [25600/50981 (50%)]\tLoss: 9659627.776119\n",
      "Train Epoch: 47 [26880/50981 (53%)]\tLoss: 9666145.502370\n",
      "Train Epoch: 47 [28160/50981 (55%)]\tLoss: 9657351.194570\n",
      "Train Epoch: 47 [29440/50981 (58%)]\tLoss: 9668354.428571\n",
      "Train Epoch: 47 [30720/50981 (60%)]\tLoss: 9670115.431535\n",
      "Train Epoch: 47 [32000/50981 (63%)]\tLoss: 9673232.139442\n",
      "Train Epoch: 47 [33280/50981 (65%)]\tLoss: 9672991.486590\n",
      "Train Epoch: 47 [34560/50981 (68%)]\tLoss: 9677239.243542\n",
      "Train Epoch: 47 [35840/50981 (70%)]\tLoss: 9678529.829181\n",
      "Train Epoch: 47 [37120/50981 (73%)]\tLoss: 9685825.721649\n",
      "Train Epoch: 47 [38400/50981 (75%)]\tLoss: 9683824.172757\n",
      "Train Epoch: 47 [39680/50981 (78%)]\tLoss: 9684407.189711\n",
      "Train Epoch: 47 [40960/50981 (80%)]\tLoss: 9693874.087227\n",
      "Train Epoch: 47 [42240/50981 (83%)]\tLoss: 9709604.661631\n",
      "Train Epoch: 47 [43520/50981 (85%)]\tLoss: 9714154.756598\n",
      "Train Epoch: 47 [44800/50981 (88%)]\tLoss: 9715876.623932\n",
      "Train Epoch: 47 [46080/50981 (90%)]\tLoss: 9716445.063712\n",
      "Train Epoch: 47 [47360/50981 (93%)]\tLoss: 9713813.105121\n",
      "Train Epoch: 47 [48640/50981 (95%)]\tLoss: 9711168.629921\n",
      "Train Epoch: 47 [49920/50981 (98%)]\tLoss: 9715819.102302\n",
      "Train Epoch: 48 [0/50981 (0%)]\tLoss: 9678526.000000\n",
      "Train Epoch: 48 [1280/50981 (3%)]\tLoss: 9401663.363636\n",
      "Train Epoch: 48 [2560/50981 (5%)]\tLoss: 9374128.809524\n",
      "Train Epoch: 48 [3840/50981 (8%)]\tLoss: 9489666.096774\n",
      "Train Epoch: 48 [5120/50981 (10%)]\tLoss: 9471205.195122\n",
      "Train Epoch: 48 [6400/50981 (13%)]\tLoss: 9503161.333333\n",
      "Train Epoch: 48 [7680/50981 (15%)]\tLoss: 9474633.245902\n",
      "Train Epoch: 48 [8960/50981 (18%)]\tLoss: 9507956.605634\n",
      "Train Epoch: 48 [10240/50981 (20%)]\tLoss: 9530311.925926\n",
      "Train Epoch: 48 [11520/50981 (23%)]\tLoss: 9525448.098901\n",
      "Train Epoch: 48 [12800/50981 (25%)]\tLoss: 9540594.673267\n",
      "Train Epoch: 48 [14080/50981 (28%)]\tLoss: 9549856.207207\n",
      "Train Epoch: 48 [15360/50981 (30%)]\tLoss: 9544285.942149\n",
      "Train Epoch: 48 [16640/50981 (33%)]\tLoss: 9554714.816794\n",
      "Train Epoch: 48 [17920/50981 (35%)]\tLoss: 9553071.723404\n",
      "Train Epoch: 48 [19200/50981 (38%)]\tLoss: 9563896.443709\n",
      "Train Epoch: 48 [20480/50981 (40%)]\tLoss: 9554923.180124\n",
      "Train Epoch: 48 [21760/50981 (43%)]\tLoss: 9550246.286550\n",
      "Train Epoch: 48 [23040/50981 (45%)]\tLoss: 9563376.823204\n",
      "Train Epoch: 48 [24320/50981 (48%)]\tLoss: 9562924.717277\n",
      "Train Epoch: 48 [25600/50981 (50%)]\tLoss: 9580294.975124\n",
      "Train Epoch: 48 [26880/50981 (53%)]\tLoss: 9582762.383886\n",
      "Train Epoch: 48 [28160/50981 (55%)]\tLoss: 9581483.511312\n",
      "Train Epoch: 48 [29440/50981 (58%)]\tLoss: 9580327.532468\n",
      "Train Epoch: 48 [30720/50981 (60%)]\tLoss: 9590475.033195\n",
      "Train Epoch: 48 [32000/50981 (63%)]\tLoss: 9595254.820717\n",
      "Train Epoch: 48 [33280/50981 (65%)]\tLoss: 9599498.666667\n",
      "Train Epoch: 48 [34560/50981 (68%)]\tLoss: 9602718.804428\n",
      "Train Epoch: 48 [35840/50981 (70%)]\tLoss: 9605516.110320\n",
      "Train Epoch: 48 [37120/50981 (73%)]\tLoss: 9605684.642612\n",
      "Train Epoch: 48 [38400/50981 (75%)]\tLoss: 9606399.013289\n",
      "Train Epoch: 48 [39680/50981 (78%)]\tLoss: 9619323.858521\n",
      "Train Epoch: 48 [40960/50981 (80%)]\tLoss: 9625596.803738\n",
      "Train Epoch: 48 [42240/50981 (83%)]\tLoss: 9627012.664653\n",
      "Train Epoch: 48 [43520/50981 (85%)]\tLoss: 9629303.387097\n",
      "Train Epoch: 48 [44800/50981 (88%)]\tLoss: 9634539.974359\n",
      "Train Epoch: 48 [46080/50981 (90%)]\tLoss: 9644027.476454\n",
      "Train Epoch: 48 [47360/50981 (93%)]\tLoss: 9649128.816712\n",
      "Train Epoch: 48 [48640/50981 (95%)]\tLoss: 9650359.091864\n",
      "Train Epoch: 48 [49920/50981 (98%)]\tLoss: 9657666.923274\n",
      "Train Epoch: 49 [0/50981 (0%)]\tLoss: 9913323.000000\n",
      "Train Epoch: 49 [1280/50981 (3%)]\tLoss: 9394561.363636\n",
      "Train Epoch: 49 [2560/50981 (5%)]\tLoss: 9462317.666667\n",
      "Train Epoch: 49 [3840/50981 (8%)]\tLoss: 9448235.483871\n",
      "Train Epoch: 49 [5120/50981 (10%)]\tLoss: 9479704.292683\n",
      "Train Epoch: 49 [6400/50981 (13%)]\tLoss: 9421809.352941\n",
      "Train Epoch: 49 [7680/50981 (15%)]\tLoss: 9435266.491803\n",
      "Train Epoch: 49 [8960/50981 (18%)]\tLoss: 9436781.380282\n",
      "Train Epoch: 49 [10240/50981 (20%)]\tLoss: 9441897.382716\n",
      "Train Epoch: 49 [11520/50981 (23%)]\tLoss: 9441323.417582\n",
      "Train Epoch: 49 [12800/50981 (25%)]\tLoss: 9448986.950495\n",
      "Train Epoch: 49 [14080/50981 (28%)]\tLoss: 9454765.756757\n",
      "Train Epoch: 49 [15360/50981 (30%)]\tLoss: 9448543.669421\n",
      "Train Epoch: 49 [16640/50981 (33%)]\tLoss: 9453067.076336\n",
      "Train Epoch: 49 [17920/50981 (35%)]\tLoss: 9447670.531915\n",
      "Train Epoch: 49 [19200/50981 (38%)]\tLoss: 9447719.781457\n",
      "Train Epoch: 49 [20480/50981 (40%)]\tLoss: 9447935.086957\n",
      "Train Epoch: 49 [21760/50981 (43%)]\tLoss: 9447491.707602\n",
      "Train Epoch: 49 [23040/50981 (45%)]\tLoss: 9459148.403315\n",
      "Train Epoch: 49 [24320/50981 (48%)]\tLoss: 9466356.010471\n",
      "Train Epoch: 49 [25600/50981 (50%)]\tLoss: 9456943.572139\n",
      "Train Epoch: 49 [26880/50981 (53%)]\tLoss: 9466939.364929\n",
      "Train Epoch: 49 [28160/50981 (55%)]\tLoss: 9471436.733032\n",
      "Train Epoch: 49 [29440/50981 (58%)]\tLoss: 9477857.705628\n",
      "Train Epoch: 49 [30720/50981 (60%)]\tLoss: 9478603.767635\n",
      "Train Epoch: 49 [32000/50981 (63%)]\tLoss: 9491397.749004\n",
      "Train Epoch: 49 [33280/50981 (65%)]\tLoss: 9497765.781609\n",
      "Train Epoch: 49 [34560/50981 (68%)]\tLoss: 9509153.988930\n",
      "Train Epoch: 49 [35840/50981 (70%)]\tLoss: 9525635.964413\n",
      "Train Epoch: 49 [37120/50981 (73%)]\tLoss: 9534694.862543\n",
      "Train Epoch: 49 [38400/50981 (75%)]\tLoss: 9536886.903654\n",
      "Train Epoch: 49 [39680/50981 (78%)]\tLoss: 9540885.102894\n",
      "Train Epoch: 49 [40960/50981 (80%)]\tLoss: 9544225.906542\n",
      "Train Epoch: 49 [42240/50981 (83%)]\tLoss: 9543551.356495\n",
      "Train Epoch: 49 [43520/50981 (85%)]\tLoss: 9548867.454545\n",
      "Train Epoch: 49 [44800/50981 (88%)]\tLoss: 9554782.566952\n",
      "Train Epoch: 49 [46080/50981 (90%)]\tLoss: 9557997.072022\n",
      "Train Epoch: 49 [47360/50981 (93%)]\tLoss: 9559901.215633\n",
      "Train Epoch: 49 [48640/50981 (95%)]\tLoss: 9567021.202100\n",
      "Train Epoch: 49 [49920/50981 (98%)]\tLoss: 9579217.843990\n",
      "Train Epoch: 50 [0/50981 (0%)]\tLoss: 8819411.000000\n",
      "Train Epoch: 50 [1280/50981 (3%)]\tLoss: 9145784.909091\n",
      "Train Epoch: 50 [2560/50981 (5%)]\tLoss: 9203765.047619\n",
      "Train Epoch: 50 [3840/50981 (8%)]\tLoss: 9242044.935484\n",
      "Train Epoch: 50 [5120/50981 (10%)]\tLoss: 9269131.804878\n",
      "Train Epoch: 50 [6400/50981 (13%)]\tLoss: 9269352.627451\n",
      "Train Epoch: 50 [7680/50981 (15%)]\tLoss: 9278878.885246\n",
      "Train Epoch: 50 [8960/50981 (18%)]\tLoss: 9267334.239437\n",
      "Train Epoch: 50 [10240/50981 (20%)]\tLoss: 9287012.197531\n",
      "Train Epoch: 50 [11520/50981 (23%)]\tLoss: 9289415.637363\n",
      "Train Epoch: 50 [12800/50981 (25%)]\tLoss: 9329413.099010\n",
      "Train Epoch: 50 [14080/50981 (28%)]\tLoss: 9351038.450450\n",
      "Train Epoch: 50 [15360/50981 (30%)]\tLoss: 9359440.603306\n",
      "Train Epoch: 50 [16640/50981 (33%)]\tLoss: 9382625.396947\n",
      "Train Epoch: 50 [17920/50981 (35%)]\tLoss: 9385836.865248\n",
      "Train Epoch: 50 [19200/50981 (38%)]\tLoss: 9389598.463576\n",
      "Train Epoch: 50 [20480/50981 (40%)]\tLoss: 9419886.720497\n",
      "Train Epoch: 50 [21760/50981 (43%)]\tLoss: 9417323.894737\n",
      "Train Epoch: 50 [23040/50981 (45%)]\tLoss: 9419034.154696\n",
      "Train Epoch: 50 [24320/50981 (48%)]\tLoss: 9414125.329843\n",
      "Train Epoch: 50 [25600/50981 (50%)]\tLoss: 9425040.766169\n",
      "Train Epoch: 50 [26880/50981 (53%)]\tLoss: 9430340.881517\n",
      "Train Epoch: 50 [28160/50981 (55%)]\tLoss: 9434209.638009\n",
      "Train Epoch: 50 [29440/50981 (58%)]\tLoss: 9432231.662338\n",
      "Train Epoch: 50 [30720/50981 (60%)]\tLoss: 9439164.717842\n",
      "Train Epoch: 50 [32000/50981 (63%)]\tLoss: 9446171.717131\n",
      "Train Epoch: 50 [33280/50981 (65%)]\tLoss: 9453157.559387\n",
      "Train Epoch: 50 [34560/50981 (68%)]\tLoss: 9463553.121771\n",
      "Train Epoch: 50 [35840/50981 (70%)]\tLoss: 9468075.822064\n",
      "Train Epoch: 50 [37120/50981 (73%)]\tLoss: 9476273.020619\n",
      "Train Epoch: 50 [38400/50981 (75%)]\tLoss: 9484135.780731\n",
      "Train Epoch: 50 [39680/50981 (78%)]\tLoss: 9491950.176849\n",
      "Train Epoch: 50 [40960/50981 (80%)]\tLoss: 9497857.177570\n",
      "Train Epoch: 50 [42240/50981 (83%)]\tLoss: 9502870.356495\n",
      "Train Epoch: 50 [43520/50981 (85%)]\tLoss: 9506063.533724\n",
      "Train Epoch: 50 [44800/50981 (88%)]\tLoss: 9512283.905983\n",
      "Train Epoch: 50 [46080/50981 (90%)]\tLoss: 9514148.224377\n",
      "Train Epoch: 50 [47360/50981 (93%)]\tLoss: 9522885.679245\n",
      "Train Epoch: 50 [48640/50981 (95%)]\tLoss: 9528179.398950\n",
      "Train Epoch: 50 [49920/50981 (98%)]\tLoss: 9533878.342711\n",
      "Train Epoch: 51 [0/50981 (0%)]\tLoss: 9642110.000000\n",
      "Train Epoch: 51 [1280/50981 (3%)]\tLoss: 9194801.818182\n",
      "Train Epoch: 51 [2560/50981 (5%)]\tLoss: 9211191.142857\n",
      "Train Epoch: 51 [3840/50981 (8%)]\tLoss: 9188479.032258\n",
      "Train Epoch: 51 [5120/50981 (10%)]\tLoss: 9195197.219512\n",
      "Train Epoch: 51 [6400/50981 (13%)]\tLoss: 9224231.882353\n",
      "Train Epoch: 51 [7680/50981 (15%)]\tLoss: 9276775.213115\n",
      "Train Epoch: 51 [8960/50981 (18%)]\tLoss: 9312695.154930\n",
      "Train Epoch: 51 [10240/50981 (20%)]\tLoss: 9342601.567901\n",
      "Train Epoch: 51 [11520/50981 (23%)]\tLoss: 9381704.923077\n",
      "Train Epoch: 51 [12800/50981 (25%)]\tLoss: 9374798.564356\n",
      "Train Epoch: 51 [14080/50981 (28%)]\tLoss: 9386081.045045\n",
      "Train Epoch: 51 [15360/50981 (30%)]\tLoss: 9386307.677686\n",
      "Train Epoch: 51 [16640/50981 (33%)]\tLoss: 9389204.251908\n",
      "Train Epoch: 51 [17920/50981 (35%)]\tLoss: 9386935.304965\n",
      "Train Epoch: 51 [19200/50981 (38%)]\tLoss: 9394619.470199\n",
      "Train Epoch: 51 [20480/50981 (40%)]\tLoss: 9399873.403727\n",
      "Train Epoch: 51 [21760/50981 (43%)]\tLoss: 9395548.298246\n",
      "Train Epoch: 51 [23040/50981 (45%)]\tLoss: 9403994.850829\n",
      "Train Epoch: 51 [24320/50981 (48%)]\tLoss: 9406465.198953\n",
      "Train Epoch: 51 [25600/50981 (50%)]\tLoss: 9417475.870647\n",
      "Train Epoch: 51 [26880/50981 (53%)]\tLoss: 9416810.620853\n",
      "Train Epoch: 51 [28160/50981 (55%)]\tLoss: 9413789.828054\n",
      "Train Epoch: 51 [29440/50981 (58%)]\tLoss: 9416716.229437\n",
      "Train Epoch: 51 [30720/50981 (60%)]\tLoss: 9414056.377593\n",
      "Train Epoch: 51 [32000/50981 (63%)]\tLoss: 9422067.569721\n",
      "Train Epoch: 51 [33280/50981 (65%)]\tLoss: 9427688.517241\n",
      "Train Epoch: 51 [34560/50981 (68%)]\tLoss: 9430214.055351\n",
      "Train Epoch: 51 [35840/50981 (70%)]\tLoss: 9427942.032028\n",
      "Train Epoch: 51 [37120/50981 (73%)]\tLoss: 9432137.718213\n",
      "Train Epoch: 51 [38400/50981 (75%)]\tLoss: 9436774.056478\n",
      "Train Epoch: 51 [39680/50981 (78%)]\tLoss: 9442332.157556\n",
      "Train Epoch: 51 [40960/50981 (80%)]\tLoss: 9448679.548287\n",
      "Train Epoch: 51 [42240/50981 (83%)]\tLoss: 9455570.018127\n",
      "Train Epoch: 51 [43520/50981 (85%)]\tLoss: 9463096.120235\n",
      "Train Epoch: 51 [44800/50981 (88%)]\tLoss: 9468801.250712\n",
      "Train Epoch: 51 [46080/50981 (90%)]\tLoss: 9477306.196676\n",
      "Train Epoch: 51 [47360/50981 (93%)]\tLoss: 9480566.280323\n",
      "Train Epoch: 51 [48640/50981 (95%)]\tLoss: 9481131.955381\n",
      "Train Epoch: 51 [49920/50981 (98%)]\tLoss: 9485045.276215\n",
      "Train Epoch: 52 [0/50981 (0%)]\tLoss: 9673240.000000\n",
      "Train Epoch: 52 [1280/50981 (3%)]\tLoss: 9397998.363636\n",
      "Train Epoch: 52 [2560/50981 (5%)]\tLoss: 9357219.238095\n",
      "Train Epoch: 52 [3840/50981 (8%)]\tLoss: 9349435.064516\n",
      "Train Epoch: 52 [5120/50981 (10%)]\tLoss: 9337792.121951\n",
      "Train Epoch: 52 [6400/50981 (13%)]\tLoss: 9393968.901961\n",
      "Train Epoch: 52 [7680/50981 (15%)]\tLoss: 9425566.393443\n",
      "Train Epoch: 52 [8960/50981 (18%)]\tLoss: 9390464.802817\n",
      "Train Epoch: 52 [10240/50981 (20%)]\tLoss: 9359236.024691\n",
      "Train Epoch: 52 [11520/50981 (23%)]\tLoss: 9370462.428571\n",
      "Train Epoch: 52 [12800/50981 (25%)]\tLoss: 9365221.455446\n",
      "Train Epoch: 52 [14080/50981 (28%)]\tLoss: 9354894.756757\n",
      "Train Epoch: 52 [15360/50981 (30%)]\tLoss: 9365521.867769\n",
      "Train Epoch: 52 [16640/50981 (33%)]\tLoss: 9374619.274809\n",
      "Train Epoch: 52 [17920/50981 (35%)]\tLoss: 9374630.120567\n",
      "Train Epoch: 52 [19200/50981 (38%)]\tLoss: 9374139.496689\n",
      "Train Epoch: 52 [20480/50981 (40%)]\tLoss: 9380938.273292\n",
      "Train Epoch: 52 [21760/50981 (43%)]\tLoss: 9373883.812865\n",
      "Train Epoch: 52 [23040/50981 (45%)]\tLoss: 9366346.629834\n",
      "Train Epoch: 52 [24320/50981 (48%)]\tLoss: 9371394.586387\n",
      "Train Epoch: 52 [25600/50981 (50%)]\tLoss: 9374796.751244\n",
      "Train Epoch: 52 [26880/50981 (53%)]\tLoss: 9384878.563981\n",
      "Train Epoch: 52 [28160/50981 (55%)]\tLoss: 9388436.289593\n",
      "Train Epoch: 52 [29440/50981 (58%)]\tLoss: 9396626.580087\n",
      "Train Epoch: 52 [30720/50981 (60%)]\tLoss: 9398016.029046\n",
      "Train Epoch: 52 [32000/50981 (63%)]\tLoss: 9405587.685259\n",
      "Train Epoch: 52 [33280/50981 (65%)]\tLoss: 9405977.291188\n",
      "Train Epoch: 52 [34560/50981 (68%)]\tLoss: 9409313.147601\n",
      "Train Epoch: 52 [35840/50981 (70%)]\tLoss: 9416374.925267\n",
      "Train Epoch: 52 [37120/50981 (73%)]\tLoss: 9419134.914089\n",
      "Train Epoch: 52 [38400/50981 (75%)]\tLoss: 9421220.272425\n",
      "Train Epoch: 52 [39680/50981 (78%)]\tLoss: 9429041.440514\n",
      "Train Epoch: 52 [40960/50981 (80%)]\tLoss: 9431546.308411\n",
      "Train Epoch: 52 [42240/50981 (83%)]\tLoss: 9431109.277946\n",
      "Train Epoch: 52 [43520/50981 (85%)]\tLoss: 9436733.175953\n",
      "Train Epoch: 52 [44800/50981 (88%)]\tLoss: 9437883.031339\n",
      "Train Epoch: 52 [46080/50981 (90%)]\tLoss: 9438673.911357\n",
      "Train Epoch: 52 [47360/50981 (93%)]\tLoss: 9445473.266846\n",
      "Train Epoch: 52 [48640/50981 (95%)]\tLoss: 9444227.524934\n",
      "Train Epoch: 52 [49920/50981 (98%)]\tLoss: 9446348.833760\n",
      "Train Epoch: 53 [0/50981 (0%)]\tLoss: 9259094.000000\n",
      "Train Epoch: 53 [1280/50981 (3%)]\tLoss: 9405699.000000\n",
      "Train Epoch: 53 [2560/50981 (5%)]\tLoss: 9380950.809524\n",
      "Train Epoch: 53 [3840/50981 (8%)]\tLoss: 9336485.903226\n",
      "Train Epoch: 53 [5120/50981 (10%)]\tLoss: 9296500.048780\n",
      "Train Epoch: 53 [6400/50981 (13%)]\tLoss: 9317411.745098\n",
      "Train Epoch: 53 [7680/50981 (15%)]\tLoss: 9312586.786885\n",
      "Train Epoch: 53 [8960/50981 (18%)]\tLoss: 9295595.169014\n",
      "Train Epoch: 53 [10240/50981 (20%)]\tLoss: 9277060.567901\n",
      "Train Epoch: 53 [11520/50981 (23%)]\tLoss: 9288778.340659\n",
      "Train Epoch: 53 [12800/50981 (25%)]\tLoss: 9287308.405941\n",
      "Train Epoch: 53 [14080/50981 (28%)]\tLoss: 9296663.936937\n",
      "Train Epoch: 53 [15360/50981 (30%)]\tLoss: 9285075.033058\n",
      "Train Epoch: 53 [16640/50981 (33%)]\tLoss: 9296890.473282\n",
      "Train Epoch: 53 [17920/50981 (35%)]\tLoss: 9299352.567376\n",
      "Train Epoch: 53 [19200/50981 (38%)]\tLoss: 9316071.463576\n",
      "Train Epoch: 53 [20480/50981 (40%)]\tLoss: 9313045.111801\n",
      "Train Epoch: 53 [21760/50981 (43%)]\tLoss: 9312778.245614\n",
      "Train Epoch: 53 [23040/50981 (45%)]\tLoss: 9318897.232044\n",
      "Train Epoch: 53 [24320/50981 (48%)]\tLoss: 9323214.020942\n",
      "Train Epoch: 53 [25600/50981 (50%)]\tLoss: 9324907.388060\n",
      "Train Epoch: 53 [26880/50981 (53%)]\tLoss: 9333864.175355\n",
      "Train Epoch: 53 [28160/50981 (55%)]\tLoss: 9340023.515837\n",
      "Train Epoch: 53 [29440/50981 (58%)]\tLoss: 9343965.406926\n",
      "Train Epoch: 53 [30720/50981 (60%)]\tLoss: 9346207.410788\n",
      "Train Epoch: 53 [32000/50981 (63%)]\tLoss: 9352209.418327\n",
      "Train Epoch: 53 [33280/50981 (65%)]\tLoss: 9353855.934866\n",
      "Train Epoch: 53 [34560/50981 (68%)]\tLoss: 9356320.988930\n",
      "Train Epoch: 53 [35840/50981 (70%)]\tLoss: 9367213.046263\n",
      "Train Epoch: 53 [37120/50981 (73%)]\tLoss: 9365308.092784\n",
      "Train Epoch: 53 [38400/50981 (75%)]\tLoss: 9369792.451827\n",
      "Train Epoch: 53 [39680/50981 (78%)]\tLoss: 9378174.752412\n",
      "Train Epoch: 53 [40960/50981 (80%)]\tLoss: 9378670.149533\n",
      "Train Epoch: 53 [42240/50981 (83%)]\tLoss: 9382738.338369\n",
      "Train Epoch: 53 [43520/50981 (85%)]\tLoss: 9387144.756598\n",
      "Train Epoch: 53 [44800/50981 (88%)]\tLoss: 9391285.740741\n",
      "Train Epoch: 53 [46080/50981 (90%)]\tLoss: 9390010.506925\n",
      "Train Epoch: 53 [47360/50981 (93%)]\tLoss: 9393700.673854\n",
      "Train Epoch: 53 [48640/50981 (95%)]\tLoss: 9399998.895013\n",
      "Train Epoch: 53 [49920/50981 (98%)]\tLoss: 9399337.373402\n",
      "Train Epoch: 54 [0/50981 (0%)]\tLoss: 8838064.000000\n",
      "Train Epoch: 54 [1280/50981 (3%)]\tLoss: 9157728.818182\n",
      "Train Epoch: 54 [2560/50981 (5%)]\tLoss: 9228388.190476\n",
      "Train Epoch: 54 [3840/50981 (8%)]\tLoss: 9212739.419355\n",
      "Train Epoch: 54 [5120/50981 (10%)]\tLoss: 9236308.756098\n",
      "Train Epoch: 54 [6400/50981 (13%)]\tLoss: 9208356.137255\n",
      "Train Epoch: 54 [7680/50981 (15%)]\tLoss: 9266854.245902\n",
      "Train Epoch: 54 [8960/50981 (18%)]\tLoss: 9249729.450704\n",
      "Train Epoch: 54 [10240/50981 (20%)]\tLoss: 9237733.061728\n",
      "Train Epoch: 54 [11520/50981 (23%)]\tLoss: 9249433.230769\n",
      "Train Epoch: 54 [12800/50981 (25%)]\tLoss: 9256998.465347\n",
      "Train Epoch: 54 [14080/50981 (28%)]\tLoss: 9252677.855856\n",
      "Train Epoch: 54 [15360/50981 (30%)]\tLoss: 9246110.438017\n",
      "Train Epoch: 54 [16640/50981 (33%)]\tLoss: 9255731.145038\n",
      "Train Epoch: 54 [17920/50981 (35%)]\tLoss: 9255220.567376\n",
      "Train Epoch: 54 [19200/50981 (38%)]\tLoss: 9253984.596026\n",
      "Train Epoch: 54 [20480/50981 (40%)]\tLoss: 9261577.975155\n",
      "Train Epoch: 54 [21760/50981 (43%)]\tLoss: 9258690.081871\n",
      "Train Epoch: 54 [23040/50981 (45%)]\tLoss: 9265410.740331\n",
      "Train Epoch: 54 [24320/50981 (48%)]\tLoss: 9279731.502618\n",
      "Train Epoch: 54 [25600/50981 (50%)]\tLoss: 9290444.278607\n",
      "Train Epoch: 54 [26880/50981 (53%)]\tLoss: 9291121.383886\n",
      "Train Epoch: 54 [28160/50981 (55%)]\tLoss: 9276000.484163\n",
      "Train Epoch: 54 [29440/50981 (58%)]\tLoss: 9286604.194805\n",
      "Train Epoch: 54 [30720/50981 (60%)]\tLoss: 9292895.116183\n",
      "Train Epoch: 54 [32000/50981 (63%)]\tLoss: 9292223.828685\n",
      "Train Epoch: 54 [33280/50981 (65%)]\tLoss: 9296684.567050\n",
      "Train Epoch: 54 [34560/50981 (68%)]\tLoss: 9304502.712177\n",
      "Train Epoch: 54 [35840/50981 (70%)]\tLoss: 9309170.704626\n",
      "Train Epoch: 54 [37120/50981 (73%)]\tLoss: 9318106.285223\n",
      "Train Epoch: 54 [38400/50981 (75%)]\tLoss: 9318385.242525\n",
      "Train Epoch: 54 [39680/50981 (78%)]\tLoss: 9316087.128617\n",
      "Train Epoch: 54 [40960/50981 (80%)]\tLoss: 9320527.504673\n",
      "Train Epoch: 54 [42240/50981 (83%)]\tLoss: 9326943.800604\n",
      "Train Epoch: 54 [43520/50981 (85%)]\tLoss: 9331427.237537\n",
      "Train Epoch: 54 [44800/50981 (88%)]\tLoss: 9335657.099715\n",
      "Train Epoch: 54 [46080/50981 (90%)]\tLoss: 9338311.911357\n",
      "Train Epoch: 54 [47360/50981 (93%)]\tLoss: 9344086.304582\n",
      "Train Epoch: 54 [48640/50981 (95%)]\tLoss: 9342917.170604\n",
      "Train Epoch: 54 [49920/50981 (98%)]\tLoss: 9351280.813299\n",
      "Train Epoch: 55 [0/50981 (0%)]\tLoss: 8715045.000000\n",
      "Train Epoch: 55 [1280/50981 (3%)]\tLoss: 8805505.181818\n",
      "Train Epoch: 55 [2560/50981 (5%)]\tLoss: 8926711.523810\n",
      "Train Epoch: 55 [3840/50981 (8%)]\tLoss: 9026521.580645\n",
      "Train Epoch: 55 [5120/50981 (10%)]\tLoss: 9024635.658537\n",
      "Train Epoch: 55 [6400/50981 (13%)]\tLoss: 9039627.843137\n",
      "Train Epoch: 55 [7680/50981 (15%)]\tLoss: 9066322.770492\n",
      "Train Epoch: 55 [8960/50981 (18%)]\tLoss: 9084621.084507\n",
      "Train Epoch: 55 [10240/50981 (20%)]\tLoss: 9124558.000000\n",
      "Train Epoch: 55 [11520/50981 (23%)]\tLoss: 9145501.181319\n",
      "Train Epoch: 55 [12800/50981 (25%)]\tLoss: 9153362.272277\n",
      "Train Epoch: 55 [14080/50981 (28%)]\tLoss: 9150167.481982\n",
      "Train Epoch: 55 [15360/50981 (30%)]\tLoss: 9154270.095041\n",
      "Train Epoch: 55 [16640/50981 (33%)]\tLoss: 9177777.568702\n",
      "Train Epoch: 55 [17920/50981 (35%)]\tLoss: 9178024.599291\n",
      "Train Epoch: 55 [19200/50981 (38%)]\tLoss: 9194154.453642\n",
      "Train Epoch: 55 [20480/50981 (40%)]\tLoss: 9191068.239130\n",
      "Train Epoch: 55 [21760/50981 (43%)]\tLoss: 9191220.599415\n",
      "Train Epoch: 55 [23040/50981 (45%)]\tLoss: 9205464.765193\n",
      "Train Epoch: 55 [24320/50981 (48%)]\tLoss: 9205769.379581\n",
      "Train Epoch: 55 [25600/50981 (50%)]\tLoss: 9213513.022388\n",
      "Train Epoch: 55 [26880/50981 (53%)]\tLoss: 9217584.883886\n",
      "Train Epoch: 55 [28160/50981 (55%)]\tLoss: 9220932.798643\n",
      "Train Epoch: 55 [29440/50981 (58%)]\tLoss: 9221880.478355\n",
      "Train Epoch: 55 [30720/50981 (60%)]\tLoss: 9235858.047718\n",
      "Train Epoch: 55 [32000/50981 (63%)]\tLoss: 9241547.623506\n",
      "Train Epoch: 55 [33280/50981 (65%)]\tLoss: 9242487.369732\n",
      "Train Epoch: 55 [34560/50981 (68%)]\tLoss: 9240093.931734\n",
      "Train Epoch: 55 [35840/50981 (70%)]\tLoss: 9243103.948399\n",
      "Train Epoch: 55 [37120/50981 (73%)]\tLoss: 9248076.685567\n",
      "Train Epoch: 55 [38400/50981 (75%)]\tLoss: 9250018.686047\n",
      "Train Epoch: 55 [39680/50981 (78%)]\tLoss: 9254494.159164\n",
      "Train Epoch: 55 [40960/50981 (80%)]\tLoss: 9261303.842679\n",
      "Train Epoch: 55 [42240/50981 (83%)]\tLoss: 9268392.660121\n",
      "Train Epoch: 55 [43520/50981 (85%)]\tLoss: 9268839.397361\n",
      "Train Epoch: 55 [44800/50981 (88%)]\tLoss: 9270828.041311\n",
      "Train Epoch: 55 [46080/50981 (90%)]\tLoss: 9278201.486150\n",
      "Train Epoch: 55 [47360/50981 (93%)]\tLoss: 9283423.292453\n",
      "Train Epoch: 55 [48640/50981 (95%)]\tLoss: 9283796.927822\n",
      "Train Epoch: 55 [49920/50981 (98%)]\tLoss: 9292801.512788\n",
      "Train Epoch: 56 [0/50981 (0%)]\tLoss: 9170701.000000\n",
      "Train Epoch: 56 [1280/50981 (3%)]\tLoss: 8833171.818182\n",
      "Train Epoch: 56 [2560/50981 (5%)]\tLoss: 8944174.000000\n",
      "Train Epoch: 56 [3840/50981 (8%)]\tLoss: 9059813.387097\n",
      "Train Epoch: 56 [5120/50981 (10%)]\tLoss: 9068849.829268\n",
      "Train Epoch: 56 [6400/50981 (13%)]\tLoss: 9060433.843137\n",
      "Train Epoch: 56 [7680/50981 (15%)]\tLoss: 9067827.885246\n",
      "Train Epoch: 56 [8960/50981 (18%)]\tLoss: 9070871.492958\n",
      "Train Epoch: 56 [10240/50981 (20%)]\tLoss: 9065409.592593\n",
      "Train Epoch: 56 [11520/50981 (23%)]\tLoss: 9080144.934066\n",
      "Train Epoch: 56 [12800/50981 (25%)]\tLoss: 9089127.534653\n",
      "Train Epoch: 56 [14080/50981 (28%)]\tLoss: 9075108.378378\n",
      "Train Epoch: 56 [15360/50981 (30%)]\tLoss: 9091541.938017\n",
      "Train Epoch: 56 [16640/50981 (33%)]\tLoss: 9114554.736641\n",
      "Train Epoch: 56 [17920/50981 (35%)]\tLoss: 9128442.237589\n",
      "Train Epoch: 56 [19200/50981 (38%)]\tLoss: 9129643.837748\n",
      "Train Epoch: 56 [20480/50981 (40%)]\tLoss: 9131642.767081\n",
      "Train Epoch: 56 [21760/50981 (43%)]\tLoss: 9142502.073099\n",
      "Train Epoch: 56 [23040/50981 (45%)]\tLoss: 9142972.185083\n",
      "Train Epoch: 56 [24320/50981 (48%)]\tLoss: 9136573.018325\n",
      "Train Epoch: 56 [25600/50981 (50%)]\tLoss: 9132171.629353\n",
      "Train Epoch: 56 [26880/50981 (53%)]\tLoss: 9131335.125592\n",
      "Train Epoch: 56 [28160/50981 (55%)]\tLoss: 9140038.970588\n",
      "Train Epoch: 56 [29440/50981 (58%)]\tLoss: 9148386.006494\n",
      "Train Epoch: 56 [30720/50981 (60%)]\tLoss: 9158826.873444\n",
      "Train Epoch: 56 [32000/50981 (63%)]\tLoss: 9168734.735060\n",
      "Train Epoch: 56 [33280/50981 (65%)]\tLoss: 9173006.346743\n",
      "Train Epoch: 56 [34560/50981 (68%)]\tLoss: 9185638.566421\n",
      "Train Epoch: 56 [35840/50981 (70%)]\tLoss: 9200987.859431\n",
      "Train Epoch: 56 [37120/50981 (73%)]\tLoss: 9200188.197595\n",
      "Train Epoch: 56 [38400/50981 (75%)]\tLoss: 9206220.287375\n",
      "Train Epoch: 56 [39680/50981 (78%)]\tLoss: 9207839.538585\n",
      "Train Epoch: 56 [40960/50981 (80%)]\tLoss: 9213192.802181\n",
      "Train Epoch: 56 [42240/50981 (83%)]\tLoss: 9222344.862538\n",
      "Train Epoch: 56 [43520/50981 (85%)]\tLoss: 9231388.019062\n",
      "Train Epoch: 56 [44800/50981 (88%)]\tLoss: 9236929.220798\n",
      "Train Epoch: 56 [46080/50981 (90%)]\tLoss: 9236487.463989\n",
      "Train Epoch: 56 [47360/50981 (93%)]\tLoss: 9238758.009434\n",
      "Train Epoch: 56 [48640/50981 (95%)]\tLoss: 9245848.342520\n",
      "Train Epoch: 56 [49920/50981 (98%)]\tLoss: 9251120.975703\n",
      "Train Epoch: 57 [0/50981 (0%)]\tLoss: 9348646.000000\n",
      "Train Epoch: 57 [1280/50981 (3%)]\tLoss: 8945430.636364\n",
      "Train Epoch: 57 [2560/50981 (5%)]\tLoss: 8915298.333333\n",
      "Train Epoch: 57 [3840/50981 (8%)]\tLoss: 8943891.451613\n",
      "Train Epoch: 57 [5120/50981 (10%)]\tLoss: 9005235.439024\n",
      "Train Epoch: 57 [6400/50981 (13%)]\tLoss: 8998149.921569\n",
      "Train Epoch: 57 [7680/50981 (15%)]\tLoss: 9010745.672131\n",
      "Train Epoch: 57 [8960/50981 (18%)]\tLoss: 9046473.492958\n",
      "Train Epoch: 57 [10240/50981 (20%)]\tLoss: 9052050.086420\n",
      "Train Epoch: 57 [11520/50981 (23%)]\tLoss: 9073834.043956\n",
      "Train Epoch: 57 [12800/50981 (25%)]\tLoss: 9044190.742574\n",
      "Train Epoch: 57 [14080/50981 (28%)]\tLoss: 9060641.846847\n",
      "Train Epoch: 57 [15360/50981 (30%)]\tLoss: 9063729.966942\n",
      "Train Epoch: 57 [16640/50981 (33%)]\tLoss: 9068783.366412\n",
      "Train Epoch: 57 [17920/50981 (35%)]\tLoss: 9078410.716312\n",
      "Train Epoch: 57 [19200/50981 (38%)]\tLoss: 9086429.642384\n",
      "Train Epoch: 57 [20480/50981 (40%)]\tLoss: 9080358.776398\n",
      "Train Epoch: 57 [21760/50981 (43%)]\tLoss: 9075453.766082\n",
      "Train Epoch: 57 [23040/50981 (45%)]\tLoss: 9079802.066298\n",
      "Train Epoch: 57 [24320/50981 (48%)]\tLoss: 9085097.821990\n",
      "Train Epoch: 57 [25600/50981 (50%)]\tLoss: 9091266.358209\n",
      "Train Epoch: 57 [26880/50981 (53%)]\tLoss: 9100249.886256\n",
      "Train Epoch: 57 [28160/50981 (55%)]\tLoss: 9106679.710407\n",
      "Train Epoch: 57 [29440/50981 (58%)]\tLoss: 9116269.471861\n",
      "Train Epoch: 57 [30720/50981 (60%)]\tLoss: 9121085.879668\n",
      "Train Epoch: 57 [32000/50981 (63%)]\tLoss: 9132475.800797\n",
      "Train Epoch: 57 [33280/50981 (65%)]\tLoss: 9136633.977011\n",
      "Train Epoch: 57 [34560/50981 (68%)]\tLoss: 9146416.040590\n",
      "Train Epoch: 57 [35840/50981 (70%)]\tLoss: 9151047.117438\n",
      "Train Epoch: 57 [37120/50981 (73%)]\tLoss: 9155708.316151\n",
      "Train Epoch: 57 [38400/50981 (75%)]\tLoss: 9160114.976744\n",
      "Train Epoch: 57 [39680/50981 (78%)]\tLoss: 9168851.607717\n",
      "Train Epoch: 57 [40960/50981 (80%)]\tLoss: 9172798.275701\n",
      "Train Epoch: 57 [42240/50981 (83%)]\tLoss: 9182798.415408\n",
      "Train Epoch: 57 [43520/50981 (85%)]\tLoss: 9187682.714076\n",
      "Train Epoch: 57 [44800/50981 (88%)]\tLoss: 9198427.334758\n",
      "Train Epoch: 57 [46080/50981 (90%)]\tLoss: 9201412.702216\n",
      "Train Epoch: 57 [47360/50981 (93%)]\tLoss: 9204417.063342\n",
      "Train Epoch: 57 [48640/50981 (95%)]\tLoss: 9209160.946194\n",
      "Train Epoch: 57 [49920/50981 (98%)]\tLoss: 9216636.978261\n",
      "Train Epoch: 58 [0/50981 (0%)]\tLoss: 8926616.000000\n",
      "Train Epoch: 58 [1280/50981 (3%)]\tLoss: 8745040.500000\n",
      "Train Epoch: 58 [2560/50981 (5%)]\tLoss: 8969627.642857\n",
      "Train Epoch: 58 [3840/50981 (8%)]\tLoss: 8999580.112903\n",
      "Train Epoch: 58 [5120/50981 (10%)]\tLoss: 9006787.109756\n",
      "Train Epoch: 58 [6400/50981 (13%)]\tLoss: 9027708.833333\n",
      "Train Epoch: 58 [7680/50981 (15%)]\tLoss: 9018548.032787\n",
      "Train Epoch: 58 [8960/50981 (18%)]\tLoss: 9029370.070423\n",
      "Train Epoch: 58 [10240/50981 (20%)]\tLoss: 9030602.222222\n",
      "Train Epoch: 58 [11520/50981 (23%)]\tLoss: 9029162.725275\n",
      "Train Epoch: 58 [12800/50981 (25%)]\tLoss: 9019299.792079\n",
      "Train Epoch: 58 [14080/50981 (28%)]\tLoss: 9021001.801802\n",
      "Train Epoch: 58 [15360/50981 (30%)]\tLoss: 9029778.198347\n",
      "Train Epoch: 58 [16640/50981 (33%)]\tLoss: 9031577.099237\n",
      "Train Epoch: 58 [17920/50981 (35%)]\tLoss: 9043436.375887\n",
      "Train Epoch: 58 [19200/50981 (38%)]\tLoss: 9062805.847682\n",
      "Train Epoch: 58 [20480/50981 (40%)]\tLoss: 9070252.987578\n",
      "Train Epoch: 58 [21760/50981 (43%)]\tLoss: 9085619.654971\n",
      "Train Epoch: 58 [23040/50981 (45%)]\tLoss: 9107833.226519\n",
      "Train Epoch: 58 [24320/50981 (48%)]\tLoss: 9116542.528796\n",
      "Train Epoch: 58 [25600/50981 (50%)]\tLoss: 9117602.457711\n",
      "Train Epoch: 58 [26880/50981 (53%)]\tLoss: 9126467.995261\n",
      "Train Epoch: 58 [28160/50981 (55%)]\tLoss: 9136632.289593\n",
      "Train Epoch: 58 [29440/50981 (58%)]\tLoss: 9133158.662338\n",
      "Train Epoch: 58 [30720/50981 (60%)]\tLoss: 9129101.132780\n",
      "Train Epoch: 58 [32000/50981 (63%)]\tLoss: 9142045.000000\n",
      "Train Epoch: 58 [33280/50981 (65%)]\tLoss: 9146146.256705\n",
      "Train Epoch: 58 [34560/50981 (68%)]\tLoss: 9150730.180812\n",
      "Train Epoch: 58 [35840/50981 (70%)]\tLoss: 9147435.555160\n",
      "Train Epoch: 58 [37120/50981 (73%)]\tLoss: 9158129.859107\n",
      "Train Epoch: 58 [38400/50981 (75%)]\tLoss: 9165065.943522\n",
      "Train Epoch: 58 [39680/50981 (78%)]\tLoss: 9162880.935691\n",
      "Train Epoch: 58 [40960/50981 (80%)]\tLoss: 9169249.663551\n",
      "Train Epoch: 58 [42240/50981 (83%)]\tLoss: 9175868.643505\n",
      "Train Epoch: 58 [43520/50981 (85%)]\tLoss: 9179212.255132\n",
      "Train Epoch: 58 [44800/50981 (88%)]\tLoss: 9188212.028490\n",
      "Train Epoch: 58 [46080/50981 (90%)]\tLoss: 9185975.008310\n",
      "Train Epoch: 58 [47360/50981 (93%)]\tLoss: 9185771.253369\n",
      "Train Epoch: 58 [48640/50981 (95%)]\tLoss: 9185432.498688\n",
      "Train Epoch: 58 [49920/50981 (98%)]\tLoss: 9185002.792839\n",
      "Train Epoch: 59 [0/50981 (0%)]\tLoss: 9555290.000000\n",
      "Train Epoch: 59 [1280/50981 (3%)]\tLoss: 8988899.227273\n",
      "Train Epoch: 59 [2560/50981 (5%)]\tLoss: 9010655.690476\n",
      "Train Epoch: 59 [3840/50981 (8%)]\tLoss: 9046023.209677\n",
      "Train Epoch: 59 [5120/50981 (10%)]\tLoss: 9024505.841463\n",
      "Train Epoch: 59 [6400/50981 (13%)]\tLoss: 9024366.245098\n",
      "Train Epoch: 59 [7680/50981 (15%)]\tLoss: 9049149.500000\n",
      "Train Epoch: 59 [8960/50981 (18%)]\tLoss: 9027338.401408\n",
      "Train Epoch: 59 [10240/50981 (20%)]\tLoss: 9026769.067901\n",
      "Train Epoch: 59 [11520/50981 (23%)]\tLoss: 9040035.478022\n",
      "Train Epoch: 59 [12800/50981 (25%)]\tLoss: 9017651.673267\n",
      "Train Epoch: 59 [14080/50981 (28%)]\tLoss: 9011688.126126\n",
      "Train Epoch: 59 [15360/50981 (30%)]\tLoss: 9021437.173554\n",
      "Train Epoch: 59 [16640/50981 (33%)]\tLoss: 9030378.015267\n",
      "Train Epoch: 59 [17920/50981 (35%)]\tLoss: 9021050.900709\n",
      "Train Epoch: 59 [19200/50981 (38%)]\tLoss: 9041968.172185\n",
      "Train Epoch: 59 [20480/50981 (40%)]\tLoss: 9023697.378882\n",
      "Train Epoch: 59 [21760/50981 (43%)]\tLoss: 9027554.678363\n",
      "Train Epoch: 59 [23040/50981 (45%)]\tLoss: 9036548.027624\n",
      "Train Epoch: 59 [24320/50981 (48%)]\tLoss: 9041679.623037\n",
      "Train Epoch: 59 [25600/50981 (50%)]\tLoss: 9046427.721393\n",
      "Train Epoch: 59 [26880/50981 (53%)]\tLoss: 9050494.156398\n",
      "Train Epoch: 59 [28160/50981 (55%)]\tLoss: 9061483.710407\n",
      "Train Epoch: 59 [29440/50981 (58%)]\tLoss: 9065423.177489\n",
      "Train Epoch: 59 [30720/50981 (60%)]\tLoss: 9071388.149378\n",
      "Train Epoch: 59 [32000/50981 (63%)]\tLoss: 9070302.852590\n",
      "Train Epoch: 59 [33280/50981 (65%)]\tLoss: 9081783.957854\n",
      "Train Epoch: 59 [34560/50981 (68%)]\tLoss: 9084395.029520\n",
      "Train Epoch: 59 [35840/50981 (70%)]\tLoss: 9091189.391459\n",
      "Train Epoch: 59 [37120/50981 (73%)]\tLoss: 9096760.958763\n",
      "Train Epoch: 59 [38400/50981 (75%)]\tLoss: 9100500.551495\n",
      "Train Epoch: 59 [39680/50981 (78%)]\tLoss: 9098514.630225\n",
      "Train Epoch: 59 [40960/50981 (80%)]\tLoss: 9105165.990654\n",
      "Train Epoch: 59 [42240/50981 (83%)]\tLoss: 9109860.027190\n",
      "Train Epoch: 59 [43520/50981 (85%)]\tLoss: 9108998.407625\n",
      "Train Epoch: 59 [44800/50981 (88%)]\tLoss: 9110547.225071\n",
      "Train Epoch: 59 [46080/50981 (90%)]\tLoss: 9111688.628809\n",
      "Train Epoch: 59 [47360/50981 (93%)]\tLoss: 9114730.859838\n",
      "Train Epoch: 59 [48640/50981 (95%)]\tLoss: 9121679.241470\n",
      "Train Epoch: 59 [49920/50981 (98%)]\tLoss: 9127527.388747\n",
      "Train Epoch: 60 [0/50981 (0%)]\tLoss: 8679468.000000\n",
      "Train Epoch: 60 [1280/50981 (3%)]\tLoss: 9082058.636364\n",
      "Train Epoch: 60 [2560/50981 (5%)]\tLoss: 9033403.738095\n",
      "Train Epoch: 60 [3840/50981 (8%)]\tLoss: 9074545.145161\n",
      "Train Epoch: 60 [5120/50981 (10%)]\tLoss: 9036576.597561\n",
      "Train Epoch: 60 [6400/50981 (13%)]\tLoss: 8997008.774510\n",
      "Train Epoch: 60 [7680/50981 (15%)]\tLoss: 9004841.991803\n",
      "Train Epoch: 60 [8960/50981 (18%)]\tLoss: 9004882.105634\n",
      "Train Epoch: 60 [10240/50981 (20%)]\tLoss: 9012124.302469\n",
      "Train Epoch: 60 [11520/50981 (23%)]\tLoss: 9030520.862637\n",
      "Train Epoch: 60 [12800/50981 (25%)]\tLoss: 9015670.054455\n",
      "Train Epoch: 60 [14080/50981 (28%)]\tLoss: 9010884.779279\n",
      "Train Epoch: 60 [15360/50981 (30%)]\tLoss: 9015537.574380\n",
      "Train Epoch: 60 [16640/50981 (33%)]\tLoss: 9001258.068702\n",
      "Train Epoch: 60 [17920/50981 (35%)]\tLoss: 9001778.758865\n",
      "Train Epoch: 60 [19200/50981 (38%)]\tLoss: 8989654.456954\n",
      "Train Epoch: 60 [20480/50981 (40%)]\tLoss: 8990850.633540\n",
      "Train Epoch: 60 [21760/50981 (43%)]\tLoss: 8998959.973684\n",
      "Train Epoch: 60 [23040/50981 (45%)]\tLoss: 9007804.654696\n",
      "Train Epoch: 60 [24320/50981 (48%)]\tLoss: 9027945.321990\n",
      "Train Epoch: 60 [25600/50981 (50%)]\tLoss: 9027164.116915\n",
      "Train Epoch: 60 [26880/50981 (53%)]\tLoss: 9020518.097156\n",
      "Train Epoch: 60 [28160/50981 (55%)]\tLoss: 9036004.649321\n",
      "Train Epoch: 60 [29440/50981 (58%)]\tLoss: 9045970.863636\n",
      "Train Epoch: 60 [30720/50981 (60%)]\tLoss: 9053570.275934\n",
      "Train Epoch: 60 [32000/50981 (63%)]\tLoss: 9056659.643426\n",
      "Train Epoch: 60 [33280/50981 (65%)]\tLoss: 9051193.074713\n",
      "Train Epoch: 60 [34560/50981 (68%)]\tLoss: 9047069.725092\n",
      "Train Epoch: 60 [35840/50981 (70%)]\tLoss: 9052308.766904\n",
      "Train Epoch: 60 [37120/50981 (73%)]\tLoss: 9058065.963918\n",
      "Train Epoch: 60 [38400/50981 (75%)]\tLoss: 9066003.596346\n",
      "Train Epoch: 60 [39680/50981 (78%)]\tLoss: 9067806.387460\n",
      "Train Epoch: 60 [40960/50981 (80%)]\tLoss: 9069387.378505\n",
      "Train Epoch: 60 [42240/50981 (83%)]\tLoss: 9072453.291541\n",
      "Train Epoch: 60 [43520/50981 (85%)]\tLoss: 9078533.368035\n",
      "Train Epoch: 60 [44800/50981 (88%)]\tLoss: 9083919.360399\n",
      "Train Epoch: 60 [46080/50981 (90%)]\tLoss: 9088426.569252\n",
      "Train Epoch: 60 [47360/50981 (93%)]\tLoss: 9089806.672507\n",
      "Train Epoch: 60 [48640/50981 (95%)]\tLoss: 9088073.591864\n",
      "Train Epoch: 60 [49920/50981 (98%)]\tLoss: 9092290.835038\n",
      "Train Epoch: 61 [0/50981 (0%)]\tLoss: 9221517.000000\n",
      "Train Epoch: 61 [1280/50981 (3%)]\tLoss: 8786505.000000\n",
      "Train Epoch: 61 [2560/50981 (5%)]\tLoss: 8909051.428571\n",
      "Train Epoch: 61 [3840/50981 (8%)]\tLoss: 8942092.161290\n",
      "Train Epoch: 61 [5120/50981 (10%)]\tLoss: 8952527.536585\n",
      "Train Epoch: 61 [6400/50981 (13%)]\tLoss: 8926584.901961\n",
      "Train Epoch: 61 [7680/50981 (15%)]\tLoss: 8930762.114754\n",
      "Train Epoch: 61 [8960/50981 (18%)]\tLoss: 8957807.507042\n",
      "Train Epoch: 61 [10240/50981 (20%)]\tLoss: 8956690.888889\n",
      "Train Epoch: 61 [11520/50981 (23%)]\tLoss: 8970498.571429\n",
      "Train Epoch: 61 [12800/50981 (25%)]\tLoss: 8969997.603960\n",
      "Train Epoch: 61 [14080/50981 (28%)]\tLoss: 8966731.594595\n",
      "Train Epoch: 61 [15360/50981 (30%)]\tLoss: 8963483.194215\n",
      "Train Epoch: 61 [16640/50981 (33%)]\tLoss: 8976942.927481\n",
      "Train Epoch: 61 [17920/50981 (35%)]\tLoss: 8969060.588652\n",
      "Train Epoch: 61 [19200/50981 (38%)]\tLoss: 8972159.152318\n",
      "Train Epoch: 61 [20480/50981 (40%)]\tLoss: 8977754.335404\n",
      "Train Epoch: 61 [21760/50981 (43%)]\tLoss: 8977650.964912\n",
      "Train Epoch: 61 [23040/50981 (45%)]\tLoss: 8993775.154696\n",
      "Train Epoch: 61 [24320/50981 (48%)]\tLoss: 8997098.492147\n",
      "Train Epoch: 61 [25600/50981 (50%)]\tLoss: 9000623.885572\n",
      "Train Epoch: 61 [26880/50981 (53%)]\tLoss: 8998400.606635\n",
      "Train Epoch: 61 [28160/50981 (55%)]\tLoss: 9000983.140271\n",
      "Train Epoch: 61 [29440/50981 (58%)]\tLoss: 9003891.186147\n",
      "Train Epoch: 61 [30720/50981 (60%)]\tLoss: 9000963.390041\n",
      "Train Epoch: 61 [32000/50981 (63%)]\tLoss: 9003775.254980\n",
      "Train Epoch: 61 [33280/50981 (65%)]\tLoss: 9010419.145594\n",
      "Train Epoch: 61 [34560/50981 (68%)]\tLoss: 9010094.955720\n",
      "Train Epoch: 61 [35840/50981 (70%)]\tLoss: 9011147.594306\n",
      "Train Epoch: 61 [37120/50981 (73%)]\tLoss: 9015126.243986\n",
      "Train Epoch: 61 [38400/50981 (75%)]\tLoss: 9016473.531561\n",
      "Train Epoch: 61 [39680/50981 (78%)]\tLoss: 9017563.985531\n",
      "Train Epoch: 61 [40960/50981 (80%)]\tLoss: 9023266.593458\n",
      "Train Epoch: 61 [42240/50981 (83%)]\tLoss: 9027401.986405\n",
      "Train Epoch: 61 [43520/50981 (85%)]\tLoss: 9033375.482405\n",
      "Train Epoch: 61 [44800/50981 (88%)]\tLoss: 9040751.203704\n",
      "Train Epoch: 61 [46080/50981 (90%)]\tLoss: 9043108.225762\n",
      "Train Epoch: 61 [47360/50981 (93%)]\tLoss: 9052158.801887\n",
      "Train Epoch: 61 [48640/50981 (95%)]\tLoss: 9057529.833333\n",
      "Train Epoch: 61 [49920/50981 (98%)]\tLoss: 9059455.351662\n",
      "Train Epoch: 62 [0/50981 (0%)]\tLoss: 7967842.000000\n",
      "Train Epoch: 62 [1280/50981 (3%)]\tLoss: 8730925.590909\n",
      "Train Epoch: 62 [2560/50981 (5%)]\tLoss: 8846399.880952\n",
      "Train Epoch: 62 [3840/50981 (8%)]\tLoss: 8854964.596774\n",
      "Train Epoch: 62 [5120/50981 (10%)]\tLoss: 8941179.890244\n",
      "Train Epoch: 62 [6400/50981 (13%)]\tLoss: 8985211.088235\n",
      "Train Epoch: 62 [7680/50981 (15%)]\tLoss: 9000841.122951\n",
      "Train Epoch: 62 [8960/50981 (18%)]\tLoss: 8990194.514085\n",
      "Train Epoch: 62 [10240/50981 (20%)]\tLoss: 8971614.265432\n",
      "Train Epoch: 62 [11520/50981 (23%)]\tLoss: 8957113.763736\n",
      "Train Epoch: 62 [12800/50981 (25%)]\tLoss: 8942947.351485\n",
      "Train Epoch: 62 [14080/50981 (28%)]\tLoss: 8956387.905405\n",
      "Train Epoch: 62 [15360/50981 (30%)]\tLoss: 8954385.450413\n",
      "Train Epoch: 62 [16640/50981 (33%)]\tLoss: 8956144.408397\n",
      "Train Epoch: 62 [17920/50981 (35%)]\tLoss: 8962532.315603\n",
      "Train Epoch: 62 [19200/50981 (38%)]\tLoss: 8979034.460265\n",
      "Train Epoch: 62 [20480/50981 (40%)]\tLoss: 8979639.338509\n",
      "Train Epoch: 62 [21760/50981 (43%)]\tLoss: 8980785.961988\n",
      "Train Epoch: 62 [23040/50981 (45%)]\tLoss: 8983151.864641\n",
      "Train Epoch: 62 [24320/50981 (48%)]\tLoss: 8999525.678010\n",
      "Train Epoch: 62 [25600/50981 (50%)]\tLoss: 8991512.470149\n",
      "Train Epoch: 62 [26880/50981 (53%)]\tLoss: 8994874.386256\n",
      "Train Epoch: 62 [28160/50981 (55%)]\tLoss: 9003024.884615\n",
      "Train Epoch: 62 [29440/50981 (58%)]\tLoss: 8999156.738095\n",
      "Train Epoch: 62 [30720/50981 (60%)]\tLoss: 9012402.757261\n",
      "Train Epoch: 62 [32000/50981 (63%)]\tLoss: 9014915.643426\n",
      "Train Epoch: 62 [33280/50981 (65%)]\tLoss: 9013154.436782\n",
      "Train Epoch: 62 [34560/50981 (68%)]\tLoss: 9005721.309963\n",
      "Train Epoch: 62 [35840/50981 (70%)]\tLoss: 9008641.585409\n",
      "Train Epoch: 62 [37120/50981 (73%)]\tLoss: 9016120.242268\n",
      "Train Epoch: 62 [38400/50981 (75%)]\tLoss: 9012434.654485\n",
      "Train Epoch: 62 [39680/50981 (78%)]\tLoss: 9016739.054662\n",
      "Train Epoch: 62 [40960/50981 (80%)]\tLoss: 9023431.152648\n",
      "Train Epoch: 62 [42240/50981 (83%)]\tLoss: 9021492.977341\n",
      "Train Epoch: 62 [43520/50981 (85%)]\tLoss: 9028109.353372\n",
      "Train Epoch: 62 [44800/50981 (88%)]\tLoss: 9029260.668091\n",
      "Train Epoch: 62 [46080/50981 (90%)]\tLoss: 9029748.621884\n",
      "Train Epoch: 62 [47360/50981 (93%)]\tLoss: 9038152.801887\n",
      "Train Epoch: 62 [48640/50981 (95%)]\tLoss: 9038524.862205\n",
      "Train Epoch: 62 [49920/50981 (98%)]\tLoss: 9039855.898977\n",
      "Train Epoch: 63 [0/50981 (0%)]\tLoss: 9324442.000000\n",
      "Train Epoch: 63 [1280/50981 (3%)]\tLoss: 8978733.727273\n",
      "Train Epoch: 63 [2560/50981 (5%)]\tLoss: 8844268.404762\n",
      "Train Epoch: 63 [3840/50981 (8%)]\tLoss: 8952069.209677\n",
      "Train Epoch: 63 [5120/50981 (10%)]\tLoss: 8983461.353659\n",
      "Train Epoch: 63 [6400/50981 (13%)]\tLoss: 8966872.117647\n",
      "Train Epoch: 63 [7680/50981 (15%)]\tLoss: 8936692.819672\n",
      "Train Epoch: 63 [8960/50981 (18%)]\tLoss: 8901815.880282\n",
      "Train Epoch: 63 [10240/50981 (20%)]\tLoss: 8884742.135802\n",
      "Train Epoch: 63 [11520/50981 (23%)]\tLoss: 8890565.054945\n",
      "Train Epoch: 63 [12800/50981 (25%)]\tLoss: 8881787.163366\n",
      "Train Epoch: 63 [14080/50981 (28%)]\tLoss: 8895004.689189\n",
      "Train Epoch: 63 [15360/50981 (30%)]\tLoss: 8901997.607438\n",
      "Train Epoch: 63 [16640/50981 (33%)]\tLoss: 8908904.148855\n",
      "Train Epoch: 63 [17920/50981 (35%)]\tLoss: 8905648.854610\n",
      "Train Epoch: 63 [19200/50981 (38%)]\tLoss: 8910196.910596\n",
      "Train Epoch: 63 [20480/50981 (40%)]\tLoss: 8909531.009317\n",
      "Train Epoch: 63 [21760/50981 (43%)]\tLoss: 8919918.751462\n",
      "Train Epoch: 63 [23040/50981 (45%)]\tLoss: 8937641.709945\n",
      "Train Epoch: 63 [24320/50981 (48%)]\tLoss: 8944395.730366\n",
      "Train Epoch: 63 [25600/50981 (50%)]\tLoss: 8959204.569652\n",
      "Train Epoch: 63 [26880/50981 (53%)]\tLoss: 8971500.000000\n",
      "Train Epoch: 63 [28160/50981 (55%)]\tLoss: 8967936.497738\n",
      "Train Epoch: 63 [29440/50981 (58%)]\tLoss: 8970034.242424\n",
      "Train Epoch: 63 [30720/50981 (60%)]\tLoss: 8976560.871369\n",
      "Train Epoch: 63 [32000/50981 (63%)]\tLoss: 8984409.362550\n",
      "Train Epoch: 63 [33280/50981 (65%)]\tLoss: 8985709.651341\n",
      "Train Epoch: 63 [34560/50981 (68%)]\tLoss: 8983865.184502\n",
      "Train Epoch: 63 [35840/50981 (70%)]\tLoss: 8987474.775801\n",
      "Train Epoch: 63 [37120/50981 (73%)]\tLoss: 8993889.309278\n",
      "Train Epoch: 63 [38400/50981 (75%)]\tLoss: 8990452.916944\n",
      "Train Epoch: 63 [39680/50981 (78%)]\tLoss: 8990576.527331\n",
      "Train Epoch: 63 [40960/50981 (80%)]\tLoss: 8986308.330218\n",
      "Train Epoch: 63 [42240/50981 (83%)]\tLoss: 8989523.135952\n",
      "Train Epoch: 63 [43520/50981 (85%)]\tLoss: 8988912.645161\n",
      "Train Epoch: 63 [44800/50981 (88%)]\tLoss: 8992377.937322\n",
      "Train Epoch: 63 [46080/50981 (90%)]\tLoss: 8992665.858726\n",
      "Train Epoch: 63 [47360/50981 (93%)]\tLoss: 8992118.222372\n",
      "Train Epoch: 63 [48640/50981 (95%)]\tLoss: 8994470.817585\n",
      "Train Epoch: 63 [49920/50981 (98%)]\tLoss: 9001366.875959\n",
      "Train Epoch: 64 [0/50981 (0%)]\tLoss: 8598287.000000\n",
      "Train Epoch: 64 [1280/50981 (3%)]\tLoss: 8584771.272727\n",
      "Train Epoch: 64 [2560/50981 (5%)]\tLoss: 8704918.238095\n",
      "Train Epoch: 64 [3840/50981 (8%)]\tLoss: 8736720.112903\n",
      "Train Epoch: 64 [5120/50981 (10%)]\tLoss: 8763773.292683\n",
      "Train Epoch: 64 [6400/50981 (13%)]\tLoss: 8747477.588235\n",
      "Train Epoch: 64 [7680/50981 (15%)]\tLoss: 8772964.950820\n",
      "Train Epoch: 64 [8960/50981 (18%)]\tLoss: 8787851.661972\n",
      "Train Epoch: 64 [10240/50981 (20%)]\tLoss: 8781708.617284\n",
      "Train Epoch: 64 [11520/50981 (23%)]\tLoss: 8808816.329670\n",
      "Train Epoch: 64 [12800/50981 (25%)]\tLoss: 8837669.485149\n",
      "Train Epoch: 64 [14080/50981 (28%)]\tLoss: 8837674.729730\n",
      "Train Epoch: 64 [15360/50981 (30%)]\tLoss: 8866029.661157\n",
      "Train Epoch: 64 [16640/50981 (33%)]\tLoss: 8865072.450382\n",
      "Train Epoch: 64 [17920/50981 (35%)]\tLoss: 8866037.673759\n",
      "Train Epoch: 64 [19200/50981 (38%)]\tLoss: 8857861.688742\n",
      "Train Epoch: 64 [20480/50981 (40%)]\tLoss: 8858748.708075\n",
      "Train Epoch: 64 [21760/50981 (43%)]\tLoss: 8864004.614035\n",
      "Train Epoch: 64 [23040/50981 (45%)]\tLoss: 8868783.870166\n",
      "Train Epoch: 64 [24320/50981 (48%)]\tLoss: 8867328.520942\n",
      "Train Epoch: 64 [25600/50981 (50%)]\tLoss: 8873135.310945\n",
      "Train Epoch: 64 [26880/50981 (53%)]\tLoss: 8890002.864929\n",
      "Train Epoch: 64 [28160/50981 (55%)]\tLoss: 8893484.404977\n",
      "Train Epoch: 64 [29440/50981 (58%)]\tLoss: 8894286.101732\n",
      "Train Epoch: 64 [30720/50981 (60%)]\tLoss: 8900637.414938\n",
      "Train Epoch: 64 [32000/50981 (63%)]\tLoss: 8905691.521912\n",
      "Train Epoch: 64 [33280/50981 (65%)]\tLoss: 8904309.731801\n",
      "Train Epoch: 64 [34560/50981 (68%)]\tLoss: 8909885.959410\n",
      "Train Epoch: 64 [35840/50981 (70%)]\tLoss: 8907197.512456\n",
      "Train Epoch: 64 [37120/50981 (73%)]\tLoss: 8914785.017182\n",
      "Train Epoch: 64 [38400/50981 (75%)]\tLoss: 8915440.970100\n",
      "Train Epoch: 64 [39680/50981 (78%)]\tLoss: 8924228.205788\n",
      "Train Epoch: 64 [40960/50981 (80%)]\tLoss: 8924327.242991\n",
      "Train Epoch: 64 [42240/50981 (83%)]\tLoss: 8938646.987915\n",
      "Train Epoch: 64 [43520/50981 (85%)]\tLoss: 8943767.272727\n",
      "Train Epoch: 64 [44800/50981 (88%)]\tLoss: 8946721.700855\n",
      "Train Epoch: 64 [46080/50981 (90%)]\tLoss: 8948942.221607\n",
      "Train Epoch: 64 [47360/50981 (93%)]\tLoss: 8954745.032345\n",
      "Train Epoch: 64 [48640/50981 (95%)]\tLoss: 8961400.333333\n",
      "Train Epoch: 64 [49920/50981 (98%)]\tLoss: 8966555.365729\n",
      "Train Epoch: 65 [0/50981 (0%)]\tLoss: 9613057.000000\n",
      "Train Epoch: 65 [1280/50981 (3%)]\tLoss: 8869710.636364\n",
      "Train Epoch: 65 [2560/50981 (5%)]\tLoss: 8865317.904762\n",
      "Train Epoch: 65 [3840/50981 (8%)]\tLoss: 8874418.258065\n",
      "Train Epoch: 65 [5120/50981 (10%)]\tLoss: 8854146.170732\n",
      "Train Epoch: 65 [6400/50981 (13%)]\tLoss: 8833705.343137\n",
      "Train Epoch: 65 [7680/50981 (15%)]\tLoss: 8818822.040984\n",
      "Train Epoch: 65 [8960/50981 (18%)]\tLoss: 8818978.492958\n",
      "Train Epoch: 65 [10240/50981 (20%)]\tLoss: 8813527.728395\n",
      "Train Epoch: 65 [11520/50981 (23%)]\tLoss: 8833062.362637\n",
      "Train Epoch: 65 [12800/50981 (25%)]\tLoss: 8841807.900990\n",
      "Train Epoch: 65 [14080/50981 (28%)]\tLoss: 8857107.270270\n",
      "Train Epoch: 65 [15360/50981 (30%)]\tLoss: 8853955.082645\n",
      "Train Epoch: 65 [16640/50981 (33%)]\tLoss: 8858775.160305\n",
      "Train Epoch: 65 [17920/50981 (35%)]\tLoss: 8868531.347518\n",
      "Train Epoch: 65 [19200/50981 (38%)]\tLoss: 8872616.933775\n",
      "Train Epoch: 65 [20480/50981 (40%)]\tLoss: 8873484.074534\n",
      "Train Epoch: 65 [21760/50981 (43%)]\tLoss: 8874807.883041\n",
      "Train Epoch: 65 [23040/50981 (45%)]\tLoss: 8875034.453039\n",
      "Train Epoch: 65 [24320/50981 (48%)]\tLoss: 8877410.554974\n",
      "Train Epoch: 65 [25600/50981 (50%)]\tLoss: 8872223.273632\n",
      "Train Epoch: 65 [26880/50981 (53%)]\tLoss: 8888600.469194\n",
      "Train Epoch: 65 [28160/50981 (55%)]\tLoss: 8893302.013575\n",
      "Train Epoch: 65 [29440/50981 (58%)]\tLoss: 8901304.056277\n",
      "Train Epoch: 65 [30720/50981 (60%)]\tLoss: 8909893.522822\n",
      "Train Epoch: 65 [32000/50981 (63%)]\tLoss: 8915250.721116\n",
      "Train Epoch: 65 [33280/50981 (65%)]\tLoss: 8919276.515326\n",
      "Train Epoch: 65 [34560/50981 (68%)]\tLoss: 8914258.116236\n",
      "Train Epoch: 65 [35840/50981 (70%)]\tLoss: 8919190.895018\n",
      "Train Epoch: 65 [37120/50981 (73%)]\tLoss: 8917297.984536\n",
      "Train Epoch: 65 [38400/50981 (75%)]\tLoss: 8914635.210963\n",
      "Train Epoch: 65 [39680/50981 (78%)]\tLoss: 8909240.911576\n",
      "Train Epoch: 65 [40960/50981 (80%)]\tLoss: 8905066.311526\n",
      "Train Epoch: 65 [42240/50981 (83%)]\tLoss: 8903440.703927\n",
      "Train Epoch: 65 [43520/50981 (85%)]\tLoss: 8906108.340176\n",
      "Train Epoch: 65 [44800/50981 (88%)]\tLoss: 8907855.888889\n",
      "Train Epoch: 65 [46080/50981 (90%)]\tLoss: 8917086.141274\n",
      "Train Epoch: 65 [47360/50981 (93%)]\tLoss: 8920382.919137\n",
      "Train Epoch: 65 [48640/50981 (95%)]\tLoss: 8926645.706037\n",
      "Train Epoch: 65 [49920/50981 (98%)]\tLoss: 8929085.103581\n",
      "Train Epoch: 66 [0/50981 (0%)]\tLoss: 9083742.000000\n",
      "Train Epoch: 66 [1280/50981 (3%)]\tLoss: 8863583.636364\n",
      "Train Epoch: 66 [2560/50981 (5%)]\tLoss: 8778270.142857\n",
      "Train Epoch: 66 [3840/50981 (8%)]\tLoss: 8702118.467742\n",
      "Train Epoch: 66 [5120/50981 (10%)]\tLoss: 8704355.475610\n",
      "Train Epoch: 66 [6400/50981 (13%)]\tLoss: 8686053.441176\n",
      "Train Epoch: 66 [7680/50981 (15%)]\tLoss: 8733861.565574\n",
      "Train Epoch: 66 [8960/50981 (18%)]\tLoss: 8737095.922535\n",
      "Train Epoch: 66 [10240/50981 (20%)]\tLoss: 8739494.777778\n",
      "Train Epoch: 66 [11520/50981 (23%)]\tLoss: 8758435.010989\n",
      "Train Epoch: 66 [12800/50981 (25%)]\tLoss: 8756529.960396\n",
      "Train Epoch: 66 [14080/50981 (28%)]\tLoss: 8759553.198198\n",
      "Train Epoch: 66 [15360/50981 (30%)]\tLoss: 8777412.859504\n",
      "Train Epoch: 66 [16640/50981 (33%)]\tLoss: 8779273.740458\n",
      "Train Epoch: 66 [17920/50981 (35%)]\tLoss: 8803596.787234\n",
      "Train Epoch: 66 [19200/50981 (38%)]\tLoss: 8802827.271523\n",
      "Train Epoch: 66 [20480/50981 (40%)]\tLoss: 8794856.428571\n",
      "Train Epoch: 66 [21760/50981 (43%)]\tLoss: 8791780.777778\n",
      "Train Epoch: 66 [23040/50981 (45%)]\tLoss: 8798330.792818\n",
      "Train Epoch: 66 [24320/50981 (48%)]\tLoss: 8799517.735602\n",
      "Train Epoch: 66 [25600/50981 (50%)]\tLoss: 8811591.126866\n",
      "Train Epoch: 66 [26880/50981 (53%)]\tLoss: 8808003.267773\n",
      "Train Epoch: 66 [28160/50981 (55%)]\tLoss: 8826959.386878\n",
      "Train Epoch: 66 [29440/50981 (58%)]\tLoss: 8830599.465368\n",
      "Train Epoch: 66 [30720/50981 (60%)]\tLoss: 8832802.674274\n",
      "Train Epoch: 66 [32000/50981 (63%)]\tLoss: 8833931.067729\n",
      "Train Epoch: 66 [33280/50981 (65%)]\tLoss: 8829502.712644\n",
      "Train Epoch: 66 [34560/50981 (68%)]\tLoss: 8837924.996310\n",
      "Train Epoch: 66 [35840/50981 (70%)]\tLoss: 8841612.679715\n",
      "Train Epoch: 66 [37120/50981 (73%)]\tLoss: 8839257.139175\n",
      "Train Epoch: 66 [38400/50981 (75%)]\tLoss: 8843359.232558\n",
      "Train Epoch: 66 [39680/50981 (78%)]\tLoss: 8845859.916399\n",
      "Train Epoch: 66 [40960/50981 (80%)]\tLoss: 8851635.014019\n",
      "Train Epoch: 66 [42240/50981 (83%)]\tLoss: 8854449.907855\n",
      "Train Epoch: 66 [43520/50981 (85%)]\tLoss: 8863291.802053\n",
      "Train Epoch: 66 [44800/50981 (88%)]\tLoss: 8869033.594017\n",
      "Train Epoch: 66 [46080/50981 (90%)]\tLoss: 8872776.937673\n",
      "Train Epoch: 66 [47360/50981 (93%)]\tLoss: 8881396.586253\n",
      "Train Epoch: 66 [48640/50981 (95%)]\tLoss: 8886879.360892\n",
      "Train Epoch: 66 [49920/50981 (98%)]\tLoss: 8889505.630435\n",
      "Train Epoch: 67 [0/50981 (0%)]\tLoss: 8744585.000000\n",
      "Train Epoch: 67 [1280/50981 (3%)]\tLoss: 8616574.500000\n",
      "Train Epoch: 67 [2560/50981 (5%)]\tLoss: 8713573.142857\n",
      "Train Epoch: 67 [3840/50981 (8%)]\tLoss: 8775018.290323\n",
      "Train Epoch: 67 [5120/50981 (10%)]\tLoss: 8719670.987805\n",
      "Train Epoch: 67 [6400/50981 (13%)]\tLoss: 8711054.882353\n",
      "Train Epoch: 67 [7680/50981 (15%)]\tLoss: 8709627.786885\n",
      "Train Epoch: 67 [8960/50981 (18%)]\tLoss: 8686549.056338\n",
      "Train Epoch: 67 [10240/50981 (20%)]\tLoss: 8717806.975309\n",
      "Train Epoch: 67 [11520/50981 (23%)]\tLoss: 8730722.609890\n",
      "Train Epoch: 67 [12800/50981 (25%)]\tLoss: 8735566.133663\n",
      "Train Epoch: 67 [14080/50981 (28%)]\tLoss: 8739575.752252\n",
      "Train Epoch: 67 [15360/50981 (30%)]\tLoss: 8738820.409091\n",
      "Train Epoch: 67 [16640/50981 (33%)]\tLoss: 8733955.694656\n",
      "Train Epoch: 67 [17920/50981 (35%)]\tLoss: 8744258.390071\n",
      "Train Epoch: 67 [19200/50981 (38%)]\tLoss: 8745806.509934\n",
      "Train Epoch: 67 [20480/50981 (40%)]\tLoss: 8743549.813665\n",
      "Train Epoch: 67 [21760/50981 (43%)]\tLoss: 8763236.915205\n",
      "Train Epoch: 67 [23040/50981 (45%)]\tLoss: 8775714.527624\n",
      "Train Epoch: 67 [24320/50981 (48%)]\tLoss: 8784851.154450\n",
      "Train Epoch: 67 [25600/50981 (50%)]\tLoss: 8797911.768657\n",
      "Train Epoch: 67 [26880/50981 (53%)]\tLoss: 8795175.481043\n",
      "Train Epoch: 67 [28160/50981 (55%)]\tLoss: 8799108.997738\n",
      "Train Epoch: 67 [29440/50981 (58%)]\tLoss: 8807531.610390\n",
      "Train Epoch: 67 [30720/50981 (60%)]\tLoss: 8812247.601660\n",
      "Train Epoch: 67 [32000/50981 (63%)]\tLoss: 8815655.450199\n",
      "Train Epoch: 67 [33280/50981 (65%)]\tLoss: 8820156.854406\n",
      "Train Epoch: 67 [34560/50981 (68%)]\tLoss: 8823241.656827\n",
      "Train Epoch: 67 [35840/50981 (70%)]\tLoss: 8827772.658363\n",
      "Train Epoch: 67 [37120/50981 (73%)]\tLoss: 8829985.319588\n",
      "Train Epoch: 67 [38400/50981 (75%)]\tLoss: 8832792.310631\n",
      "Train Epoch: 67 [39680/50981 (78%)]\tLoss: 8838436.315113\n",
      "Train Epoch: 67 [40960/50981 (80%)]\tLoss: 8836604.964174\n",
      "Train Epoch: 67 [42240/50981 (83%)]\tLoss: 8843301.518127\n",
      "Train Epoch: 67 [43520/50981 (85%)]\tLoss: 8849150.673021\n",
      "Train Epoch: 67 [44800/50981 (88%)]\tLoss: 8850417.058405\n",
      "Train Epoch: 67 [46080/50981 (90%)]\tLoss: 8857264.444598\n",
      "Train Epoch: 67 [47360/50981 (93%)]\tLoss: 8855919.513477\n",
      "Train Epoch: 67 [48640/50981 (95%)]\tLoss: 8854387.060367\n",
      "Train Epoch: 67 [49920/50981 (98%)]\tLoss: 8856538.450128\n",
      "Train Epoch: 68 [0/50981 (0%)]\tLoss: 7799565.000000\n",
      "Train Epoch: 68 [1280/50981 (3%)]\tLoss: 8746270.636364\n",
      "Train Epoch: 68 [2560/50981 (5%)]\tLoss: 8670055.047619\n",
      "Train Epoch: 68 [3840/50981 (8%)]\tLoss: 8666863.500000\n",
      "Train Epoch: 68 [5120/50981 (10%)]\tLoss: 8668228.414634\n",
      "Train Epoch: 68 [6400/50981 (13%)]\tLoss: 8678420.813725\n",
      "Train Epoch: 68 [7680/50981 (15%)]\tLoss: 8675198.139344\n",
      "Train Epoch: 68 [8960/50981 (18%)]\tLoss: 8685718.647887\n",
      "Train Epoch: 68 [10240/50981 (20%)]\tLoss: 8689282.117284\n",
      "Train Epoch: 68 [11520/50981 (23%)]\tLoss: 8692240.098901\n",
      "Train Epoch: 68 [12800/50981 (25%)]\tLoss: 8700570.331683\n",
      "Train Epoch: 68 [14080/50981 (28%)]\tLoss: 8692236.536036\n",
      "Train Epoch: 68 [15360/50981 (30%)]\tLoss: 8708083.268595\n",
      "Train Epoch: 68 [16640/50981 (33%)]\tLoss: 8694715.832061\n",
      "Train Epoch: 68 [17920/50981 (35%)]\tLoss: 8705897.226950\n",
      "Train Epoch: 68 [19200/50981 (38%)]\tLoss: 8705581.847682\n",
      "Train Epoch: 68 [20480/50981 (40%)]\tLoss: 8717749.546584\n",
      "Train Epoch: 68 [21760/50981 (43%)]\tLoss: 8731086.017544\n",
      "Train Epoch: 68 [23040/50981 (45%)]\tLoss: 8745529.917127\n",
      "Train Epoch: 68 [24320/50981 (48%)]\tLoss: 8744090.806283\n",
      "Train Epoch: 68 [25600/50981 (50%)]\tLoss: 8746565.703980\n",
      "Train Epoch: 68 [26880/50981 (53%)]\tLoss: 8762369.864929\n",
      "Train Epoch: 68 [28160/50981 (55%)]\tLoss: 8778078.377828\n",
      "Train Epoch: 68 [29440/50981 (58%)]\tLoss: 8784987.694805\n",
      "Train Epoch: 68 [30720/50981 (60%)]\tLoss: 8791782.365145\n",
      "Train Epoch: 68 [32000/50981 (63%)]\tLoss: 8800800.191235\n",
      "Train Epoch: 68 [33280/50981 (65%)]\tLoss: 8806214.490421\n",
      "Train Epoch: 68 [34560/50981 (68%)]\tLoss: 8805917.621771\n",
      "Train Epoch: 68 [35840/50981 (70%)]\tLoss: 8815050.624555\n",
      "Train Epoch: 68 [37120/50981 (73%)]\tLoss: 8817969.338488\n",
      "Train Epoch: 68 [38400/50981 (75%)]\tLoss: 8820379.563123\n",
      "Train Epoch: 68 [39680/50981 (78%)]\tLoss: 8819564.231511\n",
      "Train Epoch: 68 [40960/50981 (80%)]\tLoss: 8820156.127726\n",
      "Train Epoch: 68 [42240/50981 (83%)]\tLoss: 8825059.483384\n",
      "Train Epoch: 68 [43520/50981 (85%)]\tLoss: 8827794.759531\n",
      "Train Epoch: 68 [44800/50981 (88%)]\tLoss: 8823807.820513\n",
      "Train Epoch: 68 [46080/50981 (90%)]\tLoss: 8830984.839335\n",
      "Train Epoch: 68 [47360/50981 (93%)]\tLoss: 8831804.013477\n",
      "Train Epoch: 68 [48640/50981 (95%)]\tLoss: 8834221.769029\n",
      "Train Epoch: 68 [49920/50981 (98%)]\tLoss: 8836069.448849\n",
      "Train Epoch: 69 [0/50981 (0%)]\tLoss: 9068386.000000\n",
      "Train Epoch: 69 [1280/50981 (3%)]\tLoss: 8607018.636364\n",
      "Train Epoch: 69 [2560/50981 (5%)]\tLoss: 8578534.285714\n",
      "Train Epoch: 69 [3840/50981 (8%)]\tLoss: 8611809.822581\n",
      "Train Epoch: 69 [5120/50981 (10%)]\tLoss: 8653771.987805\n",
      "Train Epoch: 69 [6400/50981 (13%)]\tLoss: 8689970.931373\n",
      "Train Epoch: 69 [7680/50981 (15%)]\tLoss: 8687876.737705\n",
      "Train Epoch: 69 [8960/50981 (18%)]\tLoss: 8678461.647887\n",
      "Train Epoch: 69 [10240/50981 (20%)]\tLoss: 8664229.814815\n",
      "Train Epoch: 69 [11520/50981 (23%)]\tLoss: 8652972.373626\n",
      "Train Epoch: 69 [12800/50981 (25%)]\tLoss: 8662585.099010\n",
      "Train Epoch: 69 [14080/50981 (28%)]\tLoss: 8665945.945946\n",
      "Train Epoch: 69 [15360/50981 (30%)]\tLoss: 8659672.814050\n",
      "Train Epoch: 69 [16640/50981 (33%)]\tLoss: 8677526.866412\n",
      "Train Epoch: 69 [17920/50981 (35%)]\tLoss: 8693815.007092\n",
      "Train Epoch: 69 [19200/50981 (38%)]\tLoss: 8684005.430464\n",
      "Train Epoch: 69 [20480/50981 (40%)]\tLoss: 8696112.990683\n",
      "Train Epoch: 69 [21760/50981 (43%)]\tLoss: 8712318.874269\n",
      "Train Epoch: 69 [23040/50981 (45%)]\tLoss: 8724757.555249\n",
      "Train Epoch: 69 [24320/50981 (48%)]\tLoss: 8737747.468586\n",
      "Train Epoch: 69 [25600/50981 (50%)]\tLoss: 8748195.293532\n",
      "Train Epoch: 69 [26880/50981 (53%)]\tLoss: 8750946.447867\n",
      "Train Epoch: 69 [28160/50981 (55%)]\tLoss: 8754208.604072\n",
      "Train Epoch: 69 [29440/50981 (58%)]\tLoss: 8766043.344156\n",
      "Train Epoch: 69 [30720/50981 (60%)]\tLoss: 8768807.632780\n",
      "Train Epoch: 69 [32000/50981 (63%)]\tLoss: 8761069.408367\n",
      "Train Epoch: 69 [33280/50981 (65%)]\tLoss: 8774257.139847\n",
      "Train Epoch: 69 [34560/50981 (68%)]\tLoss: 8778208.035055\n",
      "Train Epoch: 69 [35840/50981 (70%)]\tLoss: 8781608.823843\n",
      "Train Epoch: 69 [37120/50981 (73%)]\tLoss: 8779547.558419\n",
      "Train Epoch: 69 [38400/50981 (75%)]\tLoss: 8779259.931894\n",
      "Train Epoch: 69 [39680/50981 (78%)]\tLoss: 8777654.233119\n",
      "Train Epoch: 69 [40960/50981 (80%)]\tLoss: 8782504.132399\n",
      "Train Epoch: 69 [42240/50981 (83%)]\tLoss: 8782868.935045\n",
      "Train Epoch: 69 [43520/50981 (85%)]\tLoss: 8789557.986804\n",
      "Train Epoch: 69 [44800/50981 (88%)]\tLoss: 8793566.115385\n",
      "Train Epoch: 69 [46080/50981 (90%)]\tLoss: 8792585.283934\n",
      "Train Epoch: 69 [47360/50981 (93%)]\tLoss: 8797937.845013\n",
      "Train Epoch: 69 [48640/50981 (95%)]\tLoss: 8802688.073491\n",
      "Train Epoch: 69 [49920/50981 (98%)]\tLoss: 8808207.626598\n",
      "Train Epoch: 70 [0/50981 (0%)]\tLoss: 8939582.000000\n",
      "Train Epoch: 70 [1280/50981 (3%)]\tLoss: 8591177.636364\n",
      "Train Epoch: 70 [2560/50981 (5%)]\tLoss: 8590095.952381\n",
      "Train Epoch: 70 [3840/50981 (8%)]\tLoss: 8556549.725806\n",
      "Train Epoch: 70 [5120/50981 (10%)]\tLoss: 8580571.048780\n",
      "Train Epoch: 70 [6400/50981 (13%)]\tLoss: 8607155.725490\n",
      "Train Epoch: 70 [7680/50981 (15%)]\tLoss: 8577186.049180\n",
      "Train Epoch: 70 [8960/50981 (18%)]\tLoss: 8574657.591549\n",
      "Train Epoch: 70 [10240/50981 (20%)]\tLoss: 8602052.111111\n",
      "Train Epoch: 70 [11520/50981 (23%)]\tLoss: 8597333.164835\n",
      "Train Epoch: 70 [12800/50981 (25%)]\tLoss: 8608327.554455\n",
      "Train Epoch: 70 [14080/50981 (28%)]\tLoss: 8635457.315315\n",
      "Train Epoch: 70 [15360/50981 (30%)]\tLoss: 8650731.876033\n",
      "Train Epoch: 70 [16640/50981 (33%)]\tLoss: 8658143.114504\n",
      "Train Epoch: 70 [17920/50981 (35%)]\tLoss: 8671091.085106\n",
      "Train Epoch: 70 [19200/50981 (38%)]\tLoss: 8673951.172185\n",
      "Train Epoch: 70 [20480/50981 (40%)]\tLoss: 8696320.201863\n",
      "Train Epoch: 70 [21760/50981 (43%)]\tLoss: 8695933.140351\n",
      "Train Epoch: 70 [23040/50981 (45%)]\tLoss: 8703620.900552\n",
      "Train Epoch: 70 [24320/50981 (48%)]\tLoss: 8713262.073298\n",
      "Train Epoch: 70 [25600/50981 (50%)]\tLoss: 8725001.196517\n",
      "Train Epoch: 70 [26880/50981 (53%)]\tLoss: 8731624.421801\n",
      "Train Epoch: 70 [28160/50981 (55%)]\tLoss: 8728104.113122\n",
      "Train Epoch: 70 [29440/50981 (58%)]\tLoss: 8735990.406926\n",
      "Train Epoch: 70 [30720/50981 (60%)]\tLoss: 8747102.522822\n",
      "Train Epoch: 70 [32000/50981 (63%)]\tLoss: 8748826.330677\n",
      "Train Epoch: 70 [33280/50981 (65%)]\tLoss: 8744010.909962\n",
      "Train Epoch: 70 [34560/50981 (68%)]\tLoss: 8752762.485240\n",
      "Train Epoch: 70 [35840/50981 (70%)]\tLoss: 8754509.530249\n",
      "Train Epoch: 70 [37120/50981 (73%)]\tLoss: 8757237.470790\n",
      "Train Epoch: 70 [38400/50981 (75%)]\tLoss: 8761855.581395\n",
      "Train Epoch: 70 [39680/50981 (78%)]\tLoss: 8761548.633441\n",
      "Train Epoch: 70 [40960/50981 (80%)]\tLoss: 8763777.825545\n",
      "Train Epoch: 70 [42240/50981 (83%)]\tLoss: 8765569.966767\n",
      "Train Epoch: 70 [43520/50981 (85%)]\tLoss: 8768340.510264\n",
      "Train Epoch: 70 [44800/50981 (88%)]\tLoss: 8764766.279202\n",
      "Train Epoch: 70 [46080/50981 (90%)]\tLoss: 8767059.224377\n",
      "Train Epoch: 70 [47360/50981 (93%)]\tLoss: 8770688.380054\n",
      "Train Epoch: 70 [48640/50981 (95%)]\tLoss: 8776365.102362\n",
      "Train Epoch: 70 [49920/50981 (98%)]\tLoss: 8777679.120205\n",
      "Train Epoch: 71 [0/50981 (0%)]\tLoss: 8547165.000000\n",
      "Train Epoch: 71 [1280/50981 (3%)]\tLoss: 8416389.590909\n",
      "Train Epoch: 71 [2560/50981 (5%)]\tLoss: 8486712.380952\n",
      "Train Epoch: 71 [3840/50981 (8%)]\tLoss: 8462372.870968\n",
      "Train Epoch: 71 [5120/50981 (10%)]\tLoss: 8497113.865854\n",
      "Train Epoch: 71 [6400/50981 (13%)]\tLoss: 8502637.784314\n",
      "Train Epoch: 71 [7680/50981 (15%)]\tLoss: 8493518.442623\n",
      "Train Epoch: 71 [8960/50981 (18%)]\tLoss: 8508746.507042\n",
      "Train Epoch: 71 [10240/50981 (20%)]\tLoss: 8554643.592593\n",
      "Train Epoch: 71 [11520/50981 (23%)]\tLoss: 8550101.510989\n",
      "Train Epoch: 71 [12800/50981 (25%)]\tLoss: 8580604.272277\n",
      "Train Epoch: 71 [14080/50981 (28%)]\tLoss: 8585232.531532\n",
      "Train Epoch: 71 [15360/50981 (30%)]\tLoss: 8603319.322314\n",
      "Train Epoch: 71 [16640/50981 (33%)]\tLoss: 8599634.175573\n",
      "Train Epoch: 71 [17920/50981 (35%)]\tLoss: 8598789.315603\n",
      "Train Epoch: 71 [19200/50981 (38%)]\tLoss: 8616521.738411\n",
      "Train Epoch: 71 [20480/50981 (40%)]\tLoss: 8621868.096273\n",
      "Train Epoch: 71 [21760/50981 (43%)]\tLoss: 8645360.412281\n",
      "Train Epoch: 71 [23040/50981 (45%)]\tLoss: 8651031.649171\n",
      "Train Epoch: 71 [24320/50981 (48%)]\tLoss: 8652233.887435\n",
      "Train Epoch: 71 [25600/50981 (50%)]\tLoss: 8661614.256219\n",
      "Train Epoch: 71 [26880/50981 (53%)]\tLoss: 8669229.409953\n",
      "Train Epoch: 71 [28160/50981 (55%)]\tLoss: 8674185.581448\n",
      "Train Epoch: 71 [29440/50981 (58%)]\tLoss: 8690938.645022\n",
      "Train Epoch: 71 [30720/50981 (60%)]\tLoss: 8691473.174274\n",
      "Train Epoch: 71 [32000/50981 (63%)]\tLoss: 8702881.745020\n",
      "Train Epoch: 71 [33280/50981 (65%)]\tLoss: 8699778.923372\n",
      "Train Epoch: 71 [34560/50981 (68%)]\tLoss: 8711158.228782\n",
      "Train Epoch: 71 [35840/50981 (70%)]\tLoss: 8712450.583630\n",
      "Train Epoch: 71 [37120/50981 (73%)]\tLoss: 8713386.091065\n",
      "Train Epoch: 71 [38400/50981 (75%)]\tLoss: 8721802.704319\n",
      "Train Epoch: 71 [39680/50981 (78%)]\tLoss: 8723501.604502\n",
      "Train Epoch: 71 [40960/50981 (80%)]\tLoss: 8722690.450156\n",
      "Train Epoch: 71 [42240/50981 (83%)]\tLoss: 8720233.990937\n",
      "Train Epoch: 71 [43520/50981 (85%)]\tLoss: 8727573.586510\n",
      "Train Epoch: 71 [44800/50981 (88%)]\tLoss: 8729283.215100\n",
      "Train Epoch: 71 [46080/50981 (90%)]\tLoss: 8739052.832410\n",
      "Train Epoch: 71 [47360/50981 (93%)]\tLoss: 8743875.456873\n",
      "Train Epoch: 71 [48640/50981 (95%)]\tLoss: 8750619.211286\n",
      "Train Epoch: 71 [49920/50981 (98%)]\tLoss: 8754259.625320\n",
      "Train Epoch: 72 [0/50981 (0%)]\tLoss: 8289301.500000\n",
      "Train Epoch: 72 [1280/50981 (3%)]\tLoss: 8449743.409091\n",
      "Train Epoch: 72 [2560/50981 (5%)]\tLoss: 8441107.714286\n",
      "Train Epoch: 72 [3840/50981 (8%)]\tLoss: 8450855.274194\n",
      "Train Epoch: 72 [5120/50981 (10%)]\tLoss: 8458070.719512\n",
      "Train Epoch: 72 [6400/50981 (13%)]\tLoss: 8507038.392157\n",
      "Train Epoch: 72 [7680/50981 (15%)]\tLoss: 8559222.819672\n",
      "Train Epoch: 72 [8960/50981 (18%)]\tLoss: 8574237.943662\n",
      "Train Epoch: 72 [10240/50981 (20%)]\tLoss: 8566609.000000\n",
      "Train Epoch: 72 [11520/50981 (23%)]\tLoss: 8574427.774725\n",
      "Train Epoch: 72 [12800/50981 (25%)]\tLoss: 8591619.321782\n",
      "Train Epoch: 72 [14080/50981 (28%)]\tLoss: 8600027.472973\n",
      "Train Epoch: 72 [15360/50981 (30%)]\tLoss: 8586979.053719\n",
      "Train Epoch: 72 [16640/50981 (33%)]\tLoss: 8601621.038168\n",
      "Train Epoch: 72 [17920/50981 (35%)]\tLoss: 8609303.570922\n",
      "Train Epoch: 72 [19200/50981 (38%)]\tLoss: 8610020.377483\n",
      "Train Epoch: 72 [20480/50981 (40%)]\tLoss: 8610798.195652\n",
      "Train Epoch: 72 [21760/50981 (43%)]\tLoss: 8604684.874269\n",
      "Train Epoch: 72 [23040/50981 (45%)]\tLoss: 8613664.494475\n",
      "Train Epoch: 72 [24320/50981 (48%)]\tLoss: 8614818.172775\n",
      "Train Epoch: 72 [25600/50981 (50%)]\tLoss: 8618473.860697\n",
      "Train Epoch: 72 [26880/50981 (53%)]\tLoss: 8632143.585308\n",
      "Train Epoch: 72 [28160/50981 (55%)]\tLoss: 8644780.000000\n",
      "Train Epoch: 72 [29440/50981 (58%)]\tLoss: 8643801.614719\n",
      "Train Epoch: 72 [30720/50981 (60%)]\tLoss: 8649287.981328\n",
      "Train Epoch: 72 [32000/50981 (63%)]\tLoss: 8660277.370518\n",
      "Train Epoch: 72 [33280/50981 (65%)]\tLoss: 8661388.530651\n",
      "Train Epoch: 72 [34560/50981 (68%)]\tLoss: 8668503.228782\n",
      "Train Epoch: 72 [35840/50981 (70%)]\tLoss: 8669035.042705\n",
      "Train Epoch: 72 [37120/50981 (73%)]\tLoss: 8670651.745704\n",
      "Train Epoch: 72 [38400/50981 (75%)]\tLoss: 8677150.961794\n",
      "Train Epoch: 72 [39680/50981 (78%)]\tLoss: 8679057.837621\n",
      "Train Epoch: 72 [40960/50981 (80%)]\tLoss: 8684386.786604\n",
      "Train Epoch: 72 [42240/50981 (83%)]\tLoss: 8697214.545317\n",
      "Train Epoch: 72 [43520/50981 (85%)]\tLoss: 8701981.555718\n",
      "Train Epoch: 72 [44800/50981 (88%)]\tLoss: 8710964.329060\n",
      "Train Epoch: 72 [46080/50981 (90%)]\tLoss: 8714151.486150\n",
      "Train Epoch: 72 [47360/50981 (93%)]\tLoss: 8716790.850404\n",
      "Train Epoch: 72 [48640/50981 (95%)]\tLoss: 8718527.402887\n",
      "Train Epoch: 72 [49920/50981 (98%)]\tLoss: 8725735.372123\n",
      "Train Epoch: 73 [0/50981 (0%)]\tLoss: 8362943.500000\n",
      "Train Epoch: 73 [1280/50981 (3%)]\tLoss: 8486470.272727\n",
      "Train Epoch: 73 [2560/50981 (5%)]\tLoss: 8375910.547619\n",
      "Train Epoch: 73 [3840/50981 (8%)]\tLoss: 8430250.725806\n",
      "Train Epoch: 73 [5120/50981 (10%)]\tLoss: 8452165.329268\n",
      "Train Epoch: 73 [6400/50981 (13%)]\tLoss: 8462854.303922\n",
      "Train Epoch: 73 [7680/50981 (15%)]\tLoss: 8481936.196721\n",
      "Train Epoch: 73 [8960/50981 (18%)]\tLoss: 8478224.943662\n",
      "Train Epoch: 73 [10240/50981 (20%)]\tLoss: 8483865.141975\n",
      "Train Epoch: 73 [11520/50981 (23%)]\tLoss: 8500423.379121\n",
      "Train Epoch: 73 [12800/50981 (25%)]\tLoss: 8502792.029703\n",
      "Train Epoch: 73 [14080/50981 (28%)]\tLoss: 8501203.644144\n",
      "Train Epoch: 73 [15360/50981 (30%)]\tLoss: 8513146.735537\n",
      "Train Epoch: 73 [16640/50981 (33%)]\tLoss: 8525131.068702\n",
      "Train Epoch: 73 [17920/50981 (35%)]\tLoss: 8532620.429078\n",
      "Train Epoch: 73 [19200/50981 (38%)]\tLoss: 8534113.105960\n",
      "Train Epoch: 73 [20480/50981 (40%)]\tLoss: 8543144.565217\n",
      "Train Epoch: 73 [21760/50981 (43%)]\tLoss: 8545085.564327\n",
      "Train Epoch: 73 [23040/50981 (45%)]\tLoss: 8554285.853591\n",
      "Train Epoch: 73 [24320/50981 (48%)]\tLoss: 8563008.918848\n",
      "Train Epoch: 73 [25600/50981 (50%)]\tLoss: 8573390.189055\n",
      "Train Epoch: 73 [26880/50981 (53%)]\tLoss: 8582761.943128\n",
      "Train Epoch: 73 [28160/50981 (55%)]\tLoss: 8592916.536199\n",
      "Train Epoch: 73 [29440/50981 (58%)]\tLoss: 8604222.783550\n",
      "Train Epoch: 73 [30720/50981 (60%)]\tLoss: 8611347.464730\n",
      "Train Epoch: 73 [32000/50981 (63%)]\tLoss: 8613441.344622\n",
      "Train Epoch: 73 [33280/50981 (65%)]\tLoss: 8620414.415709\n",
      "Train Epoch: 73 [34560/50981 (68%)]\tLoss: 8622936.559041\n",
      "Train Epoch: 73 [35840/50981 (70%)]\tLoss: 8631675.877224\n",
      "Train Epoch: 73 [37120/50981 (73%)]\tLoss: 8638450.163230\n",
      "Train Epoch: 73 [38400/50981 (75%)]\tLoss: 8643915.159468\n",
      "Train Epoch: 73 [39680/50981 (78%)]\tLoss: 8649451.887460\n",
      "Train Epoch: 73 [40960/50981 (80%)]\tLoss: 8658877.766355\n",
      "Train Epoch: 73 [42240/50981 (83%)]\tLoss: 8667115.419940\n",
      "Train Epoch: 73 [43520/50981 (85%)]\tLoss: 8665820.730205\n",
      "Train Epoch: 73 [44800/50981 (88%)]\tLoss: 8670204.675214\n",
      "Train Epoch: 73 [46080/50981 (90%)]\tLoss: 8676699.429363\n",
      "Train Epoch: 73 [47360/50981 (93%)]\tLoss: 8681381.916442\n",
      "Train Epoch: 73 [48640/50981 (95%)]\tLoss: 8685602.167979\n",
      "Train Epoch: 73 [49920/50981 (98%)]\tLoss: 8694726.708440\n",
      "Train Epoch: 74 [0/50981 (0%)]\tLoss: 8156215.000000\n",
      "Train Epoch: 74 [1280/50981 (3%)]\tLoss: 8408083.681818\n",
      "Train Epoch: 74 [2560/50981 (5%)]\tLoss: 8385532.047619\n",
      "Train Epoch: 74 [3840/50981 (8%)]\tLoss: 8372312.274194\n",
      "Train Epoch: 74 [5120/50981 (10%)]\tLoss: 8404448.182927\n",
      "Train Epoch: 74 [6400/50981 (13%)]\tLoss: 8419218.303922\n",
      "Train Epoch: 74 [7680/50981 (15%)]\tLoss: 8428266.926230\n",
      "Train Epoch: 74 [8960/50981 (18%)]\tLoss: 8466216.021127\n",
      "Train Epoch: 74 [10240/50981 (20%)]\tLoss: 8491646.907407\n",
      "Train Epoch: 74 [11520/50981 (23%)]\tLoss: 8513360.763736\n",
      "Train Epoch: 74 [12800/50981 (25%)]\tLoss: 8521178.633663\n",
      "Train Epoch: 74 [14080/50981 (28%)]\tLoss: 8535371.527027\n",
      "Train Epoch: 74 [15360/50981 (30%)]\tLoss: 8553627.780992\n",
      "Train Epoch: 74 [16640/50981 (33%)]\tLoss: 8561054.087786\n",
      "Train Epoch: 74 [17920/50981 (35%)]\tLoss: 8575833.833333\n",
      "Train Epoch: 74 [19200/50981 (38%)]\tLoss: 8583367.165563\n",
      "Train Epoch: 74 [20480/50981 (40%)]\tLoss: 8584219.795031\n",
      "Train Epoch: 74 [21760/50981 (43%)]\tLoss: 8594587.754386\n",
      "Train Epoch: 74 [23040/50981 (45%)]\tLoss: 8604691.395028\n",
      "Train Epoch: 74 [24320/50981 (48%)]\tLoss: 8611172.332461\n",
      "Train Epoch: 74 [25600/50981 (50%)]\tLoss: 8609425.490050\n",
      "Train Epoch: 74 [26880/50981 (53%)]\tLoss: 8608018.618483\n",
      "Train Epoch: 74 [28160/50981 (55%)]\tLoss: 8611755.730769\n",
      "Train Epoch: 74 [29440/50981 (58%)]\tLoss: 8622184.155844\n",
      "Train Epoch: 74 [30720/50981 (60%)]\tLoss: 8625398.780083\n",
      "Train Epoch: 74 [32000/50981 (63%)]\tLoss: 8628261.063745\n",
      "Train Epoch: 74 [33280/50981 (65%)]\tLoss: 8631906.590038\n",
      "Train Epoch: 74 [34560/50981 (68%)]\tLoss: 8635622.461255\n",
      "Train Epoch: 74 [35840/50981 (70%)]\tLoss: 8635003.437722\n",
      "Train Epoch: 74 [37120/50981 (73%)]\tLoss: 8639311.967354\n",
      "Train Epoch: 74 [38400/50981 (75%)]\tLoss: 8639916.219269\n",
      "Train Epoch: 74 [39680/50981 (78%)]\tLoss: 8640664.434084\n",
      "Train Epoch: 74 [40960/50981 (80%)]\tLoss: 8643837.404984\n",
      "Train Epoch: 74 [42240/50981 (83%)]\tLoss: 8652517.060423\n",
      "Train Epoch: 74 [43520/50981 (85%)]\tLoss: 8650620.989736\n",
      "Train Epoch: 74 [44800/50981 (88%)]\tLoss: 8660528.970085\n",
      "Train Epoch: 74 [46080/50981 (90%)]\tLoss: 8669252.907202\n",
      "Train Epoch: 74 [47360/50981 (93%)]\tLoss: 8673450.753369\n",
      "Train Epoch: 74 [48640/50981 (95%)]\tLoss: 8671991.269029\n",
      "Train Epoch: 74 [49920/50981 (98%)]\tLoss: 8675306.111253\n",
      "Train Epoch: 75 [0/50981 (0%)]\tLoss: 8597622.000000\n",
      "Train Epoch: 75 [1280/50981 (3%)]\tLoss: 8508961.636364\n",
      "Train Epoch: 75 [2560/50981 (5%)]\tLoss: 8435445.071429\n",
      "Train Epoch: 75 [3840/50981 (8%)]\tLoss: 8520685.032258\n",
      "Train Epoch: 75 [5120/50981 (10%)]\tLoss: 8541778.804878\n",
      "Train Epoch: 75 [6400/50981 (13%)]\tLoss: 8559092.303922\n",
      "Train Epoch: 75 [7680/50981 (15%)]\tLoss: 8515227.836066\n",
      "Train Epoch: 75 [8960/50981 (18%)]\tLoss: 8499000.908451\n",
      "Train Epoch: 75 [10240/50981 (20%)]\tLoss: 8499579.487654\n",
      "Train Epoch: 75 [11520/50981 (23%)]\tLoss: 8500471.104396\n",
      "Train Epoch: 75 [12800/50981 (25%)]\tLoss: 8506106.960396\n",
      "Train Epoch: 75 [14080/50981 (28%)]\tLoss: 8522853.351351\n",
      "Train Epoch: 75 [15360/50981 (30%)]\tLoss: 8533742.272727\n",
      "Train Epoch: 75 [16640/50981 (33%)]\tLoss: 8547641.049618\n",
      "Train Epoch: 75 [17920/50981 (35%)]\tLoss: 8549388.063830\n",
      "Train Epoch: 75 [19200/50981 (38%)]\tLoss: 8552835.231788\n",
      "Train Epoch: 75 [20480/50981 (40%)]\tLoss: 8554113.512422\n",
      "Train Epoch: 75 [21760/50981 (43%)]\tLoss: 8557468.824561\n",
      "Train Epoch: 75 [23040/50981 (45%)]\tLoss: 8566285.834254\n",
      "Train Epoch: 75 [24320/50981 (48%)]\tLoss: 8564573.329843\n",
      "Train Epoch: 75 [25600/50981 (50%)]\tLoss: 8573968.514925\n",
      "Train Epoch: 75 [26880/50981 (53%)]\tLoss: 8585912.755924\n",
      "Train Epoch: 75 [28160/50981 (55%)]\tLoss: 8590921.418552\n",
      "Train Epoch: 75 [29440/50981 (58%)]\tLoss: 8595891.456710\n",
      "Train Epoch: 75 [30720/50981 (60%)]\tLoss: 8593779.470954\n",
      "Train Epoch: 75 [32000/50981 (63%)]\tLoss: 8595988.145418\n",
      "Train Epoch: 75 [33280/50981 (65%)]\tLoss: 8601953.187739\n",
      "Train Epoch: 75 [34560/50981 (68%)]\tLoss: 8603043.483395\n",
      "Train Epoch: 75 [35840/50981 (70%)]\tLoss: 8602143.088968\n",
      "Train Epoch: 75 [37120/50981 (73%)]\tLoss: 8613080.436426\n",
      "Train Epoch: 75 [38400/50981 (75%)]\tLoss: 8621676.948505\n",
      "Train Epoch: 75 [39680/50981 (78%)]\tLoss: 8624539.816720\n",
      "Train Epoch: 75 [40960/50981 (80%)]\tLoss: 8630168.350467\n",
      "Train Epoch: 75 [42240/50981 (83%)]\tLoss: 8628006.845921\n",
      "Train Epoch: 75 [43520/50981 (85%)]\tLoss: 8627687.825513\n",
      "Train Epoch: 75 [44800/50981 (88%)]\tLoss: 8632516.471510\n",
      "Train Epoch: 75 [46080/50981 (90%)]\tLoss: 8632505.966759\n",
      "Train Epoch: 75 [47360/50981 (93%)]\tLoss: 8635149.401617\n",
      "Train Epoch: 75 [48640/50981 (95%)]\tLoss: 8635409.056430\n",
      "Train Epoch: 75 [49920/50981 (98%)]\tLoss: 8639411.264706\n",
      "Train Epoch: 76 [0/50981 (0%)]\tLoss: 8588826.000000\n",
      "Train Epoch: 76 [1280/50981 (3%)]\tLoss: 8197473.681818\n",
      "Train Epoch: 76 [2560/50981 (5%)]\tLoss: 8297623.785714\n",
      "Train Epoch: 76 [3840/50981 (8%)]\tLoss: 8359318.016129\n",
      "Train Epoch: 76 [5120/50981 (10%)]\tLoss: 8400463.597561\n",
      "Train Epoch: 76 [6400/50981 (13%)]\tLoss: 8434626.254902\n",
      "Train Epoch: 76 [7680/50981 (15%)]\tLoss: 8459976.737705\n",
      "Train Epoch: 76 [8960/50981 (18%)]\tLoss: 8463705.549296\n",
      "Train Epoch: 76 [10240/50981 (20%)]\tLoss: 8471945.993827\n",
      "Train Epoch: 76 [11520/50981 (23%)]\tLoss: 8472545.318681\n",
      "Train Epoch: 76 [12800/50981 (25%)]\tLoss: 8468080.485149\n",
      "Train Epoch: 76 [14080/50981 (28%)]\tLoss: 8480261.040541\n",
      "Train Epoch: 76 [15360/50981 (30%)]\tLoss: 8486378.768595\n",
      "Train Epoch: 76 [16640/50981 (33%)]\tLoss: 8495668.232824\n",
      "Train Epoch: 76 [17920/50981 (35%)]\tLoss: 8510042.549645\n",
      "Train Epoch: 76 [19200/50981 (38%)]\tLoss: 8521350.947020\n",
      "Train Epoch: 76 [20480/50981 (40%)]\tLoss: 8526050.198758\n",
      "Train Epoch: 76 [21760/50981 (43%)]\tLoss: 8543444.461988\n",
      "Train Epoch: 76 [23040/50981 (45%)]\tLoss: 8545706.395028\n",
      "Train Epoch: 76 [24320/50981 (48%)]\tLoss: 8553271.759162\n",
      "Train Epoch: 76 [25600/50981 (50%)]\tLoss: 8551203.029851\n",
      "Train Epoch: 76 [26880/50981 (53%)]\tLoss: 8554268.881517\n",
      "Train Epoch: 76 [28160/50981 (55%)]\tLoss: 8560337.828054\n",
      "Train Epoch: 76 [29440/50981 (58%)]\tLoss: 8553120.004329\n",
      "Train Epoch: 76 [30720/50981 (60%)]\tLoss: 8565295.966805\n",
      "Train Epoch: 76 [32000/50981 (63%)]\tLoss: 8573152.727092\n",
      "Train Epoch: 76 [33280/50981 (65%)]\tLoss: 8567858.791188\n",
      "Train Epoch: 76 [34560/50981 (68%)]\tLoss: 8567405.463100\n",
      "Train Epoch: 76 [35840/50981 (70%)]\tLoss: 8574373.800712\n",
      "Train Epoch: 76 [37120/50981 (73%)]\tLoss: 8583891.505155\n",
      "Train Epoch: 76 [38400/50981 (75%)]\tLoss: 8583195.083056\n",
      "Train Epoch: 76 [39680/50981 (78%)]\tLoss: 8591210.427653\n",
      "Train Epoch: 76 [40960/50981 (80%)]\tLoss: 8596243.286604\n",
      "Train Epoch: 76 [42240/50981 (83%)]\tLoss: 8598698.344411\n",
      "Train Epoch: 76 [43520/50981 (85%)]\tLoss: 8598217.678886\n",
      "Train Epoch: 76 [44800/50981 (88%)]\tLoss: 8611096.477208\n",
      "Train Epoch: 76 [46080/50981 (90%)]\tLoss: 8613586.754848\n",
      "Train Epoch: 76 [47360/50981 (93%)]\tLoss: 8617486.556604\n",
      "Train Epoch: 76 [48640/50981 (95%)]\tLoss: 8613663.998688\n",
      "Train Epoch: 76 [49920/50981 (98%)]\tLoss: 8614987.136829\n",
      "Train Epoch: 77 [0/50981 (0%)]\tLoss: 8390187.000000\n",
      "Train Epoch: 77 [1280/50981 (3%)]\tLoss: 8234049.954545\n",
      "Train Epoch: 77 [2560/50981 (5%)]\tLoss: 8357898.976190\n",
      "Train Epoch: 77 [3840/50981 (8%)]\tLoss: 8354143.983871\n",
      "Train Epoch: 77 [5120/50981 (10%)]\tLoss: 8378931.621951\n",
      "Train Epoch: 77 [6400/50981 (13%)]\tLoss: 8392343.911765\n",
      "Train Epoch: 77 [7680/50981 (15%)]\tLoss: 8423635.581967\n",
      "Train Epoch: 77 [8960/50981 (18%)]\tLoss: 8437779.077465\n",
      "Train Epoch: 77 [10240/50981 (20%)]\tLoss: 8446485.993827\n",
      "Train Epoch: 77 [11520/50981 (23%)]\tLoss: 8457438.857143\n",
      "Train Epoch: 77 [12800/50981 (25%)]\tLoss: 8476537.980198\n",
      "Train Epoch: 77 [14080/50981 (28%)]\tLoss: 8488347.941441\n",
      "Train Epoch: 77 [15360/50981 (30%)]\tLoss: 8502669.475207\n",
      "Train Epoch: 77 [16640/50981 (33%)]\tLoss: 8497426.106870\n",
      "Train Epoch: 77 [17920/50981 (35%)]\tLoss: 8505049.049645\n",
      "Train Epoch: 77 [19200/50981 (38%)]\tLoss: 8502413.963576\n",
      "Train Epoch: 77 [20480/50981 (40%)]\tLoss: 8506587.363354\n",
      "Train Epoch: 77 [21760/50981 (43%)]\tLoss: 8507094.029240\n",
      "Train Epoch: 77 [23040/50981 (45%)]\tLoss: 8521615.635359\n",
      "Train Epoch: 77 [24320/50981 (48%)]\tLoss: 8517646.041885\n",
      "Train Epoch: 77 [25600/50981 (50%)]\tLoss: 8521761.395522\n",
      "Train Epoch: 77 [26880/50981 (53%)]\tLoss: 8526481.888626\n",
      "Train Epoch: 77 [28160/50981 (55%)]\tLoss: 8534934.187783\n",
      "Train Epoch: 77 [29440/50981 (58%)]\tLoss: 8535241.246753\n",
      "Train Epoch: 77 [30720/50981 (60%)]\tLoss: 8542810.172199\n",
      "Train Epoch: 77 [32000/50981 (63%)]\tLoss: 8542125.316733\n",
      "Train Epoch: 77 [33280/50981 (65%)]\tLoss: 8547496.779693\n",
      "Train Epoch: 77 [34560/50981 (68%)]\tLoss: 8554069.426199\n",
      "Train Epoch: 77 [35840/50981 (70%)]\tLoss: 8558685.115658\n",
      "Train Epoch: 77 [37120/50981 (73%)]\tLoss: 8564129.352234\n",
      "Train Epoch: 77 [38400/50981 (75%)]\tLoss: 8569899.383721\n",
      "Train Epoch: 77 [39680/50981 (78%)]\tLoss: 8572827.535370\n",
      "Train Epoch: 77 [40960/50981 (80%)]\tLoss: 8576329.792835\n",
      "Train Epoch: 77 [42240/50981 (83%)]\tLoss: 8581295.441088\n",
      "Train Epoch: 77 [43520/50981 (85%)]\tLoss: 8581658.901760\n",
      "Train Epoch: 77 [44800/50981 (88%)]\tLoss: 8579874.129630\n",
      "Train Epoch: 77 [46080/50981 (90%)]\tLoss: 8584357.610803\n",
      "Train Epoch: 77 [47360/50981 (93%)]\tLoss: 8585084.702156\n",
      "Train Epoch: 77 [48640/50981 (95%)]\tLoss: 8588900.392388\n",
      "Train Epoch: 77 [49920/50981 (98%)]\tLoss: 8593313.406650\n",
      "Train Epoch: 78 [0/50981 (0%)]\tLoss: 8014867.500000\n",
      "Train Epoch: 78 [1280/50981 (3%)]\tLoss: 8241888.954545\n",
      "Train Epoch: 78 [2560/50981 (5%)]\tLoss: 8254388.476190\n",
      "Train Epoch: 78 [3840/50981 (8%)]\tLoss: 8356632.629032\n",
      "Train Epoch: 78 [5120/50981 (10%)]\tLoss: 8395097.512195\n",
      "Train Epoch: 78 [6400/50981 (13%)]\tLoss: 8419748.774510\n",
      "Train Epoch: 78 [7680/50981 (15%)]\tLoss: 8407462.122951\n",
      "Train Epoch: 78 [8960/50981 (18%)]\tLoss: 8425363.415493\n",
      "Train Epoch: 78 [10240/50981 (20%)]\tLoss: 8438869.858025\n",
      "Train Epoch: 78 [11520/50981 (23%)]\tLoss: 8456089.598901\n",
      "Train Epoch: 78 [12800/50981 (25%)]\tLoss: 8455990.638614\n",
      "Train Epoch: 78 [14080/50981 (28%)]\tLoss: 8446171.612613\n",
      "Train Epoch: 78 [15360/50981 (30%)]\tLoss: 8446784.776860\n",
      "Train Epoch: 78 [16640/50981 (33%)]\tLoss: 8444242.526718\n",
      "Train Epoch: 78 [17920/50981 (35%)]\tLoss: 8450804.992908\n",
      "Train Epoch: 78 [19200/50981 (38%)]\tLoss: 8458402.496689\n",
      "Train Epoch: 78 [20480/50981 (40%)]\tLoss: 8452909.664596\n",
      "Train Epoch: 78 [21760/50981 (43%)]\tLoss: 8456804.321637\n",
      "Train Epoch: 78 [23040/50981 (45%)]\tLoss: 8466985.530387\n",
      "Train Epoch: 78 [24320/50981 (48%)]\tLoss: 8478375.735602\n",
      "Train Epoch: 78 [25600/50981 (50%)]\tLoss: 8486545.467662\n",
      "Train Epoch: 78 [26880/50981 (53%)]\tLoss: 8503909.661137\n",
      "Train Epoch: 78 [28160/50981 (55%)]\tLoss: 8501861.429864\n",
      "Train Epoch: 78 [29440/50981 (58%)]\tLoss: 8499982.504329\n",
      "Train Epoch: 78 [30720/50981 (60%)]\tLoss: 8500863.997925\n",
      "Train Epoch: 78 [32000/50981 (63%)]\tLoss: 8506366.191235\n",
      "Train Epoch: 78 [33280/50981 (65%)]\tLoss: 8507521.300766\n",
      "Train Epoch: 78 [34560/50981 (68%)]\tLoss: 8516735.675277\n",
      "Train Epoch: 78 [35840/50981 (70%)]\tLoss: 8525753.419929\n",
      "Train Epoch: 78 [37120/50981 (73%)]\tLoss: 8529029.156357\n",
      "Train Epoch: 78 [38400/50981 (75%)]\tLoss: 8531033.951827\n",
      "Train Epoch: 78 [39680/50981 (78%)]\tLoss: 8541250.869775\n",
      "Train Epoch: 78 [40960/50981 (80%)]\tLoss: 8542362.165109\n",
      "Train Epoch: 78 [42240/50981 (83%)]\tLoss: 8547376.864048\n",
      "Train Epoch: 78 [43520/50981 (85%)]\tLoss: 8550409.146628\n",
      "Train Epoch: 78 [44800/50981 (88%)]\tLoss: 8557907.954416\n",
      "Train Epoch: 78 [46080/50981 (90%)]\tLoss: 8555143.249307\n",
      "Train Epoch: 78 [47360/50981 (93%)]\tLoss: 8558029.261456\n",
      "Train Epoch: 78 [48640/50981 (95%)]\tLoss: 8564601.900262\n",
      "Train Epoch: 78 [49920/50981 (98%)]\tLoss: 8569392.294118\n",
      "Train Epoch: 79 [0/50981 (0%)]\tLoss: 8251173.000000\n",
      "Train Epoch: 79 [1280/50981 (3%)]\tLoss: 8399029.909091\n",
      "Train Epoch: 79 [2560/50981 (5%)]\tLoss: 8336109.238095\n",
      "Train Epoch: 79 [3840/50981 (8%)]\tLoss: 8323392.064516\n",
      "Train Epoch: 79 [5120/50981 (10%)]\tLoss: 8306079.207317\n",
      "Train Epoch: 79 [6400/50981 (13%)]\tLoss: 8278616.529412\n",
      "Train Epoch: 79 [7680/50981 (15%)]\tLoss: 8317808.901639\n",
      "Train Epoch: 79 [8960/50981 (18%)]\tLoss: 8332990.380282\n",
      "Train Epoch: 79 [10240/50981 (20%)]\tLoss: 8336207.407407\n",
      "Train Epoch: 79 [11520/50981 (23%)]\tLoss: 8347305.538462\n",
      "Train Epoch: 79 [12800/50981 (25%)]\tLoss: 8391624.326733\n",
      "Train Epoch: 79 [14080/50981 (28%)]\tLoss: 8430754.522523\n",
      "Train Epoch: 79 [15360/50981 (30%)]\tLoss: 8423428.566116\n",
      "Train Epoch: 79 [16640/50981 (33%)]\tLoss: 8428756.694656\n",
      "Train Epoch: 79 [17920/50981 (35%)]\tLoss: 8442295.014184\n",
      "Train Epoch: 79 [19200/50981 (38%)]\tLoss: 8449799.903974\n",
      "Train Epoch: 79 [20480/50981 (40%)]\tLoss: 8448569.739130\n",
      "Train Epoch: 79 [21760/50981 (43%)]\tLoss: 8447279.049708\n",
      "Train Epoch: 79 [23040/50981 (45%)]\tLoss: 8461650.069061\n",
      "Train Epoch: 79 [24320/50981 (48%)]\tLoss: 8473683.798429\n",
      "Train Epoch: 79 [25600/50981 (50%)]\tLoss: 8475359.343284\n",
      "Train Epoch: 79 [26880/50981 (53%)]\tLoss: 8480171.720379\n",
      "Train Epoch: 79 [28160/50981 (55%)]\tLoss: 8481206.787330\n",
      "Train Epoch: 79 [29440/50981 (58%)]\tLoss: 8493475.411255\n",
      "Train Epoch: 79 [30720/50981 (60%)]\tLoss: 8495398.085062\n",
      "Train Epoch: 79 [32000/50981 (63%)]\tLoss: 8496080.398406\n",
      "Train Epoch: 79 [33280/50981 (65%)]\tLoss: 8503916.699234\n",
      "Train Epoch: 79 [34560/50981 (68%)]\tLoss: 8512347.079336\n",
      "Train Epoch: 79 [35840/50981 (70%)]\tLoss: 8510324.588968\n",
      "Train Epoch: 79 [37120/50981 (73%)]\tLoss: 8514777.871134\n",
      "Train Epoch: 79 [38400/50981 (75%)]\tLoss: 8514547.255814\n",
      "Train Epoch: 79 [39680/50981 (78%)]\tLoss: 8520402.815113\n",
      "Train Epoch: 79 [40960/50981 (80%)]\tLoss: 8524458.398754\n",
      "Train Epoch: 79 [42240/50981 (83%)]\tLoss: 8526328.373112\n",
      "Train Epoch: 79 [43520/50981 (85%)]\tLoss: 8523874.777126\n",
      "Train Epoch: 79 [44800/50981 (88%)]\tLoss: 8524640.901709\n",
      "Train Epoch: 79 [46080/50981 (90%)]\tLoss: 8525497.425208\n",
      "Train Epoch: 79 [47360/50981 (93%)]\tLoss: 8532839.497305\n",
      "Train Epoch: 79 [48640/50981 (95%)]\tLoss: 8543773.322835\n",
      "Train Epoch: 79 [49920/50981 (98%)]\tLoss: 8543658.029412\n",
      "Train Epoch: 80 [0/50981 (0%)]\tLoss: 8966558.000000\n",
      "Train Epoch: 80 [1280/50981 (3%)]\tLoss: 8516041.545455\n",
      "Train Epoch: 80 [2560/50981 (5%)]\tLoss: 8391703.095238\n",
      "Train Epoch: 80 [3840/50981 (8%)]\tLoss: 8363297.387097\n",
      "Train Epoch: 80 [5120/50981 (10%)]\tLoss: 8388223.170732\n",
      "Train Epoch: 80 [6400/50981 (13%)]\tLoss: 8418381.264706\n",
      "Train Epoch: 80 [7680/50981 (15%)]\tLoss: 8399209.614754\n",
      "Train Epoch: 80 [8960/50981 (18%)]\tLoss: 8409860.528169\n",
      "Train Epoch: 80 [10240/50981 (20%)]\tLoss: 8436817.598765\n",
      "Train Epoch: 80 [11520/50981 (23%)]\tLoss: 8441252.901099\n",
      "Train Epoch: 80 [12800/50981 (25%)]\tLoss: 8451719.841584\n",
      "Train Epoch: 80 [14080/50981 (28%)]\tLoss: 8461756.436937\n",
      "Train Epoch: 80 [15360/50981 (30%)]\tLoss: 8459911.144628\n",
      "Train Epoch: 80 [16640/50981 (33%)]\tLoss: 8448754.824427\n",
      "Train Epoch: 80 [17920/50981 (35%)]\tLoss: 8448761.436170\n",
      "Train Epoch: 80 [19200/50981 (38%)]\tLoss: 8445467.688742\n",
      "Train Epoch: 80 [20480/50981 (40%)]\tLoss: 8442406.133540\n",
      "Train Epoch: 80 [21760/50981 (43%)]\tLoss: 8458476.383041\n",
      "Train Epoch: 80 [23040/50981 (45%)]\tLoss: 8465976.834254\n",
      "Train Epoch: 80 [24320/50981 (48%)]\tLoss: 8465521.418848\n",
      "Train Epoch: 80 [25600/50981 (50%)]\tLoss: 8460599.472637\n",
      "Train Epoch: 80 [26880/50981 (53%)]\tLoss: 8462438.597156\n",
      "Train Epoch: 80 [28160/50981 (55%)]\tLoss: 8466992.330317\n",
      "Train Epoch: 80 [29440/50981 (58%)]\tLoss: 8467761.989177\n",
      "Train Epoch: 80 [30720/50981 (60%)]\tLoss: 8479025.881743\n",
      "Train Epoch: 80 [32000/50981 (63%)]\tLoss: 8477689.384462\n",
      "Train Epoch: 80 [33280/50981 (65%)]\tLoss: 8482648.480843\n",
      "Train Epoch: 80 [34560/50981 (68%)]\tLoss: 8488681.656827\n",
      "Train Epoch: 80 [35840/50981 (70%)]\tLoss: 8490633.845196\n",
      "Train Epoch: 80 [37120/50981 (73%)]\tLoss: 8487968.077320\n",
      "Train Epoch: 80 [38400/50981 (75%)]\tLoss: 8491670.627907\n",
      "Train Epoch: 80 [39680/50981 (78%)]\tLoss: 8487253.659164\n",
      "Train Epoch: 80 [40960/50981 (80%)]\tLoss: 8490925.884735\n",
      "Train Epoch: 80 [42240/50981 (83%)]\tLoss: 8500028.211480\n",
      "Train Epoch: 80 [43520/50981 (85%)]\tLoss: 8509778.973607\n",
      "Train Epoch: 80 [44800/50981 (88%)]\tLoss: 8511163.320513\n",
      "Train Epoch: 80 [46080/50981 (90%)]\tLoss: 8512749.704986\n",
      "Train Epoch: 80 [47360/50981 (93%)]\tLoss: 8514615.532345\n",
      "Train Epoch: 80 [48640/50981 (95%)]\tLoss: 8515187.569554\n",
      "Train Epoch: 80 [49920/50981 (98%)]\tLoss: 8515041.145780\n",
      "Train Epoch: 81 [0/50981 (0%)]\tLoss: 8404972.000000\n",
      "Train Epoch: 81 [1280/50981 (3%)]\tLoss: 8232965.863636\n",
      "Train Epoch: 81 [2560/50981 (5%)]\tLoss: 8336325.404762\n",
      "Train Epoch: 81 [3840/50981 (8%)]\tLoss: 8300138.370968\n",
      "Train Epoch: 81 [5120/50981 (10%)]\tLoss: 8351007.170732\n",
      "Train Epoch: 81 [6400/50981 (13%)]\tLoss: 8298054.686275\n",
      "Train Epoch: 81 [7680/50981 (15%)]\tLoss: 8330754.901639\n",
      "Train Epoch: 81 [8960/50981 (18%)]\tLoss: 8338964.971831\n",
      "Train Epoch: 81 [10240/50981 (20%)]\tLoss: 8361226.666667\n",
      "Train Epoch: 81 [11520/50981 (23%)]\tLoss: 8385608.313187\n",
      "Train Epoch: 81 [12800/50981 (25%)]\tLoss: 8392326.188119\n",
      "Train Epoch: 81 [14080/50981 (28%)]\tLoss: 8388965.792793\n",
      "Train Epoch: 81 [15360/50981 (30%)]\tLoss: 8392899.037190\n",
      "Train Epoch: 81 [16640/50981 (33%)]\tLoss: 8392613.580153\n",
      "Train Epoch: 81 [17920/50981 (35%)]\tLoss: 8396434.698582\n",
      "Train Epoch: 81 [19200/50981 (38%)]\tLoss: 8401697.682119\n",
      "Train Epoch: 81 [20480/50981 (40%)]\tLoss: 8402794.602484\n",
      "Train Epoch: 81 [21760/50981 (43%)]\tLoss: 8401910.315789\n",
      "Train Epoch: 81 [23040/50981 (45%)]\tLoss: 8405938.870166\n",
      "Train Epoch: 81 [24320/50981 (48%)]\tLoss: 8414985.075916\n",
      "Train Epoch: 81 [25600/50981 (50%)]\tLoss: 8412731.992537\n",
      "Train Epoch: 81 [26880/50981 (53%)]\tLoss: 8421685.281991\n",
      "Train Epoch: 81 [28160/50981 (55%)]\tLoss: 8429128.877828\n",
      "Train Epoch: 81 [29440/50981 (58%)]\tLoss: 8428642.792208\n",
      "Train Epoch: 81 [30720/50981 (60%)]\tLoss: 8439594.002075\n",
      "Train Epoch: 81 [32000/50981 (63%)]\tLoss: 8445152.400398\n",
      "Train Epoch: 81 [33280/50981 (65%)]\tLoss: 8449590.482759\n",
      "Train Epoch: 81 [34560/50981 (68%)]\tLoss: 8455486.289668\n",
      "Train Epoch: 81 [35840/50981 (70%)]\tLoss: 8456387.944840\n",
      "Train Epoch: 81 [37120/50981 (73%)]\tLoss: 8463800.568729\n",
      "Train Epoch: 81 [38400/50981 (75%)]\tLoss: 8468881.287375\n",
      "Train Epoch: 81 [39680/50981 (78%)]\tLoss: 8473824.414791\n",
      "Train Epoch: 81 [40960/50981 (80%)]\tLoss: 8476723.024922\n",
      "Train Epoch: 81 [42240/50981 (83%)]\tLoss: 8482331.942598\n",
      "Train Epoch: 81 [43520/50981 (85%)]\tLoss: 8488650.243402\n",
      "Train Epoch: 81 [44800/50981 (88%)]\tLoss: 8495502.933048\n",
      "Train Epoch: 81 [46080/50981 (90%)]\tLoss: 8505058.042936\n",
      "Train Epoch: 81 [47360/50981 (93%)]\tLoss: 8509988.305930\n",
      "Train Epoch: 81 [48640/50981 (95%)]\tLoss: 8510067.284777\n",
      "Train Epoch: 81 [49920/50981 (98%)]\tLoss: 8509989.741688\n",
      "Train Epoch: 82 [0/50981 (0%)]\tLoss: 7940589.000000\n",
      "Train Epoch: 82 [1280/50981 (3%)]\tLoss: 8453062.590909\n",
      "Train Epoch: 82 [2560/50981 (5%)]\tLoss: 8401092.547619\n",
      "Train Epoch: 82 [3840/50981 (8%)]\tLoss: 8374643.822581\n",
      "Train Epoch: 82 [5120/50981 (10%)]\tLoss: 8375267.987805\n",
      "Train Epoch: 82 [6400/50981 (13%)]\tLoss: 8406643.803922\n",
      "Train Epoch: 82 [7680/50981 (15%)]\tLoss: 8429954.934426\n",
      "Train Epoch: 82 [8960/50981 (18%)]\tLoss: 8396390.647887\n",
      "Train Epoch: 82 [10240/50981 (20%)]\tLoss: 8377592.067901\n",
      "Train Epoch: 82 [11520/50981 (23%)]\tLoss: 8375758.593407\n",
      "Train Epoch: 82 [12800/50981 (25%)]\tLoss: 8367376.143564\n",
      "Train Epoch: 82 [14080/50981 (28%)]\tLoss: 8397662.427928\n",
      "Train Epoch: 82 [15360/50981 (30%)]\tLoss: 8392116.392562\n",
      "Train Epoch: 82 [16640/50981 (33%)]\tLoss: 8383995.431298\n",
      "Train Epoch: 82 [17920/50981 (35%)]\tLoss: 8400424.648936\n",
      "Train Epoch: 82 [19200/50981 (38%)]\tLoss: 8391865.208609\n",
      "Train Epoch: 82 [20480/50981 (40%)]\tLoss: 8382686.543478\n",
      "Train Epoch: 82 [21760/50981 (43%)]\tLoss: 8397783.108187\n",
      "Train Epoch: 82 [23040/50981 (45%)]\tLoss: 8404593.720994\n",
      "Train Epoch: 82 [24320/50981 (48%)]\tLoss: 8415330.801047\n",
      "Train Epoch: 82 [25600/50981 (50%)]\tLoss: 8415871.572139\n",
      "Train Epoch: 82 [26880/50981 (53%)]\tLoss: 8428755.734597\n",
      "Train Epoch: 82 [28160/50981 (55%)]\tLoss: 8435813.140271\n",
      "Train Epoch: 82 [29440/50981 (58%)]\tLoss: 8435907.307359\n",
      "Train Epoch: 82 [30720/50981 (60%)]\tLoss: 8443199.319502\n",
      "Train Epoch: 82 [32000/50981 (63%)]\tLoss: 8450818.239044\n",
      "Train Epoch: 82 [33280/50981 (65%)]\tLoss: 8453067.237548\n",
      "Train Epoch: 82 [34560/50981 (68%)]\tLoss: 8455073.603321\n",
      "Train Epoch: 82 [35840/50981 (70%)]\tLoss: 8460236.170819\n",
      "Train Epoch: 82 [37120/50981 (73%)]\tLoss: 8460497.508591\n",
      "Train Epoch: 82 [38400/50981 (75%)]\tLoss: 8463089.757475\n",
      "Train Epoch: 82 [39680/50981 (78%)]\tLoss: 8467435.355305\n",
      "Train Epoch: 82 [40960/50981 (80%)]\tLoss: 8470217.669782\n",
      "Train Epoch: 82 [42240/50981 (83%)]\tLoss: 8473016.815710\n",
      "Train Epoch: 82 [43520/50981 (85%)]\tLoss: 8472093.923754\n",
      "Train Epoch: 82 [44800/50981 (88%)]\tLoss: 8476117.871795\n",
      "Train Epoch: 82 [46080/50981 (90%)]\tLoss: 8474437.486150\n",
      "Train Epoch: 82 [47360/50981 (93%)]\tLoss: 8475639.008086\n",
      "Train Epoch: 82 [48640/50981 (95%)]\tLoss: 8481449.223097\n",
      "Train Epoch: 82 [49920/50981 (98%)]\tLoss: 8485116.883632\n",
      "Train Epoch: 83 [0/50981 (0%)]\tLoss: 7900748.000000\n",
      "Train Epoch: 83 [1280/50981 (3%)]\tLoss: 8373375.363636\n",
      "Train Epoch: 83 [2560/50981 (5%)]\tLoss: 8328297.333333\n",
      "Train Epoch: 83 [3840/50981 (8%)]\tLoss: 8334390.290323\n",
      "Train Epoch: 83 [5120/50981 (10%)]\tLoss: 8278225.658537\n",
      "Train Epoch: 83 [6400/50981 (13%)]\tLoss: 8290520.843137\n",
      "Train Epoch: 83 [7680/50981 (15%)]\tLoss: 8312714.122951\n",
      "Train Epoch: 83 [8960/50981 (18%)]\tLoss: 8307861.007042\n",
      "Train Epoch: 83 [10240/50981 (20%)]\tLoss: 8321285.370370\n",
      "Train Epoch: 83 [11520/50981 (23%)]\tLoss: 8323256.175824\n",
      "Train Epoch: 83 [12800/50981 (25%)]\tLoss: 8321510.896040\n",
      "Train Epoch: 83 [14080/50981 (28%)]\tLoss: 8320845.909910\n",
      "Train Epoch: 83 [15360/50981 (30%)]\tLoss: 8316439.305785\n",
      "Train Epoch: 83 [16640/50981 (33%)]\tLoss: 8323093.217557\n",
      "Train Epoch: 83 [17920/50981 (35%)]\tLoss: 8334143.815603\n",
      "Train Epoch: 83 [19200/50981 (38%)]\tLoss: 8347155.682119\n",
      "Train Epoch: 83 [20480/50981 (40%)]\tLoss: 8359232.677019\n",
      "Train Epoch: 83 [21760/50981 (43%)]\tLoss: 8364773.540936\n",
      "Train Epoch: 83 [23040/50981 (45%)]\tLoss: 8380393.389503\n",
      "Train Epoch: 83 [24320/50981 (48%)]\tLoss: 8383339.280105\n",
      "Train Epoch: 83 [25600/50981 (50%)]\tLoss: 8396072.355721\n",
      "Train Epoch: 83 [26880/50981 (53%)]\tLoss: 8402604.732227\n",
      "Train Epoch: 83 [28160/50981 (55%)]\tLoss: 8412628.977376\n",
      "Train Epoch: 83 [29440/50981 (58%)]\tLoss: 8415764.071429\n",
      "Train Epoch: 83 [30720/50981 (60%)]\tLoss: 8418857.377593\n",
      "Train Epoch: 83 [32000/50981 (63%)]\tLoss: 8420506.298805\n",
      "Train Epoch: 83 [33280/50981 (65%)]\tLoss: 8429348.969349\n",
      "Train Epoch: 83 [34560/50981 (68%)]\tLoss: 8432315.188192\n",
      "Train Epoch: 83 [35840/50981 (70%)]\tLoss: 8427137.975089\n",
      "Train Epoch: 83 [37120/50981 (73%)]\tLoss: 8430519.871134\n",
      "Train Epoch: 83 [38400/50981 (75%)]\tLoss: 8438837.840532\n",
      "Train Epoch: 83 [39680/50981 (78%)]\tLoss: 8446111.282958\n",
      "Train Epoch: 83 [40960/50981 (80%)]\tLoss: 8450269.915888\n",
      "Train Epoch: 83 [42240/50981 (83%)]\tLoss: 8451253.288520\n",
      "Train Epoch: 83 [43520/50981 (85%)]\tLoss: 8451330.416422\n",
      "Train Epoch: 83 [44800/50981 (88%)]\tLoss: 8455282.088319\n",
      "Train Epoch: 83 [46080/50981 (90%)]\tLoss: 8464263.900277\n",
      "Train Epoch: 83 [47360/50981 (93%)]\tLoss: 8465271.845013\n",
      "Train Epoch: 83 [48640/50981 (95%)]\tLoss: 8467351.003937\n",
      "Train Epoch: 83 [49920/50981 (98%)]\tLoss: 8464519.474425\n",
      "Train Epoch: 84 [0/50981 (0%)]\tLoss: 8017728.000000\n",
      "Train Epoch: 84 [1280/50981 (3%)]\tLoss: 8376271.909091\n",
      "Train Epoch: 84 [2560/50981 (5%)]\tLoss: 8329967.738095\n",
      "Train Epoch: 84 [3840/50981 (8%)]\tLoss: 8364201.080645\n",
      "Train Epoch: 84 [5120/50981 (10%)]\tLoss: 8354504.268293\n",
      "Train Epoch: 84 [6400/50981 (13%)]\tLoss: 8346578.254902\n",
      "Train Epoch: 84 [7680/50981 (15%)]\tLoss: 8363702.942623\n",
      "Train Epoch: 84 [8960/50981 (18%)]\tLoss: 8343681.478873\n",
      "Train Epoch: 84 [10240/50981 (20%)]\tLoss: 8349310.648148\n",
      "Train Epoch: 84 [11520/50981 (23%)]\tLoss: 8329031.098901\n",
      "Train Epoch: 84 [12800/50981 (25%)]\tLoss: 8326055.490099\n",
      "Train Epoch: 84 [14080/50981 (28%)]\tLoss: 8342819.783784\n",
      "Train Epoch: 84 [15360/50981 (30%)]\tLoss: 8354451.479339\n",
      "Train Epoch: 84 [16640/50981 (33%)]\tLoss: 8356928.973282\n",
      "Train Epoch: 84 [17920/50981 (35%)]\tLoss: 8358360.638298\n",
      "Train Epoch: 84 [19200/50981 (38%)]\tLoss: 8359206.586093\n",
      "Train Epoch: 84 [20480/50981 (40%)]\tLoss: 8351117.096273\n",
      "Train Epoch: 84 [21760/50981 (43%)]\tLoss: 8350317.596491\n",
      "Train Epoch: 84 [23040/50981 (45%)]\tLoss: 8356819.930939\n",
      "Train Epoch: 84 [24320/50981 (48%)]\tLoss: 8360116.471204\n",
      "Train Epoch: 84 [25600/50981 (50%)]\tLoss: 8359817.131841\n",
      "Train Epoch: 84 [26880/50981 (53%)]\tLoss: 8363204.710900\n",
      "Train Epoch: 84 [28160/50981 (55%)]\tLoss: 8367700.538462\n",
      "Train Epoch: 84 [29440/50981 (58%)]\tLoss: 8369781.504329\n",
      "Train Epoch: 84 [30720/50981 (60%)]\tLoss: 8379396.807054\n",
      "Train Epoch: 84 [32000/50981 (63%)]\tLoss: 8379232.049801\n",
      "Train Epoch: 84 [33280/50981 (65%)]\tLoss: 8372582.678161\n",
      "Train Epoch: 84 [34560/50981 (68%)]\tLoss: 8379232.107011\n",
      "Train Epoch: 84 [35840/50981 (70%)]\tLoss: 8381158.740214\n",
      "Train Epoch: 84 [37120/50981 (73%)]\tLoss: 8389090.424399\n",
      "Train Epoch: 84 [38400/50981 (75%)]\tLoss: 8392781.038206\n",
      "Train Epoch: 84 [39680/50981 (78%)]\tLoss: 8407915.652733\n",
      "Train Epoch: 84 [40960/50981 (80%)]\tLoss: 8404349.249221\n",
      "Train Epoch: 84 [42240/50981 (83%)]\tLoss: 8410730.327795\n",
      "Train Epoch: 84 [43520/50981 (85%)]\tLoss: 8417691.887097\n",
      "Train Epoch: 84 [44800/50981 (88%)]\tLoss: 8416523.854701\n",
      "Train Epoch: 84 [46080/50981 (90%)]\tLoss: 8423990.317175\n",
      "Train Epoch: 84 [47360/50981 (93%)]\tLoss: 8432020.273585\n",
      "Train Epoch: 84 [48640/50981 (95%)]\tLoss: 8432976.484252\n",
      "Train Epoch: 84 [49920/50981 (98%)]\tLoss: 8439066.297954\n",
      "Train Epoch: 85 [0/50981 (0%)]\tLoss: 7975416.000000\n",
      "Train Epoch: 85 [1280/50981 (3%)]\tLoss: 8182197.000000\n",
      "Train Epoch: 85 [2560/50981 (5%)]\tLoss: 8205757.690476\n",
      "Train Epoch: 85 [3840/50981 (8%)]\tLoss: 8210105.629032\n",
      "Train Epoch: 85 [5120/50981 (10%)]\tLoss: 8249257.000000\n",
      "Train Epoch: 85 [6400/50981 (13%)]\tLoss: 8247789.852941\n",
      "Train Epoch: 85 [7680/50981 (15%)]\tLoss: 8250775.008197\n",
      "Train Epoch: 85 [8960/50981 (18%)]\tLoss: 8255535.042254\n",
      "Train Epoch: 85 [10240/50981 (20%)]\tLoss: 8270315.006173\n",
      "Train Epoch: 85 [11520/50981 (23%)]\tLoss: 8287601.258242\n",
      "Train Epoch: 85 [12800/50981 (25%)]\tLoss: 8280713.905941\n",
      "Train Epoch: 85 [14080/50981 (28%)]\tLoss: 8292563.630631\n",
      "Train Epoch: 85 [15360/50981 (30%)]\tLoss: 8290385.809917\n",
      "Train Epoch: 85 [16640/50981 (33%)]\tLoss: 8290752.343511\n",
      "Train Epoch: 85 [17920/50981 (35%)]\tLoss: 8290667.411348\n",
      "Train Epoch: 85 [19200/50981 (38%)]\tLoss: 8305167.605960\n",
      "Train Epoch: 85 [20480/50981 (40%)]\tLoss: 8315558.481366\n",
      "Train Epoch: 85 [21760/50981 (43%)]\tLoss: 8320938.441520\n",
      "Train Epoch: 85 [23040/50981 (45%)]\tLoss: 8325603.058011\n",
      "Train Epoch: 85 [24320/50981 (48%)]\tLoss: 8338840.191099\n",
      "Train Epoch: 85 [25600/50981 (50%)]\tLoss: 8333976.957711\n",
      "Train Epoch: 85 [26880/50981 (53%)]\tLoss: 8344591.315166\n",
      "Train Epoch: 85 [28160/50981 (55%)]\tLoss: 8355325.036199\n",
      "Train Epoch: 85 [29440/50981 (58%)]\tLoss: 8361814.811688\n",
      "Train Epoch: 85 [30720/50981 (60%)]\tLoss: 8366735.977178\n",
      "Train Epoch: 85 [32000/50981 (63%)]\tLoss: 8370861.171315\n",
      "Train Epoch: 85 [33280/50981 (65%)]\tLoss: 8380329.421456\n",
      "Train Epoch: 85 [34560/50981 (68%)]\tLoss: 8389372.103321\n",
      "Train Epoch: 85 [35840/50981 (70%)]\tLoss: 8394152.455516\n",
      "Train Epoch: 85 [37120/50981 (73%)]\tLoss: 8397438.237113\n",
      "Train Epoch: 85 [38400/50981 (75%)]\tLoss: 8397048.074751\n",
      "Train Epoch: 85 [39680/50981 (78%)]\tLoss: 8405096.712219\n",
      "Train Epoch: 85 [40960/50981 (80%)]\tLoss: 8404667.253894\n",
      "Train Epoch: 85 [42240/50981 (83%)]\tLoss: 8407781.287009\n",
      "Train Epoch: 85 [43520/50981 (85%)]\tLoss: 8408532.876833\n",
      "Train Epoch: 85 [44800/50981 (88%)]\tLoss: 8412352.739316\n",
      "Train Epoch: 85 [46080/50981 (90%)]\tLoss: 8415151.412742\n",
      "Train Epoch: 85 [47360/50981 (93%)]\tLoss: 8417391.652291\n",
      "Train Epoch: 85 [48640/50981 (95%)]\tLoss: 8418742.076115\n",
      "Train Epoch: 85 [49920/50981 (98%)]\tLoss: 8421091.865729\n",
      "Train Epoch: 86 [0/50981 (0%)]\tLoss: 8320946.000000\n",
      "Train Epoch: 86 [1280/50981 (3%)]\tLoss: 8466707.409091\n",
      "Train Epoch: 86 [2560/50981 (5%)]\tLoss: 8337953.571429\n",
      "Train Epoch: 86 [3840/50981 (8%)]\tLoss: 8275179.838710\n",
      "Train Epoch: 86 [5120/50981 (10%)]\tLoss: 8271243.939024\n",
      "Train Epoch: 86 [6400/50981 (13%)]\tLoss: 8256042.735294\n",
      "Train Epoch: 86 [7680/50981 (15%)]\tLoss: 8250552.172131\n",
      "Train Epoch: 86 [8960/50981 (18%)]\tLoss: 8266606.718310\n",
      "Train Epoch: 86 [10240/50981 (20%)]\tLoss: 8255140.438272\n",
      "Train Epoch: 86 [11520/50981 (23%)]\tLoss: 8263619.296703\n",
      "Train Epoch: 86 [12800/50981 (25%)]\tLoss: 8277963.356436\n",
      "Train Epoch: 86 [14080/50981 (28%)]\tLoss: 8289591.283784\n",
      "Train Epoch: 86 [15360/50981 (30%)]\tLoss: 8279875.524793\n",
      "Train Epoch: 86 [16640/50981 (33%)]\tLoss: 8296112.572519\n",
      "Train Epoch: 86 [17920/50981 (35%)]\tLoss: 8318800.141844\n",
      "Train Epoch: 86 [19200/50981 (38%)]\tLoss: 8319213.692053\n",
      "Train Epoch: 86 [20480/50981 (40%)]\tLoss: 8325653.645963\n",
      "Train Epoch: 86 [21760/50981 (43%)]\tLoss: 8319059.581871\n",
      "Train Epoch: 86 [23040/50981 (45%)]\tLoss: 8322238.301105\n",
      "Train Epoch: 86 [24320/50981 (48%)]\tLoss: 8330325.403141\n",
      "Train Epoch: 86 [25600/50981 (50%)]\tLoss: 8338846.077114\n",
      "Train Epoch: 86 [26880/50981 (53%)]\tLoss: 8340948.601896\n",
      "Train Epoch: 86 [28160/50981 (55%)]\tLoss: 8341745.427602\n",
      "Train Epoch: 86 [29440/50981 (58%)]\tLoss: 8340887.896104\n",
      "Train Epoch: 86 [30720/50981 (60%)]\tLoss: 8339987.549793\n",
      "Train Epoch: 86 [32000/50981 (63%)]\tLoss: 8346955.737052\n",
      "Train Epoch: 86 [33280/50981 (65%)]\tLoss: 8350313.007663\n",
      "Train Epoch: 86 [34560/50981 (68%)]\tLoss: 8354930.265683\n",
      "Train Epoch: 86 [35840/50981 (70%)]\tLoss: 8365667.409253\n",
      "Train Epoch: 86 [37120/50981 (73%)]\tLoss: 8371909.713058\n",
      "Train Epoch: 86 [38400/50981 (75%)]\tLoss: 8376155.795681\n",
      "Train Epoch: 86 [39680/50981 (78%)]\tLoss: 8372588.000000\n",
      "Train Epoch: 86 [40960/50981 (80%)]\tLoss: 8377146.059190\n",
      "Train Epoch: 86 [42240/50981 (83%)]\tLoss: 8375209.688822\n",
      "Train Epoch: 86 [43520/50981 (85%)]\tLoss: 8379391.587977\n",
      "Train Epoch: 86 [44800/50981 (88%)]\tLoss: 8383700.582621\n",
      "Train Epoch: 86 [46080/50981 (90%)]\tLoss: 8386548.788089\n",
      "Train Epoch: 86 [47360/50981 (93%)]\tLoss: 8391961.893531\n",
      "Train Epoch: 86 [48640/50981 (95%)]\tLoss: 8392141.615486\n",
      "Train Epoch: 86 [49920/50981 (98%)]\tLoss: 8392361.173913\n",
      "Train Epoch: 87 [0/50981 (0%)]\tLoss: 8431634.000000\n",
      "Train Epoch: 87 [1280/50981 (3%)]\tLoss: 8075786.727273\n",
      "Train Epoch: 87 [2560/50981 (5%)]\tLoss: 8119686.428571\n",
      "Train Epoch: 87 [3840/50981 (8%)]\tLoss: 8157680.435484\n",
      "Train Epoch: 87 [5120/50981 (10%)]\tLoss: 8186622.280488\n",
      "Train Epoch: 87 [6400/50981 (13%)]\tLoss: 8191260.666667\n",
      "Train Epoch: 87 [7680/50981 (15%)]\tLoss: 8196203.065574\n",
      "Train Epoch: 87 [8960/50981 (18%)]\tLoss: 8219630.760563\n",
      "Train Epoch: 87 [10240/50981 (20%)]\tLoss: 8235220.425926\n",
      "Train Epoch: 87 [11520/50981 (23%)]\tLoss: 8256334.884615\n",
      "Train Epoch: 87 [12800/50981 (25%)]\tLoss: 8257873.623762\n",
      "Train Epoch: 87 [14080/50981 (28%)]\tLoss: 8260323.072072\n",
      "Train Epoch: 87 [15360/50981 (30%)]\tLoss: 8274533.702479\n",
      "Train Epoch: 87 [16640/50981 (33%)]\tLoss: 8284142.377863\n",
      "Train Epoch: 87 [17920/50981 (35%)]\tLoss: 8297444.500000\n",
      "Train Epoch: 87 [19200/50981 (38%)]\tLoss: 8302238.781457\n",
      "Train Epoch: 87 [20480/50981 (40%)]\tLoss: 8300023.295031\n",
      "Train Epoch: 87 [21760/50981 (43%)]\tLoss: 8292255.198830\n",
      "Train Epoch: 87 [23040/50981 (45%)]\tLoss: 8294677.933702\n",
      "Train Epoch: 87 [24320/50981 (48%)]\tLoss: 8302008.672775\n",
      "Train Epoch: 87 [25600/50981 (50%)]\tLoss: 8302484.246269\n",
      "Train Epoch: 87 [26880/50981 (53%)]\tLoss: 8320778.327014\n",
      "Train Epoch: 87 [28160/50981 (55%)]\tLoss: 8319135.176471\n",
      "Train Epoch: 87 [29440/50981 (58%)]\tLoss: 8319920.915584\n",
      "Train Epoch: 87 [30720/50981 (60%)]\tLoss: 8329679.960581\n",
      "Train Epoch: 87 [32000/50981 (63%)]\tLoss: 8329598.282869\n",
      "Train Epoch: 87 [33280/50981 (65%)]\tLoss: 8332289.965517\n",
      "Train Epoch: 87 [34560/50981 (68%)]\tLoss: 8336241.123616\n",
      "Train Epoch: 87 [35840/50981 (70%)]\tLoss: 8346589.161922\n",
      "Train Epoch: 87 [37120/50981 (73%)]\tLoss: 8352124.408935\n",
      "Train Epoch: 87 [38400/50981 (75%)]\tLoss: 8351467.279070\n",
      "Train Epoch: 87 [39680/50981 (78%)]\tLoss: 8353451.429260\n",
      "Train Epoch: 87 [40960/50981 (80%)]\tLoss: 8352144.077882\n",
      "Train Epoch: 87 [42240/50981 (83%)]\tLoss: 8357319.821752\n",
      "Train Epoch: 87 [43520/50981 (85%)]\tLoss: 8362313.545455\n",
      "Train Epoch: 87 [44800/50981 (88%)]\tLoss: 8362986.323362\n",
      "Train Epoch: 87 [46080/50981 (90%)]\tLoss: 8370207.289474\n",
      "Train Epoch: 87 [47360/50981 (93%)]\tLoss: 8370572.066038\n",
      "Train Epoch: 87 [48640/50981 (95%)]\tLoss: 8374290.954068\n",
      "Train Epoch: 87 [49920/50981 (98%)]\tLoss: 8377458.820972\n",
      "Train Epoch: 88 [0/50981 (0%)]\tLoss: 8199273.000000\n",
      "Train Epoch: 88 [1280/50981 (3%)]\tLoss: 8249622.090909\n",
      "Train Epoch: 88 [2560/50981 (5%)]\tLoss: 8170884.238095\n",
      "Train Epoch: 88 [3840/50981 (8%)]\tLoss: 8228647.774194\n",
      "Train Epoch: 88 [5120/50981 (10%)]\tLoss: 8259184.292683\n",
      "Train Epoch: 88 [6400/50981 (13%)]\tLoss: 8264162.107843\n",
      "Train Epoch: 88 [7680/50981 (15%)]\tLoss: 8283365.926230\n",
      "Train Epoch: 88 [8960/50981 (18%)]\tLoss: 8297937.070423\n",
      "Train Epoch: 88 [10240/50981 (20%)]\tLoss: 8283683.061728\n",
      "Train Epoch: 88 [11520/50981 (23%)]\tLoss: 8279147.824176\n",
      "Train Epoch: 88 [12800/50981 (25%)]\tLoss: 8293529.514851\n",
      "Train Epoch: 88 [14080/50981 (28%)]\tLoss: 8282289.400901\n",
      "Train Epoch: 88 [15360/50981 (30%)]\tLoss: 8297313.165289\n",
      "Train Epoch: 88 [16640/50981 (33%)]\tLoss: 8315148.599237\n",
      "Train Epoch: 88 [17920/50981 (35%)]\tLoss: 8309207.060284\n",
      "Train Epoch: 88 [19200/50981 (38%)]\tLoss: 8308407.745033\n",
      "Train Epoch: 88 [20480/50981 (40%)]\tLoss: 8309509.822981\n",
      "Train Epoch: 88 [21760/50981 (43%)]\tLoss: 8301442.116959\n",
      "Train Epoch: 88 [23040/50981 (45%)]\tLoss: 8306389.069061\n",
      "Train Epoch: 88 [24320/50981 (48%)]\tLoss: 8311905.267016\n",
      "Train Epoch: 88 [25600/50981 (50%)]\tLoss: 8315350.320896\n",
      "Train Epoch: 88 [26880/50981 (53%)]\tLoss: 8319998.142180\n",
      "Train Epoch: 88 [28160/50981 (55%)]\tLoss: 8320157.024887\n",
      "Train Epoch: 88 [29440/50981 (58%)]\tLoss: 8322164.331169\n",
      "Train Epoch: 88 [30720/50981 (60%)]\tLoss: 8323682.890041\n",
      "Train Epoch: 88 [32000/50981 (63%)]\tLoss: 8329253.505976\n",
      "Train Epoch: 88 [33280/50981 (65%)]\tLoss: 8334733.528736\n",
      "Train Epoch: 88 [34560/50981 (68%)]\tLoss: 8338456.431734\n",
      "Train Epoch: 88 [35840/50981 (70%)]\tLoss: 8346776.236655\n",
      "Train Epoch: 88 [37120/50981 (73%)]\tLoss: 8352723.369416\n",
      "Train Epoch: 88 [38400/50981 (75%)]\tLoss: 8351873.770764\n",
      "Train Epoch: 88 [39680/50981 (78%)]\tLoss: 8356375.503215\n",
      "Train Epoch: 88 [40960/50981 (80%)]\tLoss: 8357146.521807\n",
      "Train Epoch: 88 [42240/50981 (83%)]\tLoss: 8358125.996979\n",
      "Train Epoch: 88 [43520/50981 (85%)]\tLoss: 8358694.923754\n",
      "Train Epoch: 88 [44800/50981 (88%)]\tLoss: 8354748.178063\n",
      "Train Epoch: 88 [46080/50981 (90%)]\tLoss: 8356850.524931\n",
      "Train Epoch: 88 [47360/50981 (93%)]\tLoss: 8361156.287062\n",
      "Train Epoch: 88 [48640/50981 (95%)]\tLoss: 8365661.727034\n",
      "Train Epoch: 88 [49920/50981 (98%)]\tLoss: 8367376.579284\n",
      "Train Epoch: 89 [0/50981 (0%)]\tLoss: 8038246.000000\n",
      "Train Epoch: 89 [1280/50981 (3%)]\tLoss: 8081613.636364\n",
      "Train Epoch: 89 [2560/50981 (5%)]\tLoss: 8134836.357143\n",
      "Train Epoch: 89 [3840/50981 (8%)]\tLoss: 8165870.177419\n",
      "Train Epoch: 89 [5120/50981 (10%)]\tLoss: 8160653.060976\n",
      "Train Epoch: 89 [6400/50981 (13%)]\tLoss: 8214676.176471\n",
      "Train Epoch: 89 [7680/50981 (15%)]\tLoss: 8259912.122951\n",
      "Train Epoch: 89 [8960/50981 (18%)]\tLoss: 8259216.063380\n",
      "Train Epoch: 89 [10240/50981 (20%)]\tLoss: 8242663.283951\n",
      "Train Epoch: 89 [11520/50981 (23%)]\tLoss: 8250400.791209\n",
      "Train Epoch: 89 [12800/50981 (25%)]\tLoss: 8244728.915842\n",
      "Train Epoch: 89 [14080/50981 (28%)]\tLoss: 8251207.558559\n",
      "Train Epoch: 89 [15360/50981 (30%)]\tLoss: 8253532.780992\n",
      "Train Epoch: 89 [16640/50981 (33%)]\tLoss: 8259321.129771\n",
      "Train Epoch: 89 [17920/50981 (35%)]\tLoss: 8273486.815603\n",
      "Train Epoch: 89 [19200/50981 (38%)]\tLoss: 8280546.245033\n",
      "Train Epoch: 89 [20480/50981 (40%)]\tLoss: 8286342.751553\n",
      "Train Epoch: 89 [21760/50981 (43%)]\tLoss: 8271726.242690\n",
      "Train Epoch: 89 [23040/50981 (45%)]\tLoss: 8274677.176796\n",
      "Train Epoch: 89 [24320/50981 (48%)]\tLoss: 8279676.439791\n",
      "Train Epoch: 89 [25600/50981 (50%)]\tLoss: 8273827.940299\n",
      "Train Epoch: 89 [26880/50981 (53%)]\tLoss: 8280897.381517\n",
      "Train Epoch: 89 [28160/50981 (55%)]\tLoss: 8287867.251131\n",
      "Train Epoch: 89 [29440/50981 (58%)]\tLoss: 8288610.484848\n",
      "Train Epoch: 89 [30720/50981 (60%)]\tLoss: 8284679.943983\n",
      "Train Epoch: 89 [32000/50981 (63%)]\tLoss: 8284819.320717\n",
      "Train Epoch: 89 [33280/50981 (65%)]\tLoss: 8293063.431034\n",
      "Train Epoch: 89 [34560/50981 (68%)]\tLoss: 8296119.369004\n",
      "Train Epoch: 89 [35840/50981 (70%)]\tLoss: 8301322.583630\n",
      "Train Epoch: 89 [37120/50981 (73%)]\tLoss: 8306430.166667\n",
      "Train Epoch: 89 [38400/50981 (75%)]\tLoss: 8309744.692691\n",
      "Train Epoch: 89 [39680/50981 (78%)]\tLoss: 8315296.583601\n",
      "Train Epoch: 89 [40960/50981 (80%)]\tLoss: 8321402.998442\n",
      "Train Epoch: 89 [42240/50981 (83%)]\tLoss: 8328048.471299\n",
      "Train Epoch: 89 [43520/50981 (85%)]\tLoss: 8337304.565982\n",
      "Train Epoch: 89 [44800/50981 (88%)]\tLoss: 8331808.923077\n",
      "Train Epoch: 89 [46080/50981 (90%)]\tLoss: 8334968.682825\n",
      "Train Epoch: 89 [47360/50981 (93%)]\tLoss: 8334996.583558\n",
      "Train Epoch: 89 [48640/50981 (95%)]\tLoss: 8340194.531496\n",
      "Train Epoch: 89 [49920/50981 (98%)]\tLoss: 8345195.381074\n",
      "Train Epoch: 90 [0/50981 (0%)]\tLoss: 7835333.000000\n",
      "Train Epoch: 90 [1280/50981 (3%)]\tLoss: 8055044.590909\n",
      "Train Epoch: 90 [2560/50981 (5%)]\tLoss: 8199180.976190\n",
      "Train Epoch: 90 [3840/50981 (8%)]\tLoss: 8227460.822581\n",
      "Train Epoch: 90 [5120/50981 (10%)]\tLoss: 8209445.158537\n",
      "Train Epoch: 90 [6400/50981 (13%)]\tLoss: 8189420.627451\n",
      "Train Epoch: 90 [7680/50981 (15%)]\tLoss: 8191608.475410\n",
      "Train Epoch: 90 [8960/50981 (18%)]\tLoss: 8184800.232394\n",
      "Train Epoch: 90 [10240/50981 (20%)]\tLoss: 8192497.586420\n",
      "Train Epoch: 90 [11520/50981 (23%)]\tLoss: 8181512.994505\n",
      "Train Epoch: 90 [12800/50981 (25%)]\tLoss: 8187257.569307\n",
      "Train Epoch: 90 [14080/50981 (28%)]\tLoss: 8192766.220721\n",
      "Train Epoch: 90 [15360/50981 (30%)]\tLoss: 8196151.739669\n",
      "Train Epoch: 90 [16640/50981 (33%)]\tLoss: 8208964.103053\n",
      "Train Epoch: 90 [17920/50981 (35%)]\tLoss: 8221722.262411\n",
      "Train Epoch: 90 [19200/50981 (38%)]\tLoss: 8232603.963576\n",
      "Train Epoch: 90 [20480/50981 (40%)]\tLoss: 8233388.621118\n",
      "Train Epoch: 90 [21760/50981 (43%)]\tLoss: 8237076.342105\n",
      "Train Epoch: 90 [23040/50981 (45%)]\tLoss: 8260781.334254\n",
      "Train Epoch: 90 [24320/50981 (48%)]\tLoss: 8268131.623037\n",
      "Train Epoch: 90 [25600/50981 (50%)]\tLoss: 8281030.271144\n",
      "Train Epoch: 90 [26880/50981 (53%)]\tLoss: 8292886.753555\n",
      "Train Epoch: 90 [28160/50981 (55%)]\tLoss: 8299111.339367\n",
      "Train Epoch: 90 [29440/50981 (58%)]\tLoss: 8297156.891775\n",
      "Train Epoch: 90 [30720/50981 (60%)]\tLoss: 8294485.917012\n",
      "Train Epoch: 90 [32000/50981 (63%)]\tLoss: 8294307.697211\n",
      "Train Epoch: 90 [33280/50981 (65%)]\tLoss: 8290847.329502\n",
      "Train Epoch: 90 [34560/50981 (68%)]\tLoss: 8292144.300738\n",
      "Train Epoch: 90 [35840/50981 (70%)]\tLoss: 8296045.425267\n",
      "Train Epoch: 90 [37120/50981 (73%)]\tLoss: 8297719.919244\n",
      "Train Epoch: 90 [38400/50981 (75%)]\tLoss: 8294589.606312\n",
      "Train Epoch: 90 [39680/50981 (78%)]\tLoss: 8303124.165595\n",
      "Train Epoch: 90 [40960/50981 (80%)]\tLoss: 8304090.228972\n",
      "Train Epoch: 90 [42240/50981 (83%)]\tLoss: 8307175.241692\n",
      "Train Epoch: 90 [43520/50981 (85%)]\tLoss: 8311758.954545\n",
      "Train Epoch: 90 [44800/50981 (88%)]\tLoss: 8315075.537037\n",
      "Train Epoch: 90 [46080/50981 (90%)]\tLoss: 8321605.704986\n",
      "Train Epoch: 90 [47360/50981 (93%)]\tLoss: 8325702.099730\n",
      "Train Epoch: 90 [48640/50981 (95%)]\tLoss: 8321799.971129\n",
      "Train Epoch: 90 [49920/50981 (98%)]\tLoss: 8322824.360614\n",
      "Train Epoch: 91 [0/50981 (0%)]\tLoss: 8242402.500000\n",
      "Train Epoch: 91 [1280/50981 (3%)]\tLoss: 8230974.500000\n",
      "Train Epoch: 91 [2560/50981 (5%)]\tLoss: 8241888.880952\n",
      "Train Epoch: 91 [3840/50981 (8%)]\tLoss: 8188061.596774\n",
      "Train Epoch: 91 [5120/50981 (10%)]\tLoss: 8199487.060976\n",
      "Train Epoch: 91 [6400/50981 (13%)]\tLoss: 8192770.245098\n",
      "Train Epoch: 91 [7680/50981 (15%)]\tLoss: 8191772.491803\n",
      "Train Epoch: 91 [8960/50981 (18%)]\tLoss: 8206821.309859\n",
      "Train Epoch: 91 [10240/50981 (20%)]\tLoss: 8235844.938272\n",
      "Train Epoch: 91 [11520/50981 (23%)]\tLoss: 8226812.219780\n",
      "Train Epoch: 91 [12800/50981 (25%)]\tLoss: 8219256.866337\n",
      "Train Epoch: 91 [14080/50981 (28%)]\tLoss: 8209592.873874\n",
      "Train Epoch: 91 [15360/50981 (30%)]\tLoss: 8204597.326446\n",
      "Train Epoch: 91 [16640/50981 (33%)]\tLoss: 8218640.469466\n",
      "Train Epoch: 91 [17920/50981 (35%)]\tLoss: 8224595.964539\n",
      "Train Epoch: 91 [19200/50981 (38%)]\tLoss: 8235211.814570\n",
      "Train Epoch: 91 [20480/50981 (40%)]\tLoss: 8230991.891304\n",
      "Train Epoch: 91 [21760/50981 (43%)]\tLoss: 8235534.409357\n",
      "Train Epoch: 91 [23040/50981 (45%)]\tLoss: 8231633.091160\n",
      "Train Epoch: 91 [24320/50981 (48%)]\tLoss: 8244620.578534\n",
      "Train Epoch: 91 [25600/50981 (50%)]\tLoss: 8243276.629353\n",
      "Train Epoch: 91 [26880/50981 (53%)]\tLoss: 8251874.760664\n",
      "Train Epoch: 91 [28160/50981 (55%)]\tLoss: 8258345.979638\n",
      "Train Epoch: 91 [29440/50981 (58%)]\tLoss: 8262874.796537\n",
      "Train Epoch: 91 [30720/50981 (60%)]\tLoss: 8269742.296680\n",
      "Train Epoch: 91 [32000/50981 (63%)]\tLoss: 8270601.984064\n",
      "Train Epoch: 91 [33280/50981 (65%)]\tLoss: 8270545.977011\n",
      "Train Epoch: 91 [34560/50981 (68%)]\tLoss: 8270532.311808\n",
      "Train Epoch: 91 [35840/50981 (70%)]\tLoss: 8273363.080071\n",
      "Train Epoch: 91 [37120/50981 (73%)]\tLoss: 8269296.620275\n",
      "Train Epoch: 91 [38400/50981 (75%)]\tLoss: 8269641.079734\n",
      "Train Epoch: 91 [39680/50981 (78%)]\tLoss: 8275761.445338\n",
      "Train Epoch: 91 [40960/50981 (80%)]\tLoss: 8280599.585670\n",
      "Train Epoch: 91 [42240/50981 (83%)]\tLoss: 8283528.256798\n",
      "Train Epoch: 91 [43520/50981 (85%)]\tLoss: 8282898.208211\n",
      "Train Epoch: 91 [44800/50981 (88%)]\tLoss: 8289985.270655\n",
      "Train Epoch: 91 [46080/50981 (90%)]\tLoss: 8290756.667590\n",
      "Train Epoch: 91 [47360/50981 (93%)]\tLoss: 8297638.642857\n",
      "Train Epoch: 91 [48640/50981 (95%)]\tLoss: 8298514.860892\n",
      "Train Epoch: 91 [49920/50981 (98%)]\tLoss: 8301054.461637\n",
      "Train Epoch: 92 [0/50981 (0%)]\tLoss: 7879248.000000\n",
      "Train Epoch: 92 [1280/50981 (3%)]\tLoss: 8057449.454545\n",
      "Train Epoch: 92 [2560/50981 (5%)]\tLoss: 8026049.166667\n",
      "Train Epoch: 92 [3840/50981 (8%)]\tLoss: 8080887.112903\n",
      "Train Epoch: 92 [5120/50981 (10%)]\tLoss: 8055337.317073\n",
      "Train Epoch: 92 [6400/50981 (13%)]\tLoss: 8086176.362745\n",
      "Train Epoch: 92 [7680/50981 (15%)]\tLoss: 8104864.909836\n",
      "Train Epoch: 92 [8960/50981 (18%)]\tLoss: 8133859.464789\n",
      "Train Epoch: 92 [10240/50981 (20%)]\tLoss: 8135234.401235\n",
      "Train Epoch: 92 [11520/50981 (23%)]\tLoss: 8127248.725275\n",
      "Train Epoch: 92 [12800/50981 (25%)]\tLoss: 8134760.900990\n",
      "Train Epoch: 92 [14080/50981 (28%)]\tLoss: 8147763.198198\n",
      "Train Epoch: 92 [15360/50981 (30%)]\tLoss: 8156664.826446\n",
      "Train Epoch: 92 [16640/50981 (33%)]\tLoss: 8151788.854962\n",
      "Train Epoch: 92 [17920/50981 (35%)]\tLoss: 8169385.244681\n",
      "Train Epoch: 92 [19200/50981 (38%)]\tLoss: 8173089.745033\n",
      "Train Epoch: 92 [20480/50981 (40%)]\tLoss: 8181531.232919\n",
      "Train Epoch: 92 [21760/50981 (43%)]\tLoss: 8197139.432749\n",
      "Train Epoch: 92 [23040/50981 (45%)]\tLoss: 8202277.784530\n",
      "Train Epoch: 92 [24320/50981 (48%)]\tLoss: 8206552.382199\n",
      "Train Epoch: 92 [25600/50981 (50%)]\tLoss: 8210963.708955\n",
      "Train Epoch: 92 [26880/50981 (53%)]\tLoss: 8213924.514218\n",
      "Train Epoch: 92 [28160/50981 (55%)]\tLoss: 8214780.414027\n",
      "Train Epoch: 92 [29440/50981 (58%)]\tLoss: 8215109.634199\n",
      "Train Epoch: 92 [30720/50981 (60%)]\tLoss: 8219317.896266\n",
      "Train Epoch: 92 [32000/50981 (63%)]\tLoss: 8222183.721116\n",
      "Train Epoch: 92 [33280/50981 (65%)]\tLoss: 8233127.911877\n",
      "Train Epoch: 92 [34560/50981 (68%)]\tLoss: 8238433.448339\n",
      "Train Epoch: 92 [35840/50981 (70%)]\tLoss: 8239109.377224\n",
      "Train Epoch: 92 [37120/50981 (73%)]\tLoss: 8245066.383162\n",
      "Train Epoch: 92 [38400/50981 (75%)]\tLoss: 8254755.669435\n",
      "Train Epoch: 92 [39680/50981 (78%)]\tLoss: 8257487.700965\n",
      "Train Epoch: 92 [40960/50981 (80%)]\tLoss: 8266290.038941\n",
      "Train Epoch: 92 [42240/50981 (83%)]\tLoss: 8271289.084592\n",
      "Train Epoch: 92 [43520/50981 (85%)]\tLoss: 8270753.461877\n",
      "Train Epoch: 92 [44800/50981 (88%)]\tLoss: 8271022.065527\n",
      "Train Epoch: 92 [46080/50981 (90%)]\tLoss: 8278959.918283\n",
      "Train Epoch: 92 [47360/50981 (93%)]\tLoss: 8283867.029650\n",
      "Train Epoch: 92 [48640/50981 (95%)]\tLoss: 8292774.738845\n",
      "Train Epoch: 92 [49920/50981 (98%)]\tLoss: 8289920.517903\n",
      "Train Epoch: 93 [0/50981 (0%)]\tLoss: 7978817.000000\n",
      "Train Epoch: 93 [1280/50981 (3%)]\tLoss: 7930971.136364\n",
      "Train Epoch: 93 [2560/50981 (5%)]\tLoss: 7987017.190476\n",
      "Train Epoch: 93 [3840/50981 (8%)]\tLoss: 8032756.403226\n",
      "Train Epoch: 93 [5120/50981 (10%)]\tLoss: 8042563.646341\n",
      "Train Epoch: 93 [6400/50981 (13%)]\tLoss: 8111757.303922\n",
      "Train Epoch: 93 [7680/50981 (15%)]\tLoss: 8118534.008197\n",
      "Train Epoch: 93 [8960/50981 (18%)]\tLoss: 8130735.901408\n",
      "Train Epoch: 93 [10240/50981 (20%)]\tLoss: 8136245.777778\n",
      "Train Epoch: 93 [11520/50981 (23%)]\tLoss: 8150173.730769\n",
      "Train Epoch: 93 [12800/50981 (25%)]\tLoss: 8160689.113861\n",
      "Train Epoch: 93 [14080/50981 (28%)]\tLoss: 8191447.288288\n",
      "Train Epoch: 93 [15360/50981 (30%)]\tLoss: 8201240.847107\n",
      "Train Epoch: 93 [16640/50981 (33%)]\tLoss: 8202761.343511\n",
      "Train Epoch: 93 [17920/50981 (35%)]\tLoss: 8198277.354610\n",
      "Train Epoch: 93 [19200/50981 (38%)]\tLoss: 8200570.350993\n",
      "Train Epoch: 93 [20480/50981 (40%)]\tLoss: 8210766.965839\n",
      "Train Epoch: 93 [21760/50981 (43%)]\tLoss: 8209614.295322\n",
      "Train Epoch: 93 [23040/50981 (45%)]\tLoss: 8220704.201657\n",
      "Train Epoch: 93 [24320/50981 (48%)]\tLoss: 8224880.507853\n",
      "Train Epoch: 93 [25600/50981 (50%)]\tLoss: 8218301.696517\n",
      "Train Epoch: 93 [26880/50981 (53%)]\tLoss: 8219726.071090\n",
      "Train Epoch: 93 [28160/50981 (55%)]\tLoss: 8225010.773756\n",
      "Train Epoch: 93 [29440/50981 (58%)]\tLoss: 8238308.612554\n",
      "Train Epoch: 93 [30720/50981 (60%)]\tLoss: 8239386.931535\n",
      "Train Epoch: 93 [32000/50981 (63%)]\tLoss: 8237164.484064\n",
      "Train Epoch: 93 [33280/50981 (65%)]\tLoss: 8238309.358238\n",
      "Train Epoch: 93 [34560/50981 (68%)]\tLoss: 8244066.461255\n",
      "Train Epoch: 93 [35840/50981 (70%)]\tLoss: 8241168.044484\n",
      "Train Epoch: 93 [37120/50981 (73%)]\tLoss: 8244809.238832\n",
      "Train Epoch: 93 [38400/50981 (75%)]\tLoss: 8247179.377076\n",
      "Train Epoch: 93 [39680/50981 (78%)]\tLoss: 8250718.503215\n",
      "Train Epoch: 93 [40960/50981 (80%)]\tLoss: 8252806.971963\n",
      "Train Epoch: 93 [42240/50981 (83%)]\tLoss: 8259683.956193\n",
      "Train Epoch: 93 [43520/50981 (85%)]\tLoss: 8263356.043988\n",
      "Train Epoch: 93 [44800/50981 (88%)]\tLoss: 8264771.525641\n",
      "Train Epoch: 93 [46080/50981 (90%)]\tLoss: 8264081.263158\n",
      "Train Epoch: 93 [47360/50981 (93%)]\tLoss: 8269661.474394\n",
      "Train Epoch: 93 [48640/50981 (95%)]\tLoss: 8268684.805774\n",
      "Train Epoch: 93 [49920/50981 (98%)]\tLoss: 8267915.475703\n",
      "Train Epoch: 94 [0/50981 (0%)]\tLoss: 7772231.500000\n",
      "Train Epoch: 94 [1280/50981 (3%)]\tLoss: 8067541.863636\n",
      "Train Epoch: 94 [2560/50981 (5%)]\tLoss: 8084167.571429\n",
      "Train Epoch: 94 [3840/50981 (8%)]\tLoss: 8100991.903226\n",
      "Train Epoch: 94 [5120/50981 (10%)]\tLoss: 8093321.219512\n",
      "Train Epoch: 94 [6400/50981 (13%)]\tLoss: 8098314.107843\n",
      "Train Epoch: 94 [7680/50981 (15%)]\tLoss: 8083529.704918\n",
      "Train Epoch: 94 [8960/50981 (18%)]\tLoss: 8112546.049296\n",
      "Train Epoch: 94 [10240/50981 (20%)]\tLoss: 8114348.888889\n",
      "Train Epoch: 94 [11520/50981 (23%)]\tLoss: 8121579.021978\n",
      "Train Epoch: 94 [12800/50981 (25%)]\tLoss: 8113738.410891\n",
      "Train Epoch: 94 [14080/50981 (28%)]\tLoss: 8147512.716216\n",
      "Train Epoch: 94 [15360/50981 (30%)]\tLoss: 8168361.851240\n",
      "Train Epoch: 94 [16640/50981 (33%)]\tLoss: 8177863.912214\n",
      "Train Epoch: 94 [17920/50981 (35%)]\tLoss: 8176489.248227\n",
      "Train Epoch: 94 [19200/50981 (38%)]\tLoss: 8171011.456954\n",
      "Train Epoch: 94 [20480/50981 (40%)]\tLoss: 8172783.717391\n",
      "Train Epoch: 94 [21760/50981 (43%)]\tLoss: 8175578.511696\n",
      "Train Epoch: 94 [23040/50981 (45%)]\tLoss: 8185239.519337\n",
      "Train Epoch: 94 [24320/50981 (48%)]\tLoss: 8184798.078534\n",
      "Train Epoch: 94 [25600/50981 (50%)]\tLoss: 8188626.271144\n",
      "Train Epoch: 94 [26880/50981 (53%)]\tLoss: 8187264.267773\n",
      "Train Epoch: 94 [28160/50981 (55%)]\tLoss: 8198614.545249\n",
      "Train Epoch: 94 [29440/50981 (58%)]\tLoss: 8203858.322511\n",
      "Train Epoch: 94 [30720/50981 (60%)]\tLoss: 8209592.026971\n",
      "Train Epoch: 94 [32000/50981 (63%)]\tLoss: 8204596.358566\n",
      "Train Epoch: 94 [33280/50981 (65%)]\tLoss: 8202113.498084\n",
      "Train Epoch: 94 [34560/50981 (68%)]\tLoss: 8202496.352399\n",
      "Train Epoch: 94 [35840/50981 (70%)]\tLoss: 8213747.008897\n",
      "Train Epoch: 94 [37120/50981 (73%)]\tLoss: 8210993.544674\n",
      "Train Epoch: 94 [38400/50981 (75%)]\tLoss: 8212602.533223\n",
      "Train Epoch: 94 [39680/50981 (78%)]\tLoss: 8219437.728296\n",
      "Train Epoch: 94 [40960/50981 (80%)]\tLoss: 8222910.101246\n",
      "Train Epoch: 94 [42240/50981 (83%)]\tLoss: 8229391.685801\n",
      "Train Epoch: 94 [43520/50981 (85%)]\tLoss: 8239203.382698\n",
      "Train Epoch: 94 [44800/50981 (88%)]\tLoss: 8242083.639601\n",
      "Train Epoch: 94 [46080/50981 (90%)]\tLoss: 8245304.572022\n",
      "Train Epoch: 94 [47360/50981 (93%)]\tLoss: 8249836.454178\n",
      "Train Epoch: 94 [48640/50981 (95%)]\tLoss: 8254500.237533\n",
      "Train Epoch: 94 [49920/50981 (98%)]\tLoss: 8254862.771100\n",
      "Train Epoch: 95 [0/50981 (0%)]\tLoss: 8045684.500000\n",
      "Train Epoch: 95 [1280/50981 (3%)]\tLoss: 8090037.909091\n",
      "Train Epoch: 95 [2560/50981 (5%)]\tLoss: 8176943.761905\n",
      "Train Epoch: 95 [3840/50981 (8%)]\tLoss: 8148081.048387\n",
      "Train Epoch: 95 [5120/50981 (10%)]\tLoss: 8106331.792683\n",
      "Train Epoch: 95 [6400/50981 (13%)]\tLoss: 8133533.931373\n",
      "Train Epoch: 95 [7680/50981 (15%)]\tLoss: 8120606.713115\n",
      "Train Epoch: 95 [8960/50981 (18%)]\tLoss: 8110555.218310\n",
      "Train Epoch: 95 [10240/50981 (20%)]\tLoss: 8116581.932099\n",
      "Train Epoch: 95 [11520/50981 (23%)]\tLoss: 8127350.835165\n",
      "Train Epoch: 95 [12800/50981 (25%)]\tLoss: 8159332.623762\n",
      "Train Epoch: 95 [14080/50981 (28%)]\tLoss: 8140233.310811\n",
      "Train Epoch: 95 [15360/50981 (30%)]\tLoss: 8154688.942149\n",
      "Train Epoch: 95 [16640/50981 (33%)]\tLoss: 8162518.977099\n",
      "Train Epoch: 95 [17920/50981 (35%)]\tLoss: 8169902.496454\n",
      "Train Epoch: 95 [19200/50981 (38%)]\tLoss: 8174941.516556\n",
      "Train Epoch: 95 [20480/50981 (40%)]\tLoss: 8186420.658385\n",
      "Train Epoch: 95 [21760/50981 (43%)]\tLoss: 8186444.681287\n",
      "Train Epoch: 95 [23040/50981 (45%)]\tLoss: 8184071.798343\n",
      "Train Epoch: 95 [24320/50981 (48%)]\tLoss: 8179489.259162\n",
      "Train Epoch: 95 [25600/50981 (50%)]\tLoss: 8179421.870647\n",
      "Train Epoch: 95 [26880/50981 (53%)]\tLoss: 8178656.945498\n",
      "Train Epoch: 95 [28160/50981 (55%)]\tLoss: 8175791.429864\n",
      "Train Epoch: 95 [29440/50981 (58%)]\tLoss: 8181768.584416\n",
      "Train Epoch: 95 [30720/50981 (60%)]\tLoss: 8180812.680498\n",
      "Train Epoch: 95 [32000/50981 (63%)]\tLoss: 8185282.515936\n",
      "Train Epoch: 95 [33280/50981 (65%)]\tLoss: 8193330.821839\n",
      "Train Epoch: 95 [34560/50981 (68%)]\tLoss: 8203084.197417\n",
      "Train Epoch: 95 [35840/50981 (70%)]\tLoss: 8204533.619217\n",
      "Train Epoch: 95 [37120/50981 (73%)]\tLoss: 8211019.419244\n",
      "Train Epoch: 95 [38400/50981 (75%)]\tLoss: 8217936.039867\n",
      "Train Epoch: 95 [39680/50981 (78%)]\tLoss: 8222686.673633\n",
      "Train Epoch: 95 [40960/50981 (80%)]\tLoss: 8224301.683801\n",
      "Train Epoch: 95 [42240/50981 (83%)]\tLoss: 8224955.382175\n",
      "Train Epoch: 95 [43520/50981 (85%)]\tLoss: 8233401.585044\n",
      "Train Epoch: 95 [44800/50981 (88%)]\tLoss: 8235264.202279\n",
      "Train Epoch: 95 [46080/50981 (90%)]\tLoss: 8237312.468144\n",
      "Train Epoch: 95 [47360/50981 (93%)]\tLoss: 8238984.649596\n",
      "Train Epoch: 95 [48640/50981 (95%)]\tLoss: 8241276.723097\n",
      "Train Epoch: 95 [49920/50981 (98%)]\tLoss: 8243631.291560\n",
      "Train Epoch: 96 [0/50981 (0%)]\tLoss: 8625164.000000\n",
      "Train Epoch: 96 [1280/50981 (3%)]\tLoss: 8042163.454545\n",
      "Train Epoch: 96 [2560/50981 (5%)]\tLoss: 8115473.261905\n",
      "Train Epoch: 96 [3840/50981 (8%)]\tLoss: 8103043.258065\n",
      "Train Epoch: 96 [5120/50981 (10%)]\tLoss: 8114156.975610\n",
      "Train Epoch: 96 [6400/50981 (13%)]\tLoss: 8128129.980392\n",
      "Train Epoch: 96 [7680/50981 (15%)]\tLoss: 8131625.131148\n",
      "Train Epoch: 96 [8960/50981 (18%)]\tLoss: 8135755.119718\n",
      "Train Epoch: 96 [10240/50981 (20%)]\tLoss: 8151476.104938\n",
      "Train Epoch: 96 [11520/50981 (23%)]\tLoss: 8176395.269231\n",
      "Train Epoch: 96 [12800/50981 (25%)]\tLoss: 8173759.950495\n",
      "Train Epoch: 96 [14080/50981 (28%)]\tLoss: 8159389.391892\n",
      "Train Epoch: 96 [15360/50981 (30%)]\tLoss: 8158584.814050\n",
      "Train Epoch: 96 [16640/50981 (33%)]\tLoss: 8154100.282443\n",
      "Train Epoch: 96 [17920/50981 (35%)]\tLoss: 8146828.663121\n",
      "Train Epoch: 96 [19200/50981 (38%)]\tLoss: 8158294.986755\n",
      "Train Epoch: 96 [20480/50981 (40%)]\tLoss: 8166423.500000\n",
      "Train Epoch: 96 [21760/50981 (43%)]\tLoss: 8169889.005848\n",
      "Train Epoch: 96 [23040/50981 (45%)]\tLoss: 8173206.662983\n",
      "Train Epoch: 96 [24320/50981 (48%)]\tLoss: 8171944.526178\n",
      "Train Epoch: 96 [25600/50981 (50%)]\tLoss: 8175109.514925\n",
      "Train Epoch: 96 [26880/50981 (53%)]\tLoss: 8176366.924171\n",
      "Train Epoch: 96 [28160/50981 (55%)]\tLoss: 8174153.153846\n",
      "Train Epoch: 96 [29440/50981 (58%)]\tLoss: 8184117.257576\n",
      "Train Epoch: 96 [30720/50981 (60%)]\tLoss: 8180023.132780\n",
      "Train Epoch: 96 [32000/50981 (63%)]\tLoss: 8181140.173307\n",
      "Train Epoch: 96 [33280/50981 (65%)]\tLoss: 8185414.739464\n",
      "Train Epoch: 96 [34560/50981 (68%)]\tLoss: 8189004.005535\n",
      "Train Epoch: 96 [35840/50981 (70%)]\tLoss: 8188554.992883\n",
      "Train Epoch: 96 [37120/50981 (73%)]\tLoss: 8192648.802405\n",
      "Train Epoch: 96 [38400/50981 (75%)]\tLoss: 8199393.244186\n",
      "Train Epoch: 96 [39680/50981 (78%)]\tLoss: 8207466.276527\n",
      "Train Epoch: 96 [40960/50981 (80%)]\tLoss: 8208417.741433\n",
      "Train Epoch: 96 [42240/50981 (83%)]\tLoss: 8210704.122356\n",
      "Train Epoch: 96 [43520/50981 (85%)]\tLoss: 8215469.412023\n",
      "Train Epoch: 96 [44800/50981 (88%)]\tLoss: 8218302.133903\n",
      "Train Epoch: 96 [46080/50981 (90%)]\tLoss: 8220047.163435\n",
      "Train Epoch: 96 [47360/50981 (93%)]\tLoss: 8221500.795148\n",
      "Train Epoch: 96 [48640/50981 (95%)]\tLoss: 8224684.636483\n",
      "Train Epoch: 96 [49920/50981 (98%)]\tLoss: 8229514.789003\n",
      "Train Epoch: 97 [0/50981 (0%)]\tLoss: 8271523.000000\n",
      "Train Epoch: 97 [1280/50981 (3%)]\tLoss: 8049797.681818\n",
      "Train Epoch: 97 [2560/50981 (5%)]\tLoss: 8090488.452381\n",
      "Train Epoch: 97 [3840/50981 (8%)]\tLoss: 8078400.129032\n",
      "Train Epoch: 97 [5120/50981 (10%)]\tLoss: 8078044.609756\n",
      "Train Epoch: 97 [6400/50981 (13%)]\tLoss: 8059240.205882\n",
      "Train Epoch: 97 [7680/50981 (15%)]\tLoss: 8057427.557377\n",
      "Train Epoch: 97 [8960/50981 (18%)]\tLoss: 8065156.838028\n",
      "Train Epoch: 97 [10240/50981 (20%)]\tLoss: 8075936.672840\n",
      "Train Epoch: 97 [11520/50981 (23%)]\tLoss: 8089725.153846\n",
      "Train Epoch: 97 [12800/50981 (25%)]\tLoss: 8100111.777228\n",
      "Train Epoch: 97 [14080/50981 (28%)]\tLoss: 8110994.815315\n",
      "Train Epoch: 97 [15360/50981 (30%)]\tLoss: 8112654.309917\n",
      "Train Epoch: 97 [16640/50981 (33%)]\tLoss: 8114828.835878\n",
      "Train Epoch: 97 [17920/50981 (35%)]\tLoss: 8125079.166667\n",
      "Train Epoch: 97 [19200/50981 (38%)]\tLoss: 8122744.251656\n",
      "Train Epoch: 97 [20480/50981 (40%)]\tLoss: 8125068.316770\n",
      "Train Epoch: 97 [21760/50981 (43%)]\tLoss: 8125721.447368\n",
      "Train Epoch: 97 [23040/50981 (45%)]\tLoss: 8127551.585635\n",
      "Train Epoch: 97 [24320/50981 (48%)]\tLoss: 8127012.204188\n",
      "Train Epoch: 97 [25600/50981 (50%)]\tLoss: 8132560.557214\n",
      "Train Epoch: 97 [26880/50981 (53%)]\tLoss: 8131148.545024\n",
      "Train Epoch: 97 [28160/50981 (55%)]\tLoss: 8136864.819005\n",
      "Train Epoch: 97 [29440/50981 (58%)]\tLoss: 8142329.419913\n",
      "Train Epoch: 97 [30720/50981 (60%)]\tLoss: 8150380.595436\n",
      "Train Epoch: 97 [32000/50981 (63%)]\tLoss: 8157608.117530\n",
      "Train Epoch: 97 [33280/50981 (65%)]\tLoss: 8157242.346743\n",
      "Train Epoch: 97 [34560/50981 (68%)]\tLoss: 8166500.000000\n",
      "Train Epoch: 97 [35840/50981 (70%)]\tLoss: 8171850.555160\n",
      "Train Epoch: 97 [37120/50981 (73%)]\tLoss: 8173071.065292\n",
      "Train Epoch: 97 [38400/50981 (75%)]\tLoss: 8178364.028239\n",
      "Train Epoch: 97 [39680/50981 (78%)]\tLoss: 8175706.967846\n",
      "Train Epoch: 97 [40960/50981 (80%)]\tLoss: 8179331.236760\n",
      "Train Epoch: 97 [42240/50981 (83%)]\tLoss: 8180751.954683\n",
      "Train Epoch: 97 [43520/50981 (85%)]\tLoss: 8184797.988270\n",
      "Train Epoch: 97 [44800/50981 (88%)]\tLoss: 8188719.844729\n",
      "Train Epoch: 97 [46080/50981 (90%)]\tLoss: 8189722.094183\n",
      "Train Epoch: 97 [47360/50981 (93%)]\tLoss: 8191432.766846\n",
      "Train Epoch: 97 [48640/50981 (95%)]\tLoss: 8196561.423885\n",
      "Train Epoch: 97 [49920/50981 (98%)]\tLoss: 8202592.372123\n",
      "Train Epoch: 98 [0/50981 (0%)]\tLoss: 8531554.000000\n",
      "Train Epoch: 98 [1280/50981 (3%)]\tLoss: 8226501.272727\n",
      "Train Epoch: 98 [2560/50981 (5%)]\tLoss: 8059156.119048\n",
      "Train Epoch: 98 [3840/50981 (8%)]\tLoss: 8025706.741935\n",
      "Train Epoch: 98 [5120/50981 (10%)]\tLoss: 8006910.878049\n",
      "Train Epoch: 98 [6400/50981 (13%)]\tLoss: 8020428.176471\n",
      "Train Epoch: 98 [7680/50981 (15%)]\tLoss: 8059119.303279\n",
      "Train Epoch: 98 [8960/50981 (18%)]\tLoss: 8052765.154930\n",
      "Train Epoch: 98 [10240/50981 (20%)]\tLoss: 8068531.462963\n",
      "Train Epoch: 98 [11520/50981 (23%)]\tLoss: 8075563.148352\n",
      "Train Epoch: 98 [12800/50981 (25%)]\tLoss: 8063629.608911\n",
      "Train Epoch: 98 [14080/50981 (28%)]\tLoss: 8062646.729730\n",
      "Train Epoch: 98 [15360/50981 (30%)]\tLoss: 8067384.090909\n",
      "Train Epoch: 98 [16640/50981 (33%)]\tLoss: 8077421.141221\n",
      "Train Epoch: 98 [17920/50981 (35%)]\tLoss: 8092399.159574\n",
      "Train Epoch: 98 [19200/50981 (38%)]\tLoss: 8096552.615894\n",
      "Train Epoch: 98 [20480/50981 (40%)]\tLoss: 8101290.453416\n",
      "Train Epoch: 98 [21760/50981 (43%)]\tLoss: 8098292.994152\n",
      "Train Epoch: 98 [23040/50981 (45%)]\tLoss: 8100295.668508\n",
      "Train Epoch: 98 [24320/50981 (48%)]\tLoss: 8110819.201571\n",
      "Train Epoch: 98 [25600/50981 (50%)]\tLoss: 8115696.395522\n",
      "Train Epoch: 98 [26880/50981 (53%)]\tLoss: 8115859.611374\n",
      "Train Epoch: 98 [28160/50981 (55%)]\tLoss: 8118723.104072\n",
      "Train Epoch: 98 [29440/50981 (58%)]\tLoss: 8120814.805195\n",
      "Train Epoch: 98 [30720/50981 (60%)]\tLoss: 8127537.458506\n",
      "Train Epoch: 98 [32000/50981 (63%)]\tLoss: 8129998.904382\n",
      "Train Epoch: 98 [33280/50981 (65%)]\tLoss: 8135039.574713\n",
      "Train Epoch: 98 [34560/50981 (68%)]\tLoss: 8141612.804428\n",
      "Train Epoch: 98 [35840/50981 (70%)]\tLoss: 8146998.925267\n",
      "Train Epoch: 98 [37120/50981 (73%)]\tLoss: 8147690.513746\n",
      "Train Epoch: 98 [38400/50981 (75%)]\tLoss: 8151998.870432\n",
      "Train Epoch: 98 [39680/50981 (78%)]\tLoss: 8151855.344051\n",
      "Train Epoch: 98 [40960/50981 (80%)]\tLoss: 8153148.574766\n",
      "Train Epoch: 98 [42240/50981 (83%)]\tLoss: 8157724.939577\n",
      "Train Epoch: 98 [43520/50981 (85%)]\tLoss: 8160171.070381\n",
      "Train Epoch: 98 [44800/50981 (88%)]\tLoss: 8163015.403134\n",
      "Train Epoch: 98 [46080/50981 (90%)]\tLoss: 8158655.797784\n",
      "Train Epoch: 98 [47360/50981 (93%)]\tLoss: 8163636.404313\n",
      "Train Epoch: 98 [48640/50981 (95%)]\tLoss: 8162626.255906\n",
      "Train Epoch: 98 [49920/50981 (98%)]\tLoss: 8171241.918159\n",
      "Train Epoch: 99 [0/50981 (0%)]\tLoss: 7769428.500000\n",
      "Train Epoch: 99 [1280/50981 (3%)]\tLoss: 7992982.272727\n",
      "Train Epoch: 99 [2560/50981 (5%)]\tLoss: 7942936.714286\n",
      "Train Epoch: 99 [3840/50981 (8%)]\tLoss: 7909976.887097\n",
      "Train Epoch: 99 [5120/50981 (10%)]\tLoss: 7929397.646341\n",
      "Train Epoch: 99 [6400/50981 (13%)]\tLoss: 7966318.754902\n",
      "Train Epoch: 99 [7680/50981 (15%)]\tLoss: 7987552.245902\n",
      "Train Epoch: 99 [8960/50981 (18%)]\tLoss: 8003760.091549\n",
      "Train Epoch: 99 [10240/50981 (20%)]\tLoss: 8009867.395062\n",
      "Train Epoch: 99 [11520/50981 (23%)]\tLoss: 8016223.280220\n",
      "Train Epoch: 99 [12800/50981 (25%)]\tLoss: 7997472.490099\n",
      "Train Epoch: 99 [14080/50981 (28%)]\tLoss: 8003824.599099\n",
      "Train Epoch: 99 [15360/50981 (30%)]\tLoss: 8014573.842975\n",
      "Train Epoch: 99 [16640/50981 (33%)]\tLoss: 8013355.125954\n",
      "Train Epoch: 99 [17920/50981 (35%)]\tLoss: 8032209.166667\n",
      "Train Epoch: 99 [19200/50981 (38%)]\tLoss: 8057296.208609\n",
      "Train Epoch: 99 [20480/50981 (40%)]\tLoss: 8066890.720497\n",
      "Train Epoch: 99 [21760/50981 (43%)]\tLoss: 8078198.526316\n",
      "Train Epoch: 99 [23040/50981 (45%)]\tLoss: 8081931.251381\n",
      "Train Epoch: 99 [24320/50981 (48%)]\tLoss: 8086495.123037\n",
      "Train Epoch: 99 [25600/50981 (50%)]\tLoss: 8096034.116915\n",
      "Train Epoch: 99 [26880/50981 (53%)]\tLoss: 8100860.317536\n",
      "Train Epoch: 99 [28160/50981 (55%)]\tLoss: 8106967.549774\n",
      "Train Epoch: 99 [29440/50981 (58%)]\tLoss: 8105941.272727\n",
      "Train Epoch: 99 [30720/50981 (60%)]\tLoss: 8113520.018672\n",
      "Train Epoch: 99 [32000/50981 (63%)]\tLoss: 8115215.422311\n",
      "Train Epoch: 99 [33280/50981 (65%)]\tLoss: 8116771.068966\n",
      "Train Epoch: 99 [34560/50981 (68%)]\tLoss: 8115228.691882\n",
      "Train Epoch: 99 [35840/50981 (70%)]\tLoss: 8118216.017794\n",
      "Train Epoch: 99 [37120/50981 (73%)]\tLoss: 8126321.487973\n",
      "Train Epoch: 99 [38400/50981 (75%)]\tLoss: 8124886.269103\n",
      "Train Epoch: 99 [39680/50981 (78%)]\tLoss: 8127783.467846\n",
      "Train Epoch: 99 [40960/50981 (80%)]\tLoss: 8134938.294393\n",
      "Train Epoch: 99 [42240/50981 (83%)]\tLoss: 8140575.129909\n",
      "Train Epoch: 99 [43520/50981 (85%)]\tLoss: 8148420.695015\n",
      "Train Epoch: 99 [44800/50981 (88%)]\tLoss: 8149278.703704\n",
      "Train Epoch: 99 [46080/50981 (90%)]\tLoss: 8155510.958449\n",
      "Train Epoch: 99 [47360/50981 (93%)]\tLoss: 8155554.006739\n",
      "Train Epoch: 99 [48640/50981 (95%)]\tLoss: 8155738.485564\n",
      "Train Epoch: 99 [49920/50981 (98%)]\tLoss: 8156592.892583\n",
      "Train Epoch: 100 [0/50981 (0%)]\tLoss: 8426095.000000\n",
      "Train Epoch: 100 [1280/50981 (3%)]\tLoss: 7976670.409091\n",
      "Train Epoch: 100 [2560/50981 (5%)]\tLoss: 8000192.404762\n",
      "Train Epoch: 100 [3840/50981 (8%)]\tLoss: 8002173.887097\n",
      "Train Epoch: 100 [5120/50981 (10%)]\tLoss: 8067986.695122\n",
      "Train Epoch: 100 [6400/50981 (13%)]\tLoss: 8045028.352941\n",
      "Train Epoch: 100 [7680/50981 (15%)]\tLoss: 8035565.008197\n",
      "Train Epoch: 100 [8960/50981 (18%)]\tLoss: 8034396.464789\n",
      "Train Epoch: 100 [10240/50981 (20%)]\tLoss: 8019492.265432\n",
      "Train Epoch: 100 [11520/50981 (23%)]\tLoss: 8012266.010989\n",
      "Train Epoch: 100 [12800/50981 (25%)]\tLoss: 8018728.495050\n",
      "Train Epoch: 100 [14080/50981 (28%)]\tLoss: 8042751.121622\n",
      "Train Epoch: 100 [15360/50981 (30%)]\tLoss: 8058210.561983\n",
      "Train Epoch: 100 [16640/50981 (33%)]\tLoss: 8076138.561069\n",
      "Train Epoch: 100 [17920/50981 (35%)]\tLoss: 8082030.985816\n",
      "Train Epoch: 100 [19200/50981 (38%)]\tLoss: 8084455.023179\n",
      "Train Epoch: 100 [20480/50981 (40%)]\tLoss: 8092619.251553\n",
      "Train Epoch: 100 [21760/50981 (43%)]\tLoss: 8097139.701754\n",
      "Train Epoch: 100 [23040/50981 (45%)]\tLoss: 8100599.397790\n",
      "Train Epoch: 100 [24320/50981 (48%)]\tLoss: 8097675.102094\n",
      "Train Epoch: 100 [25600/50981 (50%)]\tLoss: 8098881.786070\n",
      "Train Epoch: 100 [26880/50981 (53%)]\tLoss: 8108746.305687\n",
      "Train Epoch: 100 [28160/50981 (55%)]\tLoss: 8112576.617647\n",
      "Train Epoch: 100 [29440/50981 (58%)]\tLoss: 8121283.246753\n",
      "Train Epoch: 100 [30720/50981 (60%)]\tLoss: 8121671.979253\n",
      "Train Epoch: 100 [32000/50981 (63%)]\tLoss: 8123792.402390\n",
      "Train Epoch: 100 [33280/50981 (65%)]\tLoss: 8121556.844828\n",
      "Train Epoch: 100 [34560/50981 (68%)]\tLoss: 8125809.861624\n",
      "Train Epoch: 100 [35840/50981 (70%)]\tLoss: 8127901.610320\n",
      "Train Epoch: 100 [37120/50981 (73%)]\tLoss: 8134277.020619\n",
      "Train Epoch: 100 [38400/50981 (75%)]\tLoss: 8130434.463455\n",
      "Train Epoch: 100 [39680/50981 (78%)]\tLoss: 8130867.609325\n",
      "Train Epoch: 100 [40960/50981 (80%)]\tLoss: 8134752.034268\n",
      "Train Epoch: 100 [42240/50981 (83%)]\tLoss: 8134126.788520\n",
      "Train Epoch: 100 [43520/50981 (85%)]\tLoss: 8136036.001466\n",
      "Train Epoch: 100 [44800/50981 (88%)]\tLoss: 8143565.917379\n",
      "Train Epoch: 100 [46080/50981 (90%)]\tLoss: 8150435.455679\n",
      "Train Epoch: 100 [47360/50981 (93%)]\tLoss: 8152090.371968\n",
      "Train Epoch: 100 [48640/50981 (95%)]\tLoss: 8154471.389764\n",
      "Train Epoch: 100 [49920/50981 (98%)]\tLoss: 8155881.901535\n",
      "Saved model state dictionary to 'mvae_only_joint.pth'\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "def elbo_loss(recon_xray, xray, recon_ecg, ecg, mu, logvar,\n",
    "              lambda_xray=1.0, lambda_ecg=1.0, annealing_factor=1):\n",
    "\n",
    "    xray_mse, ecg_mse = 0, 0\n",
    "    if recon_xray is not None and xray is not None:\n",
    "        # Reshape to the original image size\n",
    "        xray_mse = nnf.mse_loss(recon_xray, xray, reduction='sum')\n",
    "\n",
    "    if recon_ecg is not None and ecg is not None:\n",
    "        # Reshape to the original image size\n",
    "        ecg_mse = nnf.mse_loss(recon_ecg, ecg, reduction='sum')\n",
    "\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)\n",
    "    \n",
    "    ELBO = torch.mean(lambda_xray * xray_mse + lambda_ecg * ecg_mse + annealing_factor * KLD)\n",
    "    \n",
    "    return ELBO\n",
    "\n",
    "# Utility Functions\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "# Training function\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss_meter = AverageMeter()\n",
    "    N_mini_batches = len(dataloader)\n",
    "\n",
    "    for batch_idx, (ecg, xray) in enumerate(dataloader):\n",
    "        annealing_factor = min(epoch / annealing_epochs, 1) if epoch < annealing_epochs else 1.0\n",
    "        ecg, xray = ecg.to(device), xray.to(device)\n",
    "    \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        recon_xray_joint, recon_ecg_joint, mu_joint, logvar_joint = model(xray, ecg)\n",
    "\n",
    "        joint_loss = elbo_loss(recon_xray_joint, xray, recon_ecg_joint, ecg, mu_joint, logvar_joint, lambda_xray, lambda_ecg, annealing_factor)\n",
    "\n",
    "        train_loss = joint_loss\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss_meter.update(train_loss.item(), len(ecg))\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(ecg)}/{len(dataloader.dataset)} ({100. * batch_idx / N_mini_batches:.0f}%)]\\tLoss: {train_loss_meter.avg:.6f}')\n",
    "\n",
    "    return train_loss_meter.avg\n",
    "   \n",
    "# Hyperparameters\n",
    "n_latents = 256\n",
    "epochs = 100\n",
    "annealing_epochs = 50\n",
    "lr = 1e-3\n",
    "log_interval = 10\n",
    "lambda_xray = 1.0\n",
    "lambda_ecg = 10.0\n",
    "\n",
    "# Model and optimizer setup\n",
    "model = MultimodalVAE(image_input_channels=1, ecg_input_dim=60000,latent_dim=256).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Main training and validation loop\n",
    "best_loss = float('inf')\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = train(epoch)\n",
    "\n",
    "\n",
    "# After training is complete\n",
    "torch.save(model.state_dict(), 'pretrained_models/mvae_only_joint.pth')\n",
    "print(\"Saved model state dictionary to 'mvae_only_joint.pth'\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T12:41:00.819148100Z",
     "start_time": "2024-02-19T11:32:09.682717900Z"
    }
   },
   "id": "87f3b0c580cdac96",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fine-tuning on Aspire Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e1d6bda7e186b32c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dataloader"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e68baf39cadfe7c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import pydicom\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "class XRayDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Loop through each label directory\n",
    "        for label in [0, 1]:\n",
    "            label_dir = os.path.join(root_dir, f'processed_label_{label}')\n",
    "            for folder_name in os.listdir(label_dir):\n",
    "                folder_path = os.path.join(label_dir, folder_name)\n",
    "                image_name = os.listdir(folder_path)[0]  # Assuming only one image per folder\n",
    "                if image_name.endswith('.jpg'):\n",
    "                    self.images.append(os.path.join(folder_path, image_name))\n",
    "                    self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.images[idx]\n",
    "        image = Image.open(image_path)\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "    \n",
    " \n",
    "# Initialize your dataset\n",
    "xray_dataset = XRayDataset(root_dir='D:/Aspire_xray/xray', transform=transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n",
    "    transforms.ToTensor(),\n",
    "]))\n",
    "\n",
    "\n",
    "def process_dicom(file_path, sampling_rate=500):\n",
    "    desired_length = 10 * sampling_rate  # 10 seconds of data\n",
    "    try:\n",
    "        dicom_data = pydicom.dcmread(file_path)\n",
    "        if \"WaveformSequence\" in dicom_data:\n",
    "            rhythm_waveform = dicom_data.WaveformSequence[1]\n",
    "            wave_data = rhythm_waveform.get(\"WaveformData\")\n",
    "            num_channels = rhythm_waveform.NumberOfWaveformChannels\n",
    "            wave_array = np.frombuffer(wave_data, dtype=np.int16)\n",
    "            num_samples_per_channel = wave_array.size // num_channels\n",
    "            \n",
    "            if wave_array.size % num_channels == 0:\n",
    "                wave_array = wave_array.reshape(num_samples_per_channel, num_channels)\n",
    "                \n",
    "\n",
    "                # Trim or Pad the array to 10 seconds\n",
    "                if wave_array.shape[0] > desired_length:\n",
    "                    wave_array = wave_array[:desired_length, :]\n",
    "                elif wave_array.shape[0] < desired_length:\n",
    "                    padding = np.zeros((desired_length - wave_array.shape[0], num_channels), dtype=wave_array.dtype)\n",
    "                    wave_array = np.vstack((wave_array, padding))\n",
    "                \n",
    "                # Normalize the array\n",
    "                wave_array = (wave_array - np.mean(wave_array, axis=0)) / np.std(wave_array, axis=0)\n",
    "\n",
    "                return wave_array\n",
    "            else:\n",
    "                print(f\"Unexpected data size in {file_path}. Skipping file.\")\n",
    "                return None\n",
    "        else:\n",
    "            print(f\"No Waveform data found in {file_path}. Skipping file.\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "class ECGDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.ecg_data = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Loop through each label directory\n",
    "        for label in [0, 1]:\n",
    "            label_dir = os.path.join(root_dir, f'processed_label_{label}')\n",
    "            for folder_name in os.listdir(label_dir):\n",
    "                folder_path = os.path.join(label_dir, folder_name)\n",
    "                for file_name in os.listdir(folder_path):\n",
    "                    if file_name.endswith('.dcm'):\n",
    "                        file_path = os.path.join(folder_path, file_name)\n",
    "                        ecg_waveform = process_dicom(file_path)\n",
    "                        if ecg_waveform is not None:\n",
    "                            self.ecg_data.append(ecg_waveform)\n",
    "                            self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ecg_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ecg_waveform = self.ecg_data[idx]\n",
    "        label = self.labels[idx]\n",
    "        # Reshape waveform to [1, signal_length]\n",
    "        ecg_waveform = ecg_waveform.reshape(1, -1)  \n",
    "        return torch.tensor(ecg_waveform, dtype=torch.float32), label\n",
    "\n",
    "# Usage example\n",
    "ecg_dataset = ECGDataset(root_dir='D:/Aspire_ecg/ecg')\n",
    "\n",
    "class CombinedDataset(Dataset):\n",
    "    def __init__(self, xray_dataset, ecg_dataset):\n",
    "        self.xray_dataset = xray_dataset\n",
    "        self.ecg_dataset = ecg_dataset\n",
    "        assert len(xray_dataset) == len(ecg_dataset), \"Datasets must be of the same length.\"\n",
    "        \n",
    "        # Assuming the labels are the same for both datasets and can be directly accessed\n",
    "        self.labels = [label for _, label in xray_dataset]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.xray_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        xray_image, xray_label = self.xray_dataset[idx]\n",
    "        ecg_waveform, ecg_label = self.ecg_dataset[idx]\n",
    "        \n",
    "        # Ensure the labels match if they are supposed to be the same\n",
    "        assert xray_label == ecg_label, \"Labels do not match for the same index.\"\n",
    "        \n",
    "        return xray_image, ecg_waveform, xray_label  # Use either xray_label or ecg_label\n",
    "\n",
    "    def get_labels(self):\n",
    "        return self.labels\n",
    "\n",
    "# Instantiate the combined dataset\n",
    "combined_dataset = CombinedDataset(xray_dataset, ecg_dataset)\n",
    "labels = combined_dataset.get_labels()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T01:01:30.853019100Z",
     "start_time": "2024-02-23T01:00:58.798276200Z"
    }
   },
   "id": "806ba8b0d2941cb9",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b5161ef49ffc900"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "class MultimodalClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_mvae, num_classes):\n",
    "        super(MultimodalClassifier, self).__init__()\n",
    "        self.image_encoder = pretrained_mvae.image_encoder\n",
    "        self.ecg_encoder = pretrained_mvae.ecg_encoder\n",
    "        \n",
    "        # Assuming you want to concatenate the encoded features\n",
    "        combined_feature_dim = pretrained_mvae.n_latents * 2  # Since you're likely concatenating\n",
    "        \n",
    "        # Freeze the encoder weights\n",
    "        for param in self.image_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.ecg_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(combined_feature_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, xray, ecg):\n",
    "        mu_xray, _ = self.image_encoder(xray)\n",
    "        mu_ecg, _ = self.ecg_encoder(ecg)\n",
    "        combined_features = torch.cat((mu_xray, mu_ecg), dim=1)\n",
    "        logits = self.classifier(combined_features)\n",
    "        return logits"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T01:01:30.860361500Z",
     "start_time": "2024-02-23T01:01:30.856019700Z"
    }
   },
   "id": "cad8ddd4e1fdadc",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa8d53842f4c76ac"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "def train_classifier(model, train_loader, criterion, optimizer, epochs=50):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for xray, ecg, labels in train_loader:\n",
    "            xray, ecg, labels = xray.to(device), ecg.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(xray, ecg)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader.dataset)\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}')\n",
    "\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for xray, ecg, labels in data_loader:\n",
    "            xray, ecg, labels = xray.to(device), ecg.to(device), labels.to(device)\n",
    "            logits = model(xray, ecg)\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "            \n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs[:, 1].cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "    \n",
    "    auc_score = roc_auc_score(all_labels, all_probs)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    return accuracy, auc_score"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T01:01:31.452511200Z",
     "start_time": "2024-02-23T01:01:30.858361200Z"
    }
   },
   "id": "a4c49f3ed5c831b7",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Epoch 1/50, Loss: 0.7196\n",
      "Epoch 2/50, Loss: 0.5867\n",
      "Epoch 3/50, Loss: 0.5327\n",
      "Epoch 4/50, Loss: 0.5167\n",
      "Epoch 5/50, Loss: 0.5111\n",
      "Epoch 6/50, Loss: 0.4896\n",
      "Epoch 7/50, Loss: 0.4861\n",
      "Epoch 8/50, Loss: 0.4643\n",
      "Epoch 9/50, Loss: 0.4556\n",
      "Epoch 10/50, Loss: 0.4406\n",
      "Epoch 11/50, Loss: 0.4290\n",
      "Epoch 12/50, Loss: 0.4026\n",
      "Epoch 13/50, Loss: 0.4136\n",
      "Epoch 14/50, Loss: 0.3868\n",
      "Epoch 15/50, Loss: 0.4108\n",
      "Epoch 16/50, Loss: 0.3913\n",
      "Epoch 17/50, Loss: 0.3852\n",
      "Epoch 18/50, Loss: 0.3664\n",
      "Epoch 19/50, Loss: 0.3649\n",
      "Epoch 20/50, Loss: 0.3725\n",
      "Epoch 21/50, Loss: 0.3504\n",
      "Epoch 22/50, Loss: 0.3615\n",
      "Epoch 23/50, Loss: 0.3364\n",
      "Epoch 24/50, Loss: 0.3365\n",
      "Epoch 25/50, Loss: 0.2899\n",
      "Epoch 26/50, Loss: 0.3257\n",
      "Epoch 27/50, Loss: 0.3001\n",
      "Epoch 28/50, Loss: 0.2791\n",
      "Epoch 29/50, Loss: 0.2882\n",
      "Epoch 30/50, Loss: 0.2637\n",
      "Epoch 31/50, Loss: 0.2806\n",
      "Epoch 32/50, Loss: 0.2791\n",
      "Epoch 33/50, Loss: 0.2777\n",
      "Epoch 34/50, Loss: 0.2405\n",
      "Epoch 35/50, Loss: 0.2677\n",
      "Epoch 36/50, Loss: 0.2626\n",
      "Epoch 37/50, Loss: 0.2546\n",
      "Epoch 38/50, Loss: 0.2641\n",
      "Epoch 39/50, Loss: 0.2342\n",
      "Epoch 40/50, Loss: 0.2435\n",
      "Epoch 41/50, Loss: 0.2312\n",
      "Epoch 42/50, Loss: 0.2255\n",
      "Epoch 43/50, Loss: 0.2181\n",
      "Epoch 44/50, Loss: 0.2223\n",
      "Epoch 45/50, Loss: 0.2570\n",
      "Epoch 46/50, Loss: 0.2342\n",
      "Epoch 47/50, Loss: 0.2198\n",
      "Epoch 48/50, Loss: 0.2185\n",
      "Epoch 49/50, Loss: 0.2024\n",
      "Epoch 50/50, Loss: 0.2364\n",
      "Fold 0 Results: Accuracy: 0.7375, AUC: 0.7113\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Epoch 1/50, Loss: 0.7288\n",
      "Epoch 2/50, Loss: 0.5815\n",
      "Epoch 3/50, Loss: 0.5412\n",
      "Epoch 4/50, Loss: 0.5440\n",
      "Epoch 5/50, Loss: 0.5158\n",
      "Epoch 6/50, Loss: 0.4898\n",
      "Epoch 7/50, Loss: 0.4924\n",
      "Epoch 8/50, Loss: 0.4846\n",
      "Epoch 9/50, Loss: 0.4790\n",
      "Epoch 10/50, Loss: 0.4476\n",
      "Epoch 11/50, Loss: 0.4551\n",
      "Epoch 12/50, Loss: 0.4348\n",
      "Epoch 13/50, Loss: 0.4121\n",
      "Epoch 14/50, Loss: 0.4162\n",
      "Epoch 15/50, Loss: 0.4092\n",
      "Epoch 16/50, Loss: 0.3899\n",
      "Epoch 17/50, Loss: 0.3796\n",
      "Epoch 18/50, Loss: 0.3603\n",
      "Epoch 19/50, Loss: 0.3687\n",
      "Epoch 20/50, Loss: 0.3709\n",
      "Epoch 21/50, Loss: 0.3544\n",
      "Epoch 22/50, Loss: 0.3306\n",
      "Epoch 23/50, Loss: 0.3169\n",
      "Epoch 24/50, Loss: 0.3218\n",
      "Epoch 25/50, Loss: 0.3360\n",
      "Epoch 26/50, Loss: 0.3183\n",
      "Epoch 27/50, Loss: 0.2912\n",
      "Epoch 28/50, Loss: 0.2955\n",
      "Epoch 29/50, Loss: 0.3045\n",
      "Epoch 30/50, Loss: 0.2865\n",
      "Epoch 31/50, Loss: 0.2949\n",
      "Epoch 32/50, Loss: 0.2903\n",
      "Epoch 33/50, Loss: 0.2882\n",
      "Epoch 34/50, Loss: 0.2598\n",
      "Epoch 35/50, Loss: 0.2735\n",
      "Epoch 36/50, Loss: 0.2546\n",
      "Epoch 37/50, Loss: 0.2455\n",
      "Epoch 38/50, Loss: 0.2301\n",
      "Epoch 39/50, Loss: 0.2378\n",
      "Epoch 40/50, Loss: 0.2150\n",
      "Epoch 41/50, Loss: 0.2188\n",
      "Epoch 42/50, Loss: 0.2185\n",
      "Epoch 43/50, Loss: 0.2094\n",
      "Epoch 44/50, Loss: 0.1989\n",
      "Epoch 45/50, Loss: 0.2023\n",
      "Epoch 46/50, Loss: 0.2036\n",
      "Epoch 47/50, Loss: 0.1879\n",
      "Epoch 48/50, Loss: 0.2007\n",
      "Epoch 49/50, Loss: 0.2200\n",
      "Epoch 50/50, Loss: 0.2045\n",
      "Fold 1 Results: Accuracy: 0.7625, AUC: 0.7098\n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Epoch 1/50, Loss: 0.7537\n",
      "Epoch 2/50, Loss: 0.5592\n",
      "Epoch 3/50, Loss: 0.5482\n",
      "Epoch 4/50, Loss: 0.5449\n",
      "Epoch 5/50, Loss: 0.5157\n",
      "Epoch 6/50, Loss: 0.4720\n",
      "Epoch 7/50, Loss: 0.4729\n",
      "Epoch 8/50, Loss: 0.4542\n",
      "Epoch 9/50, Loss: 0.4420\n",
      "Epoch 10/50, Loss: 0.4487\n",
      "Epoch 11/50, Loss: 0.4266\n",
      "Epoch 12/50, Loss: 0.4252\n",
      "Epoch 13/50, Loss: 0.4164\n",
      "Epoch 14/50, Loss: 0.3950\n",
      "Epoch 15/50, Loss: 0.3979\n",
      "Epoch 16/50, Loss: 0.3917\n",
      "Epoch 17/50, Loss: 0.3691\n",
      "Epoch 18/50, Loss: 0.3673\n",
      "Epoch 19/50, Loss: 0.3656\n",
      "Epoch 20/50, Loss: 0.3334\n",
      "Epoch 21/50, Loss: 0.3320\n",
      "Epoch 22/50, Loss: 0.3243\n",
      "Epoch 23/50, Loss: 0.3129\n",
      "Epoch 24/50, Loss: 0.3061\n",
      "Epoch 25/50, Loss: 0.3426\n",
      "Epoch 26/50, Loss: 0.2894\n",
      "Epoch 27/50, Loss: 0.2941\n",
      "Epoch 28/50, Loss: 0.2936\n",
      "Epoch 29/50, Loss: 0.2957\n",
      "Epoch 30/50, Loss: 0.2949\n",
      "Epoch 31/50, Loss: 0.2767\n",
      "Epoch 32/50, Loss: 0.2598\n",
      "Epoch 33/50, Loss: 0.2442\n",
      "Epoch 34/50, Loss: 0.2514\n",
      "Epoch 35/50, Loss: 0.2531\n",
      "Epoch 36/50, Loss: 0.2328\n",
      "Epoch 37/50, Loss: 0.2524\n",
      "Epoch 38/50, Loss: 0.2606\n",
      "Epoch 39/50, Loss: 0.2749\n",
      "Epoch 40/50, Loss: 0.2292\n",
      "Epoch 41/50, Loss: 0.2482\n",
      "Epoch 42/50, Loss: 0.2138\n",
      "Epoch 43/50, Loss: 0.1826\n",
      "Epoch 44/50, Loss: 0.2253\n",
      "Epoch 45/50, Loss: 0.2156\n",
      "Epoch 46/50, Loss: 0.2191\n",
      "Epoch 47/50, Loss: 0.2087\n",
      "Epoch 48/50, Loss: 0.2028\n",
      "Epoch 49/50, Loss: 0.2047\n",
      "Epoch 50/50, Loss: 0.2020\n",
      "Fold 2 Results: Accuracy: 0.7000, AUC: 0.7188\n",
      "\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "Epoch 1/50, Loss: 0.7537\n",
      "Epoch 2/50, Loss: 0.5697\n",
      "Epoch 3/50, Loss: 0.5517\n",
      "Epoch 4/50, Loss: 0.5199\n",
      "Epoch 5/50, Loss: 0.4939\n",
      "Epoch 6/50, Loss: 0.4856\n",
      "Epoch 7/50, Loss: 0.4774\n",
      "Epoch 8/50, Loss: 0.4529\n",
      "Epoch 9/50, Loss: 0.4242\n",
      "Epoch 10/50, Loss: 0.4413\n",
      "Epoch 11/50, Loss: 0.4151\n",
      "Epoch 12/50, Loss: 0.4400\n",
      "Epoch 13/50, Loss: 0.4163\n",
      "Epoch 14/50, Loss: 0.4036\n",
      "Epoch 15/50, Loss: 0.3786\n",
      "Epoch 16/50, Loss: 0.3761\n",
      "Epoch 17/50, Loss: 0.3809\n",
      "Epoch 18/50, Loss: 0.3553\n",
      "Epoch 19/50, Loss: 0.3356\n",
      "Epoch 20/50, Loss: 0.3656\n",
      "Epoch 21/50, Loss: 0.3173\n",
      "Epoch 22/50, Loss: 0.3186\n",
      "Epoch 23/50, Loss: 0.3112\n",
      "Epoch 24/50, Loss: 0.3163\n",
      "Epoch 25/50, Loss: 0.2871\n",
      "Epoch 26/50, Loss: 0.2832\n",
      "Epoch 27/50, Loss: 0.2981\n",
      "Epoch 28/50, Loss: 0.3072\n",
      "Epoch 29/50, Loss: 0.2664\n",
      "Epoch 30/50, Loss: 0.2641\n",
      "Epoch 31/50, Loss: 0.2437\n",
      "Epoch 32/50, Loss: 0.2475\n",
      "Epoch 33/50, Loss: 0.2562\n",
      "Epoch 34/50, Loss: 0.2313\n",
      "Epoch 35/50, Loss: 0.2166\n",
      "Epoch 36/50, Loss: 0.2305\n",
      "Epoch 37/50, Loss: 0.2356\n",
      "Epoch 38/50, Loss: 0.2380\n",
      "Epoch 39/50, Loss: 0.2268\n",
      "Epoch 40/50, Loss: 0.2141\n",
      "Epoch 41/50, Loss: 0.2130\n",
      "Epoch 42/50, Loss: 0.2061\n",
      "Epoch 43/50, Loss: 0.2172\n",
      "Epoch 44/50, Loss: 0.1939\n",
      "Epoch 45/50, Loss: 0.1931\n",
      "Epoch 46/50, Loss: 0.2051\n",
      "Epoch 47/50, Loss: 0.1792\n",
      "Epoch 48/50, Loss: 0.1885\n",
      "Epoch 49/50, Loss: 0.1810\n",
      "Epoch 50/50, Loss: 0.1680\n",
      "Fold 3 Results: Accuracy: 0.7000, AUC: 0.7351\n",
      "\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "Epoch 1/50, Loss: 0.6771\n",
      "Epoch 2/50, Loss: 0.5782\n",
      "Epoch 3/50, Loss: 0.5373\n",
      "Epoch 4/50, Loss: 0.5236\n",
      "Epoch 5/50, Loss: 0.5099\n",
      "Epoch 6/50, Loss: 0.5160\n",
      "Epoch 7/50, Loss: 0.5141\n",
      "Epoch 8/50, Loss: 0.4877\n",
      "Epoch 9/50, Loss: 0.4599\n",
      "Epoch 10/50, Loss: 0.4631\n",
      "Epoch 11/50, Loss: 0.4425\n",
      "Epoch 12/50, Loss: 0.4345\n",
      "Epoch 13/50, Loss: 0.4219\n",
      "Epoch 14/50, Loss: 0.4235\n",
      "Epoch 15/50, Loss: 0.4056\n",
      "Epoch 16/50, Loss: 0.4195\n",
      "Epoch 17/50, Loss: 0.3815\n",
      "Epoch 18/50, Loss: 0.3817\n",
      "Epoch 19/50, Loss: 0.3777\n",
      "Epoch 20/50, Loss: 0.3761\n",
      "Epoch 21/50, Loss: 0.3666\n",
      "Epoch 22/50, Loss: 0.3620\n",
      "Epoch 23/50, Loss: 0.3521\n",
      "Epoch 24/50, Loss: 0.3431\n",
      "Epoch 25/50, Loss: 0.3190\n",
      "Epoch 26/50, Loss: 0.3238\n",
      "Epoch 27/50, Loss: 0.3108\n",
      "Epoch 28/50, Loss: 0.2982\n",
      "Epoch 29/50, Loss: 0.2681\n",
      "Epoch 30/50, Loss: 0.2880\n",
      "Epoch 31/50, Loss: 0.2724\n",
      "Epoch 32/50, Loss: 0.2730\n",
      "Epoch 33/50, Loss: 0.2661\n",
      "Epoch 34/50, Loss: 0.2403\n",
      "Epoch 35/50, Loss: 0.2814\n",
      "Epoch 36/50, Loss: 0.2427\n",
      "Epoch 37/50, Loss: 0.2207\n",
      "Epoch 38/50, Loss: 0.2162\n",
      "Epoch 39/50, Loss: 0.2497\n",
      "Epoch 40/50, Loss: 0.2414\n",
      "Epoch 41/50, Loss: 0.2142\n",
      "Epoch 42/50, Loss: 0.1981\n",
      "Epoch 43/50, Loss: 0.2360\n",
      "Epoch 44/50, Loss: 0.1875\n",
      "Epoch 45/50, Loss: 0.1872\n",
      "Epoch 46/50, Loss: 0.1845\n",
      "Epoch 47/50, Loss: 0.1789\n",
      "Epoch 48/50, Loss: 0.1967\n",
      "Epoch 49/50, Loss: 0.1796\n",
      "Epoch 50/50, Loss: 0.1847\n",
      "Fold 4 Results: Accuracy: 0.8125, AUC: 0.7879\n",
      "\n",
      "FOLD 5\n",
      "--------------------------------\n",
      "Epoch 1/50, Loss: 0.7221\n",
      "Epoch 2/50, Loss: 0.5568\n",
      "Epoch 3/50, Loss: 0.5305\n",
      "Epoch 4/50, Loss: 0.5243\n",
      "Epoch 5/50, Loss: 0.4959\n",
      "Epoch 6/50, Loss: 0.4757\n",
      "Epoch 7/50, Loss: 0.4848\n",
      "Epoch 8/50, Loss: 0.4580\n",
      "Epoch 9/50, Loss: 0.4599\n",
      "Epoch 10/50, Loss: 0.4297\n",
      "Epoch 11/50, Loss: 0.4219\n",
      "Epoch 12/50, Loss: 0.4221\n",
      "Epoch 13/50, Loss: 0.4190\n",
      "Epoch 14/50, Loss: 0.3995\n",
      "Epoch 15/50, Loss: 0.3878\n",
      "Epoch 16/50, Loss: 0.3907\n",
      "Epoch 17/50, Loss: 0.3642\n",
      "Epoch 18/50, Loss: 0.3700\n",
      "Epoch 19/50, Loss: 0.3635\n",
      "Epoch 20/50, Loss: 0.3372\n",
      "Epoch 21/50, Loss: 0.3464\n",
      "Epoch 22/50, Loss: 0.3334\n",
      "Epoch 23/50, Loss: 0.3314\n",
      "Epoch 24/50, Loss: 0.3497\n",
      "Epoch 25/50, Loss: 0.3419\n",
      "Epoch 26/50, Loss: 0.3255\n",
      "Epoch 27/50, Loss: 0.3548\n",
      "Epoch 28/50, Loss: 0.3103\n",
      "Epoch 29/50, Loss: 0.3185\n",
      "Epoch 30/50, Loss: 0.3018\n",
      "Epoch 31/50, Loss: 0.2982\n",
      "Epoch 32/50, Loss: 0.2746\n",
      "Epoch 33/50, Loss: 0.2838\n",
      "Epoch 34/50, Loss: 0.2530\n",
      "Epoch 35/50, Loss: 0.2653\n",
      "Epoch 36/50, Loss: 0.2843\n",
      "Epoch 37/50, Loss: 0.2532\n",
      "Epoch 38/50, Loss: 0.2522\n",
      "Epoch 39/50, Loss: 0.2484\n",
      "Epoch 40/50, Loss: 0.2039\n",
      "Epoch 41/50, Loss: 0.2266\n",
      "Epoch 42/50, Loss: 0.2211\n",
      "Epoch 43/50, Loss: 0.2090\n",
      "Epoch 44/50, Loss: 0.2273\n",
      "Epoch 45/50, Loss: 0.2019\n",
      "Epoch 46/50, Loss: 0.2286\n",
      "Epoch 47/50, Loss: 0.2149\n",
      "Epoch 48/50, Loss: 0.2120\n",
      "Epoch 49/50, Loss: 0.2242\n",
      "Epoch 50/50, Loss: 0.1965\n",
      "Fold 5 Results: Accuracy: 0.7342, AUC: 0.7182\n",
      "\n",
      "FOLD 6\n",
      "--------------------------------\n",
      "Epoch 1/50, Loss: 0.7276\n",
      "Epoch 2/50, Loss: 0.5828\n",
      "Epoch 3/50, Loss: 0.5652\n",
      "Epoch 4/50, Loss: 0.5381\n",
      "Epoch 5/50, Loss: 0.5298\n",
      "Epoch 6/50, Loss: 0.5208\n",
      "Epoch 7/50, Loss: 0.4921\n",
      "Epoch 8/50, Loss: 0.4832\n",
      "Epoch 9/50, Loss: 0.4670\n",
      "Epoch 10/50, Loss: 0.4712\n",
      "Epoch 11/50, Loss: 0.4455\n",
      "Epoch 12/50, Loss: 0.4589\n",
      "Epoch 13/50, Loss: 0.4261\n",
      "Epoch 14/50, Loss: 0.4212\n",
      "Epoch 15/50, Loss: 0.3944\n",
      "Epoch 16/50, Loss: 0.4101\n",
      "Epoch 17/50, Loss: 0.3897\n",
      "Epoch 18/50, Loss: 0.3938\n",
      "Epoch 19/50, Loss: 0.3790\n",
      "Epoch 20/50, Loss: 0.3869\n",
      "Epoch 21/50, Loss: 0.3593\n",
      "Epoch 22/50, Loss: 0.3514\n",
      "Epoch 23/50, Loss: 0.3563\n",
      "Epoch 24/50, Loss: 0.3439\n",
      "Epoch 25/50, Loss: 0.3243\n",
      "Epoch 26/50, Loss: 0.3141\n",
      "Epoch 27/50, Loss: 0.3143\n",
      "Epoch 28/50, Loss: 0.3191\n",
      "Epoch 29/50, Loss: 0.2866\n",
      "Epoch 30/50, Loss: 0.2835\n",
      "Epoch 31/50, Loss: 0.3047\n",
      "Epoch 32/50, Loss: 0.2942\n",
      "Epoch 33/50, Loss: 0.2793\n",
      "Epoch 34/50, Loss: 0.2483\n",
      "Epoch 35/50, Loss: 0.2660\n",
      "Epoch 36/50, Loss: 0.2555\n",
      "Epoch 37/50, Loss: 0.2527\n",
      "Epoch 38/50, Loss: 0.2402\n",
      "Epoch 39/50, Loss: 0.2275\n",
      "Epoch 40/50, Loss: 0.2359\n",
      "Epoch 41/50, Loss: 0.2321\n",
      "Epoch 42/50, Loss: 0.2286\n",
      "Epoch 43/50, Loss: 0.2108\n",
      "Epoch 44/50, Loss: 0.2438\n",
      "Epoch 45/50, Loss: 0.2384\n",
      "Epoch 46/50, Loss: 0.2289\n",
      "Epoch 47/50, Loss: 0.2216\n",
      "Epoch 48/50, Loss: 0.2344\n",
      "Epoch 49/50, Loss: 0.2132\n",
      "Epoch 50/50, Loss: 0.2198\n",
      "Fold 6 Results: Accuracy: 0.8608, AUC: 0.8634\n",
      "\n",
      "FOLD 7\n",
      "--------------------------------\n",
      "Epoch 1/50, Loss: 0.7545\n",
      "Epoch 2/50, Loss: 0.5480\n",
      "Epoch 3/50, Loss: 0.5334\n",
      "Epoch 4/50, Loss: 0.5398\n",
      "Epoch 5/50, Loss: 0.5226\n",
      "Epoch 6/50, Loss: 0.4889\n",
      "Epoch 7/50, Loss: 0.4780\n",
      "Epoch 8/50, Loss: 0.4889\n",
      "Epoch 9/50, Loss: 0.4687\n",
      "Epoch 10/50, Loss: 0.4456\n",
      "Epoch 11/50, Loss: 0.4314\n",
      "Epoch 12/50, Loss: 0.4350\n",
      "Epoch 13/50, Loss: 0.4301\n",
      "Epoch 14/50, Loss: 0.4091\n",
      "Epoch 15/50, Loss: 0.4023\n",
      "Epoch 16/50, Loss: 0.3887\n",
      "Epoch 17/50, Loss: 0.3882\n",
      "Epoch 18/50, Loss: 0.3526\n",
      "Epoch 19/50, Loss: 0.3627\n",
      "Epoch 20/50, Loss: 0.3638\n",
      "Epoch 21/50, Loss: 0.3542\n",
      "Epoch 22/50, Loss: 0.3446\n",
      "Epoch 23/50, Loss: 0.3377\n",
      "Epoch 24/50, Loss: 0.3392\n",
      "Epoch 25/50, Loss: 0.3273\n",
      "Epoch 26/50, Loss: 0.3110\n",
      "Epoch 27/50, Loss: 0.2796\n",
      "Epoch 28/50, Loss: 0.2845\n",
      "Epoch 29/50, Loss: 0.2816\n",
      "Epoch 30/50, Loss: 0.2919\n",
      "Epoch 31/50, Loss: 0.2949\n",
      "Epoch 32/50, Loss: 0.2856\n",
      "Epoch 33/50, Loss: 0.2597\n",
      "Epoch 34/50, Loss: 0.2762\n",
      "Epoch 35/50, Loss: 0.2378\n",
      "Epoch 36/50, Loss: 0.2423\n",
      "Epoch 37/50, Loss: 0.2300\n",
      "Epoch 38/50, Loss: 0.2538\n",
      "Epoch 39/50, Loss: 0.2395\n",
      "Epoch 40/50, Loss: 0.2354\n",
      "Epoch 41/50, Loss: 0.2076\n",
      "Epoch 42/50, Loss: 0.2334\n",
      "Epoch 43/50, Loss: 0.2419\n",
      "Epoch 44/50, Loss: 0.2368\n",
      "Epoch 45/50, Loss: 0.2433\n",
      "Epoch 46/50, Loss: 0.2072\n",
      "Epoch 47/50, Loss: 0.2141\n",
      "Epoch 48/50, Loss: 0.2086\n",
      "Epoch 49/50, Loss: 0.2099\n",
      "Epoch 50/50, Loss: 0.2196\n",
      "Fold 7 Results: Accuracy: 0.7468, AUC: 0.7430\n",
      "\n",
      "FOLD 8\n",
      "--------------------------------\n",
      "Epoch 1/50, Loss: 0.7714\n",
      "Epoch 2/50, Loss: 0.5814\n",
      "Epoch 3/50, Loss: 0.5414\n",
      "Epoch 4/50, Loss: 0.5270\n",
      "Epoch 5/50, Loss: 0.5056\n",
      "Epoch 6/50, Loss: 0.5176\n",
      "Epoch 7/50, Loss: 0.4851\n",
      "Epoch 8/50, Loss: 0.4691\n",
      "Epoch 9/50, Loss: 0.4611\n",
      "Epoch 10/50, Loss: 0.4443\n",
      "Epoch 11/50, Loss: 0.4795\n",
      "Epoch 12/50, Loss: 0.4125\n",
      "Epoch 13/50, Loss: 0.4126\n",
      "Epoch 14/50, Loss: 0.4399\n",
      "Epoch 15/50, Loss: 0.4091\n",
      "Epoch 16/50, Loss: 0.3919\n",
      "Epoch 17/50, Loss: 0.3830\n",
      "Epoch 18/50, Loss: 0.3880\n",
      "Epoch 19/50, Loss: 0.3638\n",
      "Epoch 20/50, Loss: 0.3705\n",
      "Epoch 21/50, Loss: 0.3621\n",
      "Epoch 22/50, Loss: 0.3400\n",
      "Epoch 23/50, Loss: 0.3347\n",
      "Epoch 24/50, Loss: 0.3110\n",
      "Epoch 25/50, Loss: 0.3402\n",
      "Epoch 26/50, Loss: 0.3173\n",
      "Epoch 27/50, Loss: 0.3062\n",
      "Epoch 28/50, Loss: 0.2981\n",
      "Epoch 29/50, Loss: 0.2625\n",
      "Epoch 30/50, Loss: 0.2885\n",
      "Epoch 31/50, Loss: 0.2774\n",
      "Epoch 32/50, Loss: 0.3043\n",
      "Epoch 33/50, Loss: 0.2872\n",
      "Epoch 34/50, Loss: 0.2620\n",
      "Epoch 35/50, Loss: 0.2696\n",
      "Epoch 36/50, Loss: 0.2583\n",
      "Epoch 37/50, Loss: 0.2181\n",
      "Epoch 38/50, Loss: 0.2518\n",
      "Epoch 39/50, Loss: 0.2544\n",
      "Epoch 40/50, Loss: 0.2428\n",
      "Epoch 41/50, Loss: 0.2047\n",
      "Epoch 42/50, Loss: 0.2291\n",
      "Epoch 43/50, Loss: 0.2193\n",
      "Epoch 44/50, Loss: 0.2402\n",
      "Epoch 45/50, Loss: 0.2349\n",
      "Epoch 46/50, Loss: 0.2304\n",
      "Epoch 47/50, Loss: 0.2068\n",
      "Epoch 48/50, Loss: 0.2175\n",
      "Epoch 49/50, Loss: 0.2297\n",
      "Epoch 50/50, Loss: 0.1999\n",
      "Fold 8 Results: Accuracy: 0.7848, AUC: 0.8160\n",
      "\n",
      "FOLD 9\n",
      "--------------------------------\n",
      "Epoch 1/50, Loss: 0.6971\n",
      "Epoch 2/50, Loss: 0.5760\n",
      "Epoch 3/50, Loss: 0.5337\n",
      "Epoch 4/50, Loss: 0.5165\n",
      "Epoch 5/50, Loss: 0.5039\n",
      "Epoch 6/50, Loss: 0.4942\n",
      "Epoch 7/50, Loss: 0.4716\n",
      "Epoch 8/50, Loss: 0.4890\n",
      "Epoch 9/50, Loss: 0.4543\n",
      "Epoch 10/50, Loss: 0.4660\n",
      "Epoch 11/50, Loss: 0.4326\n",
      "Epoch 12/50, Loss: 0.4222\n",
      "Epoch 13/50, Loss: 0.3973\n",
      "Epoch 14/50, Loss: 0.4073\n",
      "Epoch 15/50, Loss: 0.3820\n",
      "Epoch 16/50, Loss: 0.3766\n",
      "Epoch 17/50, Loss: 0.3887\n",
      "Epoch 18/50, Loss: 0.3803\n",
      "Epoch 19/50, Loss: 0.3550\n",
      "Epoch 20/50, Loss: 0.3428\n",
      "Epoch 21/50, Loss: 0.3288\n",
      "Epoch 22/50, Loss: 0.3116\n",
      "Epoch 23/50, Loss: 0.3328\n",
      "Epoch 24/50, Loss: 0.2925\n",
      "Epoch 25/50, Loss: 0.2863\n",
      "Epoch 26/50, Loss: 0.2892\n",
      "Epoch 27/50, Loss: 0.2941\n",
      "Epoch 28/50, Loss: 0.2906\n",
      "Epoch 29/50, Loss: 0.2748\n",
      "Epoch 30/50, Loss: 0.2864\n",
      "Epoch 31/50, Loss: 0.2574\n",
      "Epoch 32/50, Loss: 0.2495\n",
      "Epoch 33/50, Loss: 0.2548\n",
      "Epoch 34/50, Loss: 0.2379\n",
      "Epoch 35/50, Loss: 0.2347\n",
      "Epoch 36/50, Loss: 0.2679\n",
      "Epoch 37/50, Loss: 0.2194\n",
      "Epoch 38/50, Loss: 0.2425\n",
      "Epoch 39/50, Loss: 0.2178\n",
      "Epoch 40/50, Loss: 0.2149\n",
      "Epoch 41/50, Loss: 0.2279\n",
      "Epoch 42/50, Loss: 0.1790\n",
      "Epoch 43/50, Loss: 0.2220\n",
      "Epoch 44/50, Loss: 0.2069\n",
      "Epoch 45/50, Loss: 0.1922\n",
      "Epoch 46/50, Loss: 0.1788\n",
      "Epoch 47/50, Loss: 0.1815\n",
      "Epoch 48/50, Loss: 0.2030\n",
      "Epoch 49/50, Loss: 0.2020\n",
      "Epoch 50/50, Loss: 0.1911\n",
      "Fold 9 Results: Accuracy: 0.7215, AUC: 0.7818\n",
      "\n",
      "Mean Accuracy: 0.7561, STD: 0.0484\n",
      "Mean AUC: 0.7585, STD: 0.0493\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def set_seed(seed_value):\n",
    "    \"\"\"Set seed for reproducibility.\"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "# Set a seed value\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "fold_results = []\n",
    "\n",
    "for fold, (train_ids, test_ids) in enumerate(skf.split(np.zeros(len(labels)), labels)):\n",
    "    print(f'FOLD {fold}')\n",
    "    print('--------------------------------')\n",
    "    \n",
    "    train_subset = Subset(combined_dataset, train_ids)\n",
    "    test_subset = Subset(combined_dataset, test_ids)\n",
    "    \n",
    "    train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_subset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # Load the pre-trained Multimodal VAE\n",
    "    pretrained_mvae = MultimodalVAE(image_input_channels=1, ecg_input_dim=60000, latent_dim=256)\n",
    "    pretrained_mvae.load_state_dict(torch.load('pretrained_models/multimodal_vae_only_joint_100.pth', map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu')))\n",
    "    pretrained_mvae.to(device)\n",
    "    \n",
    "    # Initialize the classifier for this fold\n",
    "    model = MultimodalClassifier(pretrained_mvae=pretrained_mvae, num_classes=2).to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Train the model\n",
    "    train_classifier(model, train_loader, criterion, optimizer, epochs=50)\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    accuracy, auc_score = evaluate_model(model, test_loader)\n",
    "    fold_results.append((accuracy, auc_score))\n",
    "    print(f'Fold {fold} Results: Accuracy: {accuracy:.4f}, AUC: {auc_score:.4f}\\n')\n",
    "    \n",
    "# Calculate and print the mean and STD for each metric across folds\n",
    "accuracies, aucs = zip(*fold_results)\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "std_accuracy = np.std(accuracies)\n",
    "mean_auc = np.mean(aucs)\n",
    "std_auc = np.std(aucs)\n",
    "\n",
    "print(f'Mean Accuracy: {mean_accuracy:.4f}, STD: {std_accuracy:.4f}')\n",
    "print(f'Mean AUC: {mean_auc:.4f}, STD: {std_auc:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T04:22:23.407288500Z",
     "start_time": "2024-02-23T01:01:39.710229100Z"
    }
   },
   "id": "b73e6ffb3a5dcb1a",
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
